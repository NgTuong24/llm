{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b80177b3-f371-48b1-a930-b60115276c78",
   "metadata": {},
   "source": [
    "# 1. Chunk optimization\n",
    "- Tạo chỉ mục, thêm chỉ mục vào metadata\n",
    "# 2. Multi-representation Indexing\n",
    "- Smaller chunks -- ParentDocumentRetriever -- Chia tài liệu thành các thành phần nhỏ hơn\n",
    "- Summary -- Tóm tắt nội dung tài liệu, nhúng bản tóm tắt\n",
    "# 3. ColBERT\n",
    "https://www.youtube.com/watch?v=xTzUn3G9YA0\n",
    "\n",
    "# 4. RAPTOR\n",
    "- ý tưởng nhúng và phân cụm tài liệu -> tóm tắt cụm tài liệu -> tiếp tục nhúng và phân cụm các tài liệu tóm tắt..\n",
    "- cuối cùng thêm tất cả các tài liệu tóm tắt theo cụm vào data ban đầu\n",
    "- embedding toàn bộ -> retriever "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d4f2a396-330f-4f3a-bbcd-f1fd971f2513",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['LANGCHAIN_TRACING_V2'] = 'true'\n",
    "os.environ['LANGCHAIN_ENDPOINT'] = 'https://api.smith.langchain.com'\n",
    "os.environ['LANGCHAIN_API_KEY'] = \"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\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bec17cd2-00e7-4517-817e-8d29f1acb536",
   "metadata": {},
   "source": [
    "# 1. Chunk optimization\n",
    "https://www.youtube.com/watch?v=8OJC21T2SL4\n",
    "- 1.1. Character Split\n",
    "- 1.2. Recursive Character Split\n",
    "- 1.3. Document Specific Splitting\n",
    "- 1.4. Semantic Splitting (With Embeddings)"
   ]
  },
  {
   "cell_type": "raw",
   "id": "a812ea45-d7f5-44d1-877d-da6965d162ed",
   "metadata": {},
   "source": [
    "# 1.1. Character Split\n",
    "# chia tach theo ki tu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "4ec601e3-ce50-4da4-9ba8-d0f2a3428a43",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(page_content='Madam Speaker, Madam Vice President, our First Lady and Second Gentleman. Members of Congress and th'),\n",
       " Document(page_content='e Cabinet. Justices of the Supreme Court. My fellow Americans.  \\n\\nLast year COVID-19 kept us apart. '),\n",
       " Document(page_content='This year we are finally together again. \\n\\nTonight, we meet as Democrats Republicans and Independent'),\n",
       " Document(page_content='s. But most importantly as Americans. \\n\\nWith a duty to one another to the American people to the Con'),\n",
       " Document(page_content='stitution. \\n\\nAnd with an unwavering resolve that freedom will always triumph over tyranny. \\n\\nSix day')]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_text_splitters import CharacterTextSplitter\n",
    "\n",
    "with open(r\"F:\\CMC\\CMC_Study\\Code\\data\\state_of_the_union.txt\", encoding='utf-8') as f:\n",
    "    text = f.read()\n",
    "text_splitter = CharacterTextSplitter(chunk_size=100, chunk_overlap=0, separator=\"\", strip_whitespace=False)\n",
    "\n",
    "text = text_splitter.create_documents([text])\n",
    "text[0:5]"
   ]
  },
  {
   "cell_type": "raw",
   "id": "59511f51-91ff-4b9e-b92d-84816c13999d",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "raw",
   "id": "edbfbe7f-f821-4368-af0a-67993b07bd8c",
   "metadata": {},
   "source": [
    "# 1.2. Recursive Character Split\n",
    "# chia theo ki tu + tách đoạn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "6f6101e7-f226-4c9b-bf01-971360b30614",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Madam Speaker, Madam Vice President, our First Lady and Second Gentleman. Members of Congress and the Cabinet. Justices of the Supreme Court. My fellow Americans.  \n",
      "\n",
      "Last year COVID-19 kept us apart. This year we are finally together again. \n",
      "\n",
      "Tonight, we meet as Democrats Republicans and Independents. But most importantly as Americans. \n",
      "\n",
      "With a duty to one another to the American people to the Constitution. \n",
      "\n",
      "And with an unwavering resolve that freedom will always triumph over tyranny. \n",
      "\n",
      "Six day\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[Document(page_content='Madam Speaker, Madam Vice President, our First Lady and Second Gentleman. Members of Congress and the Cabinet. Justices of the Supreme Court. My'),\n",
       " Document(page_content='fellow Americans.'),\n",
       " Document(page_content='Last year COVID-19 kept us apart. This year we are finally together again.'),\n",
       " Document(page_content='Tonight, we meet as Democrats Republicans and Independents. But most importantly as Americans.'),\n",
       " Document(page_content='With a duty to one another to the American people to the Constitution.')]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "\n",
    "# Read the text from the file\n",
    "with open(r\"F:\\CMC\\CMC_Study\\Code\\data\\state_of_the_union.txt\", encoding='utf-8') as f:\n",
    "    text = f.read()\n",
    "print(text[:500])\n",
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=150, chunk_overlap=0)\n",
    "\n",
    "text = text_splitter.create_documents([text])\n",
    "text[0:5]"
   ]
  },
  {
   "cell_type": "raw",
   "id": "62d1f318-3979-4a8f-84cf-fa09c6dc555b",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "raw",
   "id": "2c24d1fc-9c62-4f5b-9350-d110c8f07115",
   "metadata": {},
   "source": [
    "# 1.3 Document Specific Splitting\n",
    "- MarkdownTextSpliter\n",
    "- PythonCodeTextSpliter\n",
    "- js: RecursiveCharacterTextSplitter.from_language(language=Language.JS, chunk_size...)\n",
    "- pdf: parition_pdf | elements_to_json"
   ]
  },
  {
   "cell_type": "raw",
   "id": "b1bc2db2-3ac8-40f4-8515-777621585b91",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "raw",
   "id": "8305ecf0-a788-401a-9cfe-e9e8d45b2964",
   "metadata": {},
   "source": [
    "# 1.4. Semantic Splitting (With Embeddings)\n",
    "- ebed | cosi,  dựa trên ngữ nghĩa giữa 2 phần để gộp thành 1 chunk hay tách"
   ]
  },
  {
   "cell_type": "raw",
   "id": "854539d1-9b46-4402-aa9d-8627a002671a",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "raw",
   "id": "0a2c645a-36e2-4b50-8878-e28630f62640",
   "metadata": {},
   "source": [
    "# 1.5. Agentic Splitting\n",
    "obj = hub.pull(\"wfh/proposal-indexing\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b323fa0-99ae-4992-ab77-cec6ad31ae8d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "raw",
   "id": "9aefd170-675f-41b5-8c2f-491ba4659ccd",
   "metadata": {},
   "source": [
    "################### RecursiveCharacterTextSplitter #########################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "54dfe16e-3b3a-4eec-bddf-482908094fe1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import shutil\n",
    "\n",
    "from langchain.document_loaders.pdf import PyPDFDirectoryLoader\n",
    "from langchain.schema.document import Document\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "from langchain.vectorstores.chroma import Chroma\n",
    "from langchain_community.embeddings.ollama import OllamaEmbeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "266b71d6-f45c-4d93-bd45-732cbd21fb74",
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_PATH = \"data\"\n",
    "CHROMA_PATH = \"chroma\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "94d372fa-3392-4ca7-8021-5e0aaec770be",
   "metadata": {},
   "outputs": [],
   "source": [
    "# embed\n",
    "embeddings = OllamaEmbeddings(model=\"llama3\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "04c49a8c-1f1e-4b79-82d1-888686a66db4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load data\n",
    "document_loader = PyPDFDirectoryLoader(DATA_PATH)\n",
    "docs = document_loader.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "53cde8b1-8cd0-4eac-989c-53bd0fd81b29",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Spliter\n",
    "def split_documents(documents: list[Document]):\n",
    "    text_splitter = RecursiveCharacterTextSplitter(\n",
    "        chunk_size=800,\n",
    "        chunk_overlap=80,\n",
    "        length_function=len,\n",
    "        is_separator_regex=False,\n",
    "    )\n",
    "    return text_splitter.split_documents(documents)\n",
    "\n",
    "chunks = split_documents(docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4233362-d2f8-4ace-8626-88b22d07368c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#----------------------------------------------------------------------------------------------------\n",
    "# Add Index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "623ce6a5-3e69-40ce-abe7-0a48ed4cf9f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lập chỉ mục\n",
    "def add_to_chroma(chunks: list[Document]):\n",
    "    # Load the existing database.\n",
    "    db = Chroma(\n",
    "        persist_directory=CHROMA_PATH, embedding_function=embeddings\n",
    "    )\n",
    "\n",
    "    # Calculate Page IDs.\n",
    "    chunks_with_ids = calculate_chunk_ids(chunks)\n",
    "\n",
    "    # Add or Update the documents.\n",
    "    existing_items = db.get(include=[])  # IDs are always included by default\n",
    "    existing_ids = set(existing_items[\"ids\"])\n",
    "    \n",
    "    print(existing_ids)\n",
    "    print(f\"Number of existing documents in DB: {len(existing_ids)}\")\n",
    "\n",
    "    # Only add documents that don't exist in the DB.\n",
    "    new_chunks = []\n",
    "    for chunk in chunks_with_ids:\n",
    "        if chunk.metadata[\"id\"] not in existing_ids:\n",
    "            new_chunks.append(chunk)\n",
    "    print(new_chunks)\n",
    "    if len(new_chunks):\n",
    "        print(f\"Adding new documents: {len(new_chunks)}\")\n",
    "        new_chunk_ids = [chunk.metadata[\"id\"] for chunk in new_chunks]\n",
    "        db.add_documents(new_chunks, ids=new_chunk_ids)\n",
    "        db.persist()\n",
    "    else:\n",
    "        print(\"No new documents to add\")\n",
    "    print(f\"Adding new documents: Success\")   \n",
    "\n",
    "def calculate_chunk_ids(chunks):\n",
    "    # This will create IDs like \"data/filename.pdf:6:2\"\n",
    "    # Page Source : Page Number : Chunk Index\n",
    "\n",
    "    last_page_id = None\n",
    "    current_chunk_index = 0\n",
    "\n",
    "    for chunk in chunks:\n",
    "        source = chunk.metadata.get(\"source\")\n",
    "        page = chunk.metadata.get(\"page\")\n",
    "        current_page_id = f\"{source}:{page}\"\n",
    "\n",
    "        # If the page ID is the same as the last one, increment the index.\n",
    "        if current_page_id == last_page_id:\n",
    "            current_chunk_index += 1\n",
    "        else:\n",
    "            current_chunk_index = 0\n",
    "\n",
    "        # Calculate the chunk ID.\n",
    "        chunk_id = f\"{current_page_id}:{current_chunk_index}\"\n",
    "        last_page_id = current_page_id\n",
    "\n",
    "        # Add it to the page meta-data.\n",
    "        chunk.metadata[\"id\"] = chunk_id\n",
    "    return chunks\n",
    "    \n",
    "def clear_database():\n",
    "    if os.path.exists(CHROMA_PATH):\n",
    "        shutil.rmtree(CHROMA_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a3661f6-ce34-4aee-aa07-78ddc34abd08",
   "metadata": {},
   "outputs": [],
   "source": [
    "add_to_chroma(chunks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a2c8406-93e1-418d-a1d5-86ee8f817786",
   "metadata": {},
   "outputs": [],
   "source": [
    "#----------------------------------------------------------------------------------------------------\n",
    "# query 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e258bc5c-70da-469b-84d4-6c4e38b47113",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.prompts import ChatPromptTemplate\n",
    "from langchain_community.llms.ollama import Ollama\n",
    "\n",
    "\n",
    "PROMPT_TEMPLATE = \"\"\"\n",
    "Answer the question based only on the following context:{context}\n",
    "---\n",
    "Answer the question based on the above context: {question}\n",
    "\"\"\"\n",
    "def query_rag(query_text: str):\n",
    "    \n",
    "    # embed\n",
    "    embedding_function = embeddings\n",
    "    \n",
    "    # Prepare the DB.\n",
    "    db = Chroma(persist_directory=CHROMA_PATH, embedding_function=embedding_function)\n",
    "\n",
    "    # Search the DB.\n",
    "    results = db.similarity_search_with_score(query_text, k=5)\n",
    "\n",
    "    context_text = \"\\n\\n---\\n\\n\".join([doc.page_content for doc, _score in results])\n",
    "    prompt_template = ChatPromptTemplate.from_template(PROMPT_TEMPLATE)\n",
    "    prompt = prompt_template.format(context=context_text, question=query_text)\n",
    "    # print(prompt)\n",
    "\n",
    "    model = Ollama(model=\"llama3\")\n",
    "    response_text = model.invoke(prompt)\n",
    "\n",
    "    sources = [doc.metadata.get(\"id\", None) for doc, _score in results]\n",
    "    formatted_response = f\"Response: {response_text}\\nSources: {sources}\"\n",
    "    print(formatted_response)\n",
    "    return response_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "38b64fe1-7da2-4593-9d13-f224169704ce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Query:  what is the dataset\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Response: According to the provided context, the dataset used contains a diverse range of fire and smoke scenarios, including indoor and outdoor fires, small and large fires, low-light and high-light conditions, and normal scenes without fire. The dataset consists of 26,520 images, divided into 21,216 for training and 5,304 for testing.\n",
      "Sources: ['data\\\\yolov8.pdf:10:0', 'data\\\\yolov8.pdf:13:2', 'data\\\\yolov8.pdf:11:2', 'data\\\\yolov8.pdf:15:0', 'data\\\\yolov8.pdf:7:3']\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Query:  q\n"
     ]
    }
   ],
   "source": [
    "query = input(\"Query: \")\n",
    "while query != \"q\":\n",
    "    query_rag(query_text=query)\n",
    "    query = input(\"Query: \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ed9f79d-8759-4fef-bf94-4c9ca3e27d04",
   "metadata": {},
   "outputs": [],
   "source": [
    "#----------------------------------------------------------------------------------------------------\n",
    "# query 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "559e368f-328a-47ee-bf2c-222aa067dabd",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Query:  what is the dataset?\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "context: \n",
      "page_content='tative of the overall dataset. Other pre-processing steps,\\nsuch as resizing or normalizing the data, may also benecessary. The goal is to have a large enough, balanced\\ndataset that can generalize well to new data.\\nStep 3: Model selection This step involves selecting the\\nappropriate object detection algorithm for training the ﬁre\\ndetection model. There are several algorithms to choose\\nfrom, such as YOLOv8, Faster R-CNN, and SSD, eachwith its own advantages and disadvantages. The selected\\nalgorithm should have good performance on the collected\\ndataset and be capable of handling different ﬁre scenarios,depending on the requirements of the smart ﬁre detection\\nsystem. YOLOv8 is a popular choice due to its speed andaccuracy, but other algorithms can also be used based on\\nspeciﬁc needs.' metadata={'id': 'data\\\\yolov8.pdf:7:1', 'page': 7, 'source': 'data\\\\yolov8.pdf'}\n",
      "\n",
      "page_content='costs associated with false alarms.\\n•Cost-effective: The proposed approach may be cost-\\neffective compared to traditional ﬁre detection methods\\nas it can be implemented using low-cost cameras and\\nhardware, reducing the need for expensive ﬁre detectionsystems.\\n•Large dataset: Unlike other methods that use small\\nnumber of datasets, a large dataset containing ﬁre,smoke, and normal scenes is used. The dataset has real-\\nworld images and videos collected from various\\nsources. The dataset has a diverse range of ﬁrescenarios, including indoor and outdoor ﬁres, small\\nand large ﬁres, and low-light and high-light conditions.\\nA deep CNN gathers essential data from big datasets toproduce precise predictions and reduce overﬁtting.\\nThe following is how the remaining work is structured.' metadata={'id': 'data\\\\yolov8.pdf:1:5', 'page': 1, 'source': 'data\\\\yolov8.pdf'}\n",
      "\n",
      "page_content='detected objects highlighted. The SFD algorithm is a\\npowerful tool for detecting ﬁres in real-time and enablesquick and effective responses to potential ﬁre hazards.The smart ﬁre detection (SFD) uses Yolov8 object\\ndetection model. It involves several steps:\\n•The video input source is set up from either a live\\ncamera or pre-recorded video ﬁle.\\n•The video capture process is started, and each frame of\\nthe video is looped through.\\n•Image pre-processing techniques are applied to each\\nframe, and the pre-processed frame is passed to the\\nYolov8 model for object detection.\\n•The detected objects are checked for ﬁre-related\\nclasses, such as ‘‘ﬂames’’, ‘‘smoke’’, or ‘‘embers’’.\\n•If a ﬁre-related class is detected, an alarm is triggered,\\nand relevant authorities are notiﬁed.' metadata={'id': 'data\\\\yolov8.pdf:9:1', 'page': 9, 'source': 'data\\\\yolov8.pdf'}\n",
      "\n",
      "\n",
      "Answer:  According to the text, the dataset used in this approach is a large dataset containing fire, smoke, and normal scenes. This dataset has real-world images and videos collected from various sources and includes a diverse range of fire scenarios, including indoor and outdoor fires, small and large fires, and low-light and high-light conditions.\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Query:  q\n"
     ]
    }
   ],
   "source": [
    "from langchain.chains import create_retrieval_chain\n",
    "from langchain_community.embeddings.ollama import OllamaEmbeddings\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain.chains.combine_documents import create_stuff_documents_chain\n",
    "from langchain.vectorstores.chroma import Chroma\n",
    "from langchain_community.llms.ollama import Ollama\n",
    "\n",
    "from langchain_core.prompts import MessagesPlaceholder\n",
    "from langchain_core.messages import HumanMessage, AIMessage\n",
    "from langchain.chains.history_aware_retriever import create_history_aware_retriever\n",
    "\n",
    "CHROMA_PATH = \"chroma\"\n",
    "DATA_PATH = \"data\"\n",
    "\n",
    "embeddings = OllamaEmbeddings(model=\"llama3\")\n",
    "\n",
    "db = Chroma(persist_directory=CHROMA_PATH, embedding_function=embeddings)\n",
    "\n",
    "model = Ollama(model=\"llama3\")\n",
    "prompt = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", \"Answer the user's questions based on the context: {context}\"),\n",
    "    MessagesPlaceholder(variable_name=\"chat_history\"),\n",
    "    (\"user\", \"{input}\")\n",
    "])\n",
    "\n",
    "document_chain = create_stuff_documents_chain(llm=model, prompt=prompt)\n",
    "\n",
    "retriever = db.as_retriever(search_kwargs={\"k\": 3})\n",
    "\n",
    "retriever_prompt = ChatPromptTemplate.from_messages([\n",
    "    (\"user\", \"{input}\"),\n",
    "    MessagesPlaceholder(variable_name=\"chat_history\"),\n",
    "    (\"user\", \"Given the above conversation, generate a search query to look up in order to get information relevant to the conversation\")\n",
    "])\n",
    "\n",
    "history_aware_retriever = create_history_aware_retriever(\n",
    "    llm=model,\n",
    "    prompt=retriever_prompt,\n",
    "    retriever=retriever)\n",
    "\n",
    "retriever_chain = create_retrieval_chain(history_aware_retriever, document_chain)\n",
    "\n",
    "\n",
    "def process_chat(chain, question, chat_history):\n",
    "    response = chain.invoke({\n",
    "        \"chat_history\": chat_history,\n",
    "        \"input\": question,\n",
    "    })\n",
    "    return response\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    query = input(\"Query: \")\n",
    "    chat_history = []\n",
    "    while query != \"q\":\n",
    "        # docs_relevant = retriever.get_relevant_documents(query)\n",
    "        # print(docs_relevant)\n",
    "        ans = process_chat(chain=retriever_chain, question=query, chat_history=chat_history)\n",
    "        chat_history.append(HumanMessage(content=query))\n",
    "        chat_history.append(AIMessage(content=ans[\"answer\"]))\n",
    "        print(\"context: \")\n",
    "        for docs in ans[\"context\"]:\n",
    "            print(docs)\n",
    "            print()\n",
    "        print('\\nAnswer: ', ans[\"answer\"])\n",
    "        query = input(\"Query: \")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e710d3bf-5060-41d3-abbf-36bbf3b00a9d",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# 2. Multi-representation Indexing\n",
    "- Smaller chunks -- ParentDocumentRetriever -- Chia tài liệu thành các thành phần nhỏ hơn - tạo các id duy nhất\n",
    "- Summary -- Tóm tắt nội dung tài liệu, nhúng bản tóm tắt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "17890036-26ec-4afd-b78e-aacffd5a422a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Smaller chunks\n",
    "from langchain.retrievers.multi_vector import MultiVectorRetriever\n",
    "from langchain.retrievers import ParentDocumentRetriever"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "73bc1625-cc18-43f9-b1f0-b0c0e3b8d736",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.storage import InMemoryByteStore\n",
    "from langchain_community.vectorstores.chroma import Chroma\n",
    "from langchain_community.document_loaders import TextLoader\n",
    "# from langchain_openai import OpenAIEmbeddings\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "75b64f97-cf50-4471-868e-90d00e9c8d7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.embeddings.ollama import OllamaEmbeddings\n",
    "from langchain_community.llms.ollama import Ollama\n",
    "\n",
    "embeddings = OllamaEmbeddings(model=\"llama3\")\n",
    "model = Ollama(model=\"llama3\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "5e382bea-f1fc-44b8-a9a1-dd07c957a7e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "loaders = [\n",
    "    # TextLoader(r\"F:\\CMC\\CMC_Study\\Code\\data\\paul_graham_essays.txt\"),\n",
    "    TextLoader(r\"F:\\CMC\\CMC_Study\\Code\\data\\state_of_the_union.txt\", encoding=\"utf-8\"),\n",
    "]\n",
    "docs = []\n",
    "for loader in loaders:\n",
    "    docs.extend(loader.load())\n",
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=5000)\n",
    "docs = text_splitter.split_documents(docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "7115f3f0-e0a9-42e8-826d-fdc296b1b035",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "36a1cbc9-a170-473a-97f4-6dddc0b21fbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# The vectorstore to use to index the child chunks\n",
    "vectorstore = Chroma(\n",
    "    collection_name=\"full_documents\", embedding_function=embeddings\n",
    ")\n",
    "# The storage layer for the parent documents\n",
    "store = InMemoryByteStore()\n",
    "id_key = \"doc_id\"\n",
    "# The retriever (empty to start)\n",
    "retriever = MultiVectorRetriever(\n",
    "    vectorstore=vectorstore,\n",
    "    byte_store=store,\n",
    "    id_key=id_key,\n",
    ")\n",
    "import uuid   # tao id duy nhat cho tung document\n",
    "doc_ids = [str(uuid.uuid4()) for _ in docs]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "67b311bb-80fe-42bd-a64f-19e5fd374637",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The splitter to use to create smaller chunks\n",
    "child_text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "3866f3fa-3433-4141-9948-5340d06fb394",
   "metadata": {},
   "outputs": [],
   "source": [
    "# chia thành các document nhỏ hơn, thêm doc_ids cha vào trong các documents con\n",
    "sub_docs = []\n",
    "for i, doc in enumerate(docs):\n",
    "    _id = doc_ids[i]  # doc_ids của các document parent\n",
    "    _sub_docs = child_text_splitter.split_documents([doc])\n",
    "    for _doc in _sub_docs:\n",
    "        _doc.metadata[id_key] = _id\n",
    "    sub_docs.extend(_sub_docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "6944fa9f-4f6c-4ab8-a0e8-2b62c7e4aeab",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(page_content='Madam Speaker, Madam Vice President, our First Lady and Second Gentleman. Members of Congress and the Cabinet. Justices of the Supreme Court. My fellow Americans.  \\n\\nLast year COVID-19 kept us apart. This year we are finally together again. \\n\\nTonight, we meet as Democrats Republicans and Independents. But most importantly as Americans. \\n\\nWith a duty to one another to the American people to the Constitution. \\n\\nAnd with an unwavering resolve that freedom will always triumph over tyranny. \\n\\nSix days ago, Russia’s Vladimir Putin sought to shake the foundations of the free world thinking he could make it bend to his menacing ways. But he badly miscalculated. \\n\\nHe thought he could roll into Ukraine and the world would roll over. Instead he met a wall of strength he never imagined. \\n\\nHe met the Ukrainian people. \\n\\nFrom President Zelenskyy to every Ukrainian, their fearlessness, their courage, their determination, inspires the world.', metadata={'source': 'F:\\\\CMC\\\\CMC_Study\\\\Code\\\\data\\\\state_of_the_union.txt', 'doc_id': 'f6247fe3-25e2-4444-aa1a-61f46b87e6a6'}),\n",
       " Document(page_content='He met the Ukrainian people. \\n\\nFrom President Zelenskyy to every Ukrainian, their fearlessness, their courage, their determination, inspires the world. \\n\\nGroups of citizens blocking tanks with their bodies. Everyone from students to retirees teachers turned soldiers defending their homeland. \\n\\nIn this struggle as President Zelenskyy said in his speech to the European Parliament “Light will win over darkness.” The Ukrainian Ambassador to the United States is here tonight. \\n\\nLet each of us here tonight in this Chamber send an unmistakable signal to Ukraine and to the world. \\n\\nPlease rise if you are able and show that, Yes, we the United States of America stand with the Ukrainian people. \\n\\nThroughout our history we’ve learned this lesson when dictators do not pay a price for their aggression they cause more chaos.   \\n\\nThey keep moving.   \\n\\nAnd the costs and the threats to America and the world keep rising.', metadata={'source': 'F:\\\\CMC\\\\CMC_Study\\\\Code\\\\data\\\\state_of_the_union.txt', 'doc_id': 'f6247fe3-25e2-4444-aa1a-61f46b87e6a6'}),\n",
       " Document(page_content='They keep moving.   \\n\\nAnd the costs and the threats to America and the world keep rising.   \\n\\nThat’s why the NATO Alliance was created to secure peace and stability in Europe after World War 2. \\n\\nThe United States is a member along with 29 other nations. \\n\\nIt matters. American diplomacy matters. American resolve matters. \\n\\nPutin’s latest attack on Ukraine was premeditated and unprovoked. \\n\\nHe rejected repeated efforts at diplomacy. \\n\\nHe thought the West and NATO wouldn’t respond. And he thought he could divide us at home. Putin was wrong. We were ready.  Here is what we did.   \\n\\nWe prepared extensively and carefully. \\n\\nWe spent months building a coalition of other freedom-loving nations from Europe and the Americas to Asia and Africa to confront Putin. \\n\\nI spent countless hours unifying our European allies. We shared with the world in advance what we knew Putin was planning and precisely how he would try to falsely justify his aggression.  \\n\\nWe countered Russia’s lies with truth.', metadata={'source': 'F:\\\\CMC\\\\CMC_Study\\\\Code\\\\data\\\\state_of_the_union.txt', 'doc_id': 'f6247fe3-25e2-4444-aa1a-61f46b87e6a6'}),\n",
       " Document(page_content='We countered Russia’s lies with truth.   \\n\\nAnd now that he has acted the free world is holding him accountable. \\n\\nAlong with twenty-seven members of the European Union including France, Germany, Italy, as well as countries like the United Kingdom, Canada, Japan, Korea, Australia, New Zealand, and many others, even Switzerland. \\n\\nWe are inflicting pain on Russia and supporting the people of Ukraine. Putin is now isolated from the world more than ever. \\n\\nTogether with our allies –we are right now enforcing powerful economic sanctions. \\n\\nWe are cutting off Russia’s largest banks from the international financial system.  \\n\\nPreventing Russia’s central bank from defending the Russian Ruble making Putin’s $630 Billion “war fund” worthless.   \\n\\nWe are choking off Russia’s access to technology that will sap its economic strength and weaken its military for years to come.', metadata={'source': 'F:\\\\CMC\\\\CMC_Study\\\\Code\\\\data\\\\state_of_the_union.txt', 'doc_id': 'f6247fe3-25e2-4444-aa1a-61f46b87e6a6'}),\n",
       " Document(page_content='We are choking off Russia’s access to technology that will sap its economic strength and weaken its military for years to come.  \\n\\nTonight I say to the Russian oligarchs and corrupt leaders who have bilked billions of dollars off this violent regime no more. \\n\\nThe U.S. Department of Justice is assembling a dedicated task force to go after the crimes of Russian oligarchs.  \\n\\nWe are joining with our European allies to find and seize your yachts your luxury apartments your private jets. We are coming for your ill-begotten gains. \\n\\nAnd tonight I am announcing that we will join our allies in closing off American air space to all Russian flights – further isolating Russia – and adding an additional squeeze –on their economy. The Ruble has lost 30% of its value. \\n\\nThe Russian stock market has lost 40% of its value and trading remains suspended. Russia’s economy is reeling and Putin alone is to blame.', metadata={'source': 'F:\\\\CMC\\\\CMC_Study\\\\Code\\\\data\\\\state_of_the_union.txt', 'doc_id': 'f6247fe3-25e2-4444-aa1a-61f46b87e6a6'}),\n",
       " Document(page_content='The Russian stock market has lost 40% of its value and trading remains suspended. Russia’s economy is reeling and Putin alone is to blame. \\n\\nTogether with our allies we are providing support to the Ukrainians in their fight for freedom. Military assistance. Economic assistance. Humanitarian assistance. \\n\\nWe are giving more than $1 Billion in direct assistance to Ukraine. \\n\\nAnd we will continue to aid the Ukrainian people as they defend their country and to help ease their suffering.  \\n\\nLet me be clear, our forces are not engaged and will not engage in conflict with Russian forces in Ukraine.  \\n\\nOur forces are not going to Europe to fight in Ukraine, but to defend our NATO Allies – in the event that Putin decides to keep moving west.', metadata={'source': 'F:\\\\CMC\\\\CMC_Study\\\\Code\\\\data\\\\state_of_the_union.txt', 'doc_id': 'f6247fe3-25e2-4444-aa1a-61f46b87e6a6'}),\n",
       " Document(page_content='Our forces are not going to Europe to fight in Ukraine, but to defend our NATO Allies – in the event that Putin decides to keep moving west.  \\n\\nFor that purpose we’ve mobilized American ground forces, air squadrons, and ship deployments to protect NATO countries including Poland, Romania, Latvia, Lithuania, and Estonia. \\n\\nAs I have made crystal clear the United States and our Allies will defend every inch of territory of NATO countries with the full force of our collective power.  \\n\\nAnd we remain clear-eyed. The Ukrainians are fighting back with pure courage. But the next few days weeks, months, will be hard on them.  \\n\\nPutin has unleashed violence and chaos.  But while he may make gains on the battlefield – he will pay a continuing high price over the long run. \\n\\nAnd a proud Ukrainian people, who have known 30 years  of independence, have repeatedly shown that they will not tolerate anyone who tries to take their country backwards.', metadata={'source': 'F:\\\\CMC\\\\CMC_Study\\\\Code\\\\data\\\\state_of_the_union.txt', 'doc_id': 'bb9690b4-ac92-4ec6-b2ab-9453faf6656f'}),\n",
       " Document(page_content='And a proud Ukrainian people, who have known 30 years  of independence, have repeatedly shown that they will not tolerate anyone who tries to take their country backwards.  \\n\\nTo all Americans, I will be honest with you, as I’ve always promised. A Russian dictator, invading a foreign country, has costs around the world. \\n\\nAnd I’m taking robust action to make sure the pain of our sanctions  is targeted at Russia’s economy. And I will use every tool at our disposal to protect American businesses and consumers. \\n\\nTonight, I can announce that the United States has worked with 30 other countries to release 60 Million barrels of oil from reserves around the world.  \\n\\nAmerica will lead that effort, releasing 30 Million barrels from our own Strategic Petroleum Reserve. And we stand ready to do more if necessary, unified with our allies.  \\n\\nThese steps will help blunt gas prices here at home. And I know the news about what’s happening can seem alarming.', metadata={'source': 'F:\\\\CMC\\\\CMC_Study\\\\Code\\\\data\\\\state_of_the_union.txt', 'doc_id': 'bb9690b4-ac92-4ec6-b2ab-9453faf6656f'}),\n",
       " Document(page_content='These steps will help blunt gas prices here at home. And I know the news about what’s happening can seem alarming. \\n\\nBut I want you to know that we are going to be okay. \\n\\nWhen the history of this era is written Putin’s war on Ukraine will have left Russia weaker and the rest of the world stronger. \\n\\nWhile it shouldn’t have taken something so terrible for people around the world to see what’s at stake now everyone sees it clearly. \\n\\nWe see the unity among leaders of nations and a more unified Europe a more unified West. And we see unity among the people who are gathering in cities in large crowds around the world even in Russia to demonstrate their support for Ukraine.  \\n\\nIn the battle between democracy and autocracy, democracies are rising to the moment, and the world is clearly choosing the side of peace and security. \\n\\nThis is a real test. It’s going to take time. So let us continue to draw inspiration from the iron will of the Ukrainian people.', metadata={'source': 'F:\\\\CMC\\\\CMC_Study\\\\Code\\\\data\\\\state_of_the_union.txt', 'doc_id': 'bb9690b4-ac92-4ec6-b2ab-9453faf6656f'}),\n",
       " Document(page_content='This is a real test. It’s going to take time. So let us continue to draw inspiration from the iron will of the Ukrainian people. \\n\\nTo our fellow Ukrainian Americans who forge a deep bond that connects our two nations we stand with you. \\n\\nPutin may circle Kyiv with tanks, but he will never gain the hearts and souls of the Ukrainian people. \\n\\nHe will never extinguish their love of freedom. He will never weaken the resolve of the free world. \\n\\nWe meet tonight in an America that has lived through two of the hardest years this nation has ever faced. \\n\\nThe pandemic has been punishing. \\n\\nAnd so many families are living paycheck to paycheck, struggling to keep up with the rising cost of food, gas, housing, and so much more. \\n\\nI understand. \\n\\nI remember when my Dad had to leave our home in Scranton, Pennsylvania to find work. I grew up in a family where if the price of food went up, you felt it.', metadata={'source': 'F:\\\\CMC\\\\CMC_Study\\\\Code\\\\data\\\\state_of_the_union.txt', 'doc_id': 'bb9690b4-ac92-4ec6-b2ab-9453faf6656f'}),\n",
       " Document(page_content='I understand. \\n\\nI remember when my Dad had to leave our home in Scranton, Pennsylvania to find work. I grew up in a family where if the price of food went up, you felt it. \\n\\nThat’s why one of the first things I did as President was fight to pass the American Rescue Plan.  \\n\\nBecause people were hurting. We needed to act, and we did. \\n\\nFew pieces of legislation have done more in a critical moment in our history to lift us out of crisis. \\n\\nIt fueled our efforts to vaccinate the nation and combat COVID-19. It delivered immediate economic relief for tens of millions of Americans.  \\n\\nHelped put food on their table, keep a roof over their heads, and cut the cost of health insurance. \\n\\nAnd as my Dad used to say, it gave people a little breathing room. \\n\\nAnd unlike the $2 Trillion tax cut passed in the previous administration that benefitted the top 1% of Americans, the American Rescue Plan helped working people—and left no one behind. \\n\\nAnd it worked. It created jobs. Lots of jobs.', metadata={'source': 'F:\\\\CMC\\\\CMC_Study\\\\Code\\\\data\\\\state_of_the_union.txt', 'doc_id': 'bb9690b4-ac92-4ec6-b2ab-9453faf6656f'}),\n",
       " Document(page_content='And it worked. It created jobs. Lots of jobs. \\n\\nIn fact—our economy created over 6.5 Million new jobs just last year, more jobs created in one year  \\nthan ever before in the history of America. \\n\\nOur economy grew at a rate of 5.7% last year, the strongest growth in nearly 40 years, the first step in bringing fundamental change to an economy that hasn’t worked for the working people of this nation for too long.  \\n\\nFor the past 40 years we were told that if we gave tax breaks to those at the very top, the benefits would trickle down to everyone else. \\n\\nBut that trickle-down theory led to weaker economic growth, lower wages, bigger deficits, and the widest gap between those at the top and everyone else in nearly a century. \\n\\nVice President Harris and I ran for office with a new economic vision for America.', metadata={'source': 'F:\\\\CMC\\\\CMC_Study\\\\Code\\\\data\\\\state_of_the_union.txt', 'doc_id': 'bb9690b4-ac92-4ec6-b2ab-9453faf6656f'}),\n",
       " Document(page_content='Vice President Harris and I ran for office with a new economic vision for America. \\n\\nInvest in America. Educate Americans. Grow the workforce. Build the economy from the bottom up  \\nand the middle out, not from the top down.  \\n\\nBecause we know that when the middle class grows, the poor have a ladder up and the wealthy do very well. \\n\\nAmerica used to have the best roads, bridges, and airports on Earth. \\n\\nNow our infrastructure is ranked 13th in the world. \\n\\nWe won’t be able to compete for the jobs of the 21st Century if we don’t fix that. \\n\\nThat’s why it was so important to pass the Bipartisan Infrastructure Law—the most sweeping investment to rebuild America in history. \\n\\nThis was a bipartisan effort, and I want to thank the members of both parties who worked to make it happen. \\n\\nWe’re done talking about infrastructure weeks. \\n\\nWe’re going to have an infrastructure decade.', metadata={'source': 'F:\\\\CMC\\\\CMC_Study\\\\Code\\\\data\\\\state_of_the_union.txt', 'doc_id': '0301892c-1859-467b-8ce8-0e8367999d3a'}),\n",
       " Document(page_content='We’re done talking about infrastructure weeks. \\n\\nWe’re going to have an infrastructure decade. \\n\\nIt is going to transform America and put us on a path to win the economic competition of the 21st Century that we face with the rest of the world—particularly with China.  \\n\\nAs I’ve told Xi Jinping, it is never a good bet to bet against the American people. \\n\\nWe’ll create good jobs for millions of Americans, modernizing roads, airports, ports, and waterways all across America. \\n\\nAnd we’ll do it all to withstand the devastating effects of the climate crisis and promote environmental justice. \\n\\nWe’ll build a national network of 500,000 electric vehicle charging stations, begin to replace poisonous lead pipes—so every child—and every American—has clean water to drink at home and at school, provide affordable high-speed internet for every American—urban, suburban, rural, and tribal communities. \\n\\n4,000 projects have already been announced.', metadata={'source': 'F:\\\\CMC\\\\CMC_Study\\\\Code\\\\data\\\\state_of_the_union.txt', 'doc_id': '0301892c-1859-467b-8ce8-0e8367999d3a'}),\n",
       " Document(page_content='4,000 projects have already been announced. \\n\\nAnd tonight, I’m announcing that this year we will start fixing over 65,000 miles of highway and 1,500 bridges in disrepair. \\n\\nWhen we use taxpayer dollars to rebuild America – we are going to Buy American: buy American products to support American jobs. \\n\\nThe federal government spends about $600 Billion a year to keep the country safe and secure. \\n\\nThere’s been a law on the books for almost a century \\nto make sure taxpayers’ dollars support American jobs and businesses. \\n\\nEvery Administration says they’ll do it, but we are actually doing it. \\n\\nWe will buy American to make sure everything from the deck of an aircraft carrier to the steel on highway guardrails are made in America. \\n\\nBut to compete for the best jobs of the future, we also need to level the playing field with China and other competitors.', metadata={'source': 'F:\\\\CMC\\\\CMC_Study\\\\Code\\\\data\\\\state_of_the_union.txt', 'doc_id': '0301892c-1859-467b-8ce8-0e8367999d3a'}),\n",
       " Document(page_content='But to compete for the best jobs of the future, we also need to level the playing field with China and other competitors. \\n\\nThat’s why it is so important to pass the Bipartisan Innovation Act sitting in Congress that will make record investments in emerging technologies and American manufacturing. \\n\\nLet me give you one example of why it’s so important to pass it. \\n\\nIf you travel 20 miles east of Columbus, Ohio, you’ll find 1,000 empty acres of land. \\n\\nIt won’t look like much, but if you stop and look closely, you’ll see a “Field of dreams,” the ground on which America’s future will be built. \\n\\nThis is where Intel, the American company that helped build Silicon Valley, is going to build its $20 billion semiconductor “mega site”. \\n\\nUp to eight state-of-the-art factories in one place. 10,000 new good-paying jobs. \\n\\nSome of the most sophisticated manufacturing in the world to make computer chips the size of a fingertip that power the world and our everyday lives.', metadata={'source': 'F:\\\\CMC\\\\CMC_Study\\\\Code\\\\data\\\\state_of_the_union.txt', 'doc_id': '0301892c-1859-467b-8ce8-0e8367999d3a'}),\n",
       " Document(page_content='Some of the most sophisticated manufacturing in the world to make computer chips the size of a fingertip that power the world and our everyday lives. \\n\\nSmartphones. The Internet. Technology we have yet to invent. \\n\\nBut that’s just the beginning. \\n\\nIntel’s CEO, Pat Gelsinger, who is here tonight, told me they are ready to increase their investment from  \\n$20 billion to $100 billion. \\n\\nThat would be one of the biggest investments in manufacturing in American history. \\n\\nAnd all they’re waiting for is for you to pass this bill. \\n\\nSo let’s not wait any longer. Send it to my desk. I’ll sign it.  \\n\\nAnd we will really take off. \\n\\nAnd Intel is not alone. \\n\\nThere’s something happening in America. \\n\\nJust look around and you’ll see an amazing story. \\n\\nThe rebirth of the pride that comes from stamping products “Made In America.” The revitalization of American manufacturing.   \\n\\nCompanies are choosing to build new factories here, when just a few years ago, they would have built them overseas.', metadata={'source': 'F:\\\\CMC\\\\CMC_Study\\\\Code\\\\data\\\\state_of_the_union.txt', 'doc_id': '0301892c-1859-467b-8ce8-0e8367999d3a'}),\n",
       " Document(page_content='Companies are choosing to build new factories here, when just a few years ago, they would have built them overseas. \\n\\nThat’s what is happening. Ford is investing $11 billion to build electric vehicles, creating 11,000 jobs across the country. \\n\\nGM is making the largest investment in its history—$7 billion to build electric vehicles, creating 4,000 jobs in Michigan. \\n\\nAll told, we created 369,000 new manufacturing jobs in America just last year. \\n\\nPowered by people I’ve met like JoJo Burgess, from generations of union steelworkers from Pittsburgh, who’s here with us tonight. \\n\\nAs Ohio Senator Sherrod Brown says, “It’s time to bury the label “Rust Belt.” \\n\\nIt’s time. \\n\\nBut with all the bright spots in our economy, record job growth and higher wages, too many families are struggling to keep up with the bills.', metadata={'source': 'F:\\\\CMC\\\\CMC_Study\\\\Code\\\\data\\\\state_of_the_union.txt', 'doc_id': '0301892c-1859-467b-8ce8-0e8367999d3a'}),\n",
       " Document(page_content='It’s time. \\n\\nBut with all the bright spots in our economy, record job growth and higher wages, too many families are struggling to keep up with the bills.  \\n\\nInflation is robbing them of the gains they might otherwise feel. \\n\\nI get it. That’s why my top priority is getting prices under control. \\n\\nLook, our economy roared back faster than most predicted, but the pandemic meant that businesses had a hard time hiring enough workers to keep up production in their factories. \\n\\nThe pandemic also disrupted global supply chains. \\n\\nWhen factories close, it takes longer to make goods and get them from the warehouse to the store, and prices go up. \\n\\nLook at cars. \\n\\nLast year, there weren’t enough semiconductors to make all the cars that people wanted to buy. \\n\\nAnd guess what, prices of automobiles went up. \\n\\nSo—we have a choice. \\n\\nOne way to fight inflation is to drive down wages and make Americans poorer.  \\n\\nI have a better plan to fight inflation. \\n\\nLower your costs, not your wages.', metadata={'source': 'F:\\\\CMC\\\\CMC_Study\\\\Code\\\\data\\\\state_of_the_union.txt', 'doc_id': 'dd327e15-383a-45cb-87e4-41b333555360'}),\n",
       " Document(page_content='So—we have a choice. \\n\\nOne way to fight inflation is to drive down wages and make Americans poorer.  \\n\\nI have a better plan to fight inflation. \\n\\nLower your costs, not your wages. \\n\\nMake more cars and semiconductors in America. \\n\\nMore infrastructure and innovation in America. \\n\\nMore goods moving faster and cheaper in America. \\n\\nMore jobs where you can earn a good living in America. \\n\\nAnd instead of relying on foreign supply chains, let’s make it in America. \\n\\nEconomists call it “increasing the productive capacity of our economy.” \\n\\nI call it building a better America. \\n\\nMy plan to fight inflation will lower your costs and lower the deficit. \\n\\n17 Nobel laureates in economics say my plan will ease long-term inflationary pressures. Top business leaders and most Americans support my plan. And here’s the plan: \\n\\nFirst – cut the cost of prescription drugs. Just look at insulin. One in ten Americans has diabetes. In Virginia, I met a 13-year-old boy named Joshua Davis.', metadata={'source': 'F:\\\\CMC\\\\CMC_Study\\\\Code\\\\data\\\\state_of_the_union.txt', 'doc_id': 'dd327e15-383a-45cb-87e4-41b333555360'}),\n",
       " Document(page_content='First – cut the cost of prescription drugs. Just look at insulin. One in ten Americans has diabetes. In Virginia, I met a 13-year-old boy named Joshua Davis.  \\n\\nHe and his Dad both have Type 1 diabetes, which means they need insulin every day. Insulin costs about $10 a vial to make.  \\n\\nBut drug companies charge families like Joshua and his Dad up to 30 times more. I spoke with Joshua’s mom. \\n\\nImagine what it’s like to look at your child who needs insulin and have no idea how you’re going to pay for it.  \\n\\nWhat it does to your dignity, your ability to look your child in the eye, to be the parent you expect to be. \\n\\nJoshua is here with us tonight. Yesterday was his birthday. Happy birthday, buddy.  \\n\\nFor Joshua, and for the 200,000 other young people with Type 1 diabetes, let’s cap the cost of insulin at $35 a month so everyone can afford it.', metadata={'source': 'F:\\\\CMC\\\\CMC_Study\\\\Code\\\\data\\\\state_of_the_union.txt', 'doc_id': 'dd327e15-383a-45cb-87e4-41b333555360'}),\n",
       " Document(page_content='For Joshua, and for the 200,000 other young people with Type 1 diabetes, let’s cap the cost of insulin at $35 a month so everyone can afford it.  \\n\\nDrug companies will still do very well. And while we’re at it let Medicare negotiate lower prices for prescription drugs, like the VA already does. \\n\\nLook, the American Rescue Plan is helping millions of families on Affordable Care Act plans save $2,400 a year on their health care premiums. Let’s close the coverage gap and make those savings permanent. \\n\\nSecond – cut energy costs for families an average of $500 a year by combatting climate change.  \\n\\nLet’s provide investments and tax credits to weatherize your homes and businesses to be energy efficient and you get a tax credit; double America’s clean energy production in solar, wind, and so much more;  lower the price of electric vehicles, saving you another $80 a month because you’ll never have to pay at the gas pump again.', metadata={'source': 'F:\\\\CMC\\\\CMC_Study\\\\Code\\\\data\\\\state_of_the_union.txt', 'doc_id': 'dd327e15-383a-45cb-87e4-41b333555360'}),\n",
       " Document(page_content='Third – cut the cost of child care. Many families pay up to $14,000 a year for child care per child.  \\n\\nMiddle-class and working families shouldn’t have to pay more than 7% of their income for care of young children.  \\n\\nMy plan will cut the cost in half for most families and help parents, including millions of women, who left the workforce during the pandemic because they couldn’t afford child care, to be able to get back to work. \\n\\nMy plan doesn’t stop there. It also includes home and long-term care. More affordable housing. And Pre-K for every 3- and 4-year-old.  \\n\\nAll of these will lower costs. \\n\\nAnd under my plan, nobody earning less than $400,000 a year will pay an additional penny in new taxes. Nobody.  \\n\\nThe one thing all Americans agree on is that the tax system is not fair. We have to fix it.  \\n\\nI’m not looking to punish anyone. But let’s make sure corporations and the wealthiest Americans start paying their fair share.', metadata={'source': 'F:\\\\CMC\\\\CMC_Study\\\\Code\\\\data\\\\state_of_the_union.txt', 'doc_id': 'dd327e15-383a-45cb-87e4-41b333555360'}),\n",
       " Document(page_content='I’m not looking to punish anyone. But let’s make sure corporations and the wealthiest Americans start paying their fair share. \\n\\nJust last year, 55 Fortune 500 corporations earned $40 billion in profits and paid zero dollars in federal income tax.  \\n\\nThat’s simply not fair. That’s why I’ve proposed a 15% minimum tax rate for corporations. \\n\\nWe got more than 130 countries to agree on a global minimum tax rate so companies can’t get out of paying their taxes at home by shipping jobs and factories overseas. \\n\\nThat’s why I’ve proposed closing loopholes so the very wealthy don’t pay a lower tax rate than a teacher or a firefighter.  \\n\\nSo that’s my plan. It will grow the economy and lower costs for families. \\n\\nSo what are we waiting for? Let’s get this done. And while you’re at it, confirm my nominees to the Federal Reserve, which plays a critical role in fighting inflation.', metadata={'source': 'F:\\\\CMC\\\\CMC_Study\\\\Code\\\\data\\\\state_of_the_union.txt', 'doc_id': 'dd327e15-383a-45cb-87e4-41b333555360'}),\n",
       " Document(page_content='So what are we waiting for? Let’s get this done. And while you’re at it, confirm my nominees to the Federal Reserve, which plays a critical role in fighting inflation.  \\n\\nMy plan will not only lower costs to give families a fair shot, it will lower the deficit. \\n\\nThe previous Administration not only ballooned the deficit with tax cuts for the very wealthy and corporations, it undermined the watchdogs whose job was to keep pandemic relief funds from being wasted. \\n\\nBut in my administration, the watchdogs have been welcomed back. \\n\\nWe’re going after the criminals who stole billions in relief money meant for small businesses and millions of Americans.  \\n\\nAnd tonight, I’m announcing that the Justice Department will name a chief prosecutor for pandemic fraud. \\n\\nBy the end of this year, the deficit will be down to less than half what it was before I took office.  \\n\\nThe only president ever to cut the deficit by more than one trillion dollars in a single year.', metadata={'source': 'F:\\\\CMC\\\\CMC_Study\\\\Code\\\\data\\\\state_of_the_union.txt', 'doc_id': '3990480c-d6c0-4abc-92f3-c3461b65ec0a'}),\n",
       " Document(page_content='The only president ever to cut the deficit by more than one trillion dollars in a single year. \\n\\nLowering your costs also means demanding more competition. \\n\\nI’m a capitalist, but capitalism without competition isn’t capitalism. \\n\\nIt’s exploitation—and it drives up prices. \\n\\nWhen corporations don’t have to compete, their profits go up, your prices go up, and small businesses and family farmers and ranchers go under. \\n\\nWe see it happening with ocean carriers moving goods in and out of America. \\n\\nDuring the pandemic, these foreign-owned companies raised prices by as much as 1,000% and made record profits. \\n\\nTonight, I’m announcing a crackdown on these companies overcharging American businesses and consumers. \\n\\nAnd as Wall Street firms take over more nursing homes, quality in those homes has gone down and costs have gone up.  \\n\\nThat ends on my watch. \\n\\nMedicare is going to set higher standards for nursing homes and make sure your loved ones get the care they deserve and expect.', metadata={'source': 'F:\\\\CMC\\\\CMC_Study\\\\Code\\\\data\\\\state_of_the_union.txt', 'doc_id': '3990480c-d6c0-4abc-92f3-c3461b65ec0a'}),\n",
       " Document(page_content='That ends on my watch. \\n\\nMedicare is going to set higher standards for nursing homes and make sure your loved ones get the care they deserve and expect. \\n\\nWe’ll also cut costs and keep the economy going strong by giving workers a fair shot, provide more training and apprenticeships, hire them based on their skills not degrees. \\n\\nLet’s pass the Paycheck Fairness Act and paid leave.  \\n\\nRaise the minimum wage to $15 an hour and extend the Child Tax Credit, so no one has to raise a family in poverty. \\n\\nLet’s increase Pell Grants and increase our historic support of HBCUs, and invest in what Jill—our First Lady who teaches full-time—calls America’s best-kept secret: community colleges. \\n\\nAnd let’s pass the PRO Act when a majority of workers want to form a union—they shouldn’t be stopped.  \\n\\nWhen we invest in our workers, when we build the economy from the bottom up and the middle out together, we can do something we haven’t done in a long time: build a better America.', metadata={'source': 'F:\\\\CMC\\\\CMC_Study\\\\Code\\\\data\\\\state_of_the_union.txt', 'doc_id': '3990480c-d6c0-4abc-92f3-c3461b65ec0a'}),\n",
       " Document(page_content='When we invest in our workers, when we build the economy from the bottom up and the middle out together, we can do something we haven’t done in a long time: build a better America. \\n\\nFor more than two years, COVID-19 has impacted every decision in our lives and the life of the nation. \\n\\nAnd I know you’re tired, frustrated, and exhausted. \\n\\nBut I also know this. \\n\\nBecause of the progress we’ve made, because of your resilience and the tools we have, tonight I can say  \\nwe are moving forward safely, back to more normal routines.  \\n\\nWe’ve reached a new moment in the fight against COVID-19, with severe cases down to a level not seen since last July.  \\n\\nJust a few days ago, the Centers for Disease Control and Prevention—the CDC—issued new mask guidelines. \\n\\nUnder these new guidelines, most Americans in most of the country can now be mask free.   \\n\\nAnd based on the projections, more of the country will reach that point across the next couple of weeks.', metadata={'source': 'F:\\\\CMC\\\\CMC_Study\\\\Code\\\\data\\\\state_of_the_union.txt', 'doc_id': '3990480c-d6c0-4abc-92f3-c3461b65ec0a'}),\n",
       " Document(page_content='Under these new guidelines, most Americans in most of the country can now be mask free.   \\n\\nAnd based on the projections, more of the country will reach that point across the next couple of weeks. \\n\\nThanks to the progress we have made this past year, COVID-19 need no longer control our lives.  \\n\\nI know some are talking about “living with COVID-19”. Tonight – I say that we will never just accept living with COVID-19. \\n\\nWe will continue to combat the virus as we do other diseases. And because this is a virus that mutates and spreads, we will stay on guard. \\n\\nHere are four common sense steps as we move forward safely.  \\n\\nFirst, stay protected with vaccines and treatments. We know how incredibly effective vaccines are. If you’re vaccinated and boosted you have the highest degree of protection. \\n\\nWe will never give up on vaccinating more Americans. Now, I know parents with kids under 5 are eager to see a vaccine authorized for their children.', metadata={'source': 'F:\\\\CMC\\\\CMC_Study\\\\Code\\\\data\\\\state_of_the_union.txt', 'doc_id': '3990480c-d6c0-4abc-92f3-c3461b65ec0a'}),\n",
       " Document(page_content='We will never give up on vaccinating more Americans. Now, I know parents with kids under 5 are eager to see a vaccine authorized for their children. \\n\\nThe scientists are working hard to get that done and we’ll be ready with plenty of vaccines when they do. \\n\\nWe’re also ready with anti-viral treatments. If you get COVID-19, the Pfizer pill reduces your chances of ending up in the hospital by 90%.  \\n\\nWe’ve ordered more of these pills than anyone in the world. And Pfizer is working overtime to get us 1 Million pills this month and more than double that next month.  \\n\\nAnd we’re launching the “Test to Treat” initiative so people can get tested at a pharmacy, and if they’re positive, receive antiviral pills on the spot at no cost.  \\n\\nIf you’re immunocompromised or have some other vulnerability, we have treatments and free high-quality masks. \\n\\nWe’re leaving no one behind or ignoring anyone’s needs as we move forward.', metadata={'source': 'F:\\\\CMC\\\\CMC_Study\\\\Code\\\\data\\\\state_of_the_union.txt', 'doc_id': '3990480c-d6c0-4abc-92f3-c3461b65ec0a'}),\n",
       " Document(page_content='If you’re immunocompromised or have some other vulnerability, we have treatments and free high-quality masks. \\n\\nWe’re leaving no one behind or ignoring anyone’s needs as we move forward. \\n\\nAnd on testing, we have made hundreds of millions of tests available for you to order for free.   \\n\\nEven if you already ordered free tests tonight, I am announcing that you can order more from covidtests.gov starting next week. \\n\\nSecond – we must prepare for new variants. Over the past year, we’ve gotten much better at detecting new variants. \\n\\nIf necessary, we’ll be able to deploy new vaccines within 100 days instead of many more months or years.  \\n\\nAnd, if Congress provides the funds we need, we’ll have new stockpiles of tests, masks, and pills ready if needed. \\n\\nI cannot promise a new variant won’t come. But I can promise you we’ll do everything within our power to be ready if it does.  \\n\\nThird – we can end the shutdown of schools and businesses. We have the tools we need.', metadata={'source': 'F:\\\\CMC\\\\CMC_Study\\\\Code\\\\data\\\\state_of_the_union.txt', 'doc_id': 'cb68a831-6c3a-444c-8a91-b605e738db74'}),\n",
       " Document(page_content='Third – we can end the shutdown of schools and businesses. We have the tools we need. \\n\\nIt’s time for Americans to get back to work and fill our great downtowns again.  People working from home can feel safe to begin to return to the office.   \\n\\nWe’re doing that here in the federal government. The vast majority of federal workers will once again work in person. \\n\\nOur schools are open. Let’s keep it that way. Our kids need to be in school. \\n\\nAnd with 75% of adult Americans fully vaccinated and hospitalizations down by 77%, most Americans can remove their masks, return to work, stay in the classroom, and move forward safely. \\n\\nWe achieved this because we provided free vaccines, treatments, tests, and masks. \\n\\nOf course, continuing this costs money. \\n\\nI will soon send Congress a request. \\n\\nThe vast majority of Americans have used these tools and may want to again, so I expect Congress to pass it quickly.   \\n\\nFourth, we will continue vaccinating the world.', metadata={'source': 'F:\\\\CMC\\\\CMC_Study\\\\Code\\\\data\\\\state_of_the_union.txt', 'doc_id': 'cb68a831-6c3a-444c-8a91-b605e738db74'}),\n",
       " Document(page_content='The vast majority of Americans have used these tools and may want to again, so I expect Congress to pass it quickly.   \\n\\nFourth, we will continue vaccinating the world.     \\n\\nWe’ve sent 475 Million vaccine doses to 112 countries, more than any other nation. \\n\\nAnd we won’t stop. \\n\\nWe have lost so much to COVID-19. Time with one another. And worst of all, so much loss of life. \\n\\nLet’s use this moment to reset. Let’s stop looking at COVID-19 as a partisan dividing line and see it for what it is: A God-awful disease.  \\n\\nLet’s stop seeing each other as enemies, and start seeing each other for who we really are: Fellow Americans.  \\n\\nWe can’t change how divided we’ve been. But we can change how we move forward—on COVID-19 and other issues we must face together. \\n\\nI recently visited the New York City Police Department days after the funerals of Officer Wilbert Mora and his partner, Officer Jason Rivera.', metadata={'source': 'F:\\\\CMC\\\\CMC_Study\\\\Code\\\\data\\\\state_of_the_union.txt', 'doc_id': 'cb68a831-6c3a-444c-8a91-b605e738db74'}),\n",
       " Document(page_content='I recently visited the New York City Police Department days after the funerals of Officer Wilbert Mora and his partner, Officer Jason Rivera. \\n\\nThey were responding to a 9-1-1 call when a man shot and killed them with a stolen gun. \\n\\nOfficer Mora was 27 years old. \\n\\nOfficer Rivera was 22. \\n\\nBoth Dominican Americans who’d grown up on the same streets they later chose to patrol as police officers. \\n\\nI spoke with their families and told them that we are forever in debt for their sacrifice, and we will carry on their mission to restore the trust and safety every community deserves. \\n\\nI’ve worked on these issues a long time. \\n\\nI know what works: Investing in crime preventionand community police officers who’ll walk the beat, who’ll know the neighborhood, and who can restore trust and safety. \\n\\nSo let’s not abandon our streets. Or choose between safety and equal justice. \\n\\nLet’s come together to protect our communities, restore trust, and hold law enforcement accountable.', metadata={'source': 'F:\\\\CMC\\\\CMC_Study\\\\Code\\\\data\\\\state_of_the_union.txt', 'doc_id': 'cb68a831-6c3a-444c-8a91-b605e738db74'}),\n",
       " Document(page_content='So let’s not abandon our streets. Or choose between safety and equal justice. \\n\\nLet’s come together to protect our communities, restore trust, and hold law enforcement accountable. \\n\\nThat’s why the Justice Department required body cameras, banned chokeholds, and restricted no-knock warrants for its officers. \\n\\nThat’s why the American Rescue Plan provided $350 Billion that cities, states, and counties can use to hire more police and invest in proven strategies like community violence interruption—trusted messengers breaking the cycle of violence and trauma and giving young people hope.  \\n\\nWe should all agree: The answer is not to Defund the police. The answer is to FUND the police with the resources and training they need to protect our communities. \\n\\nI ask Democrats and Republicans alike: Pass my budget and keep our neighborhoods safe.', metadata={'source': 'F:\\\\CMC\\\\CMC_Study\\\\Code\\\\data\\\\state_of_the_union.txt', 'doc_id': 'cb68a831-6c3a-444c-8a91-b605e738db74'}),\n",
       " Document(page_content='I ask Democrats and Republicans alike: Pass my budget and keep our neighborhoods safe.  \\n\\nAnd I will keep doing everything in my power to crack down on gun trafficking and ghost guns you can buy online and make at home—they have no serial numbers and can’t be traced. \\n\\nAnd I ask Congress to pass proven measures to reduce gun violence. Pass universal background checks. Why should anyone on a terrorist list be able to purchase a weapon? \\n\\nBan assault weapons and high-capacity magazines. \\n\\nRepeal the liability shield that makes gun manufacturers the only industry in America that can’t be sued. \\n\\nThese laws don’t infringe on the Second Amendment. They save lives. \\n\\nThe most fundamental right in America is the right to vote – and to have it counted. And it’s under assault. \\n\\nIn state after state, new laws have been passed, not only to suppress the vote, but to subvert entire elections. \\n\\nWe cannot let this happen.', metadata={'source': 'F:\\\\CMC\\\\CMC_Study\\\\Code\\\\data\\\\state_of_the_union.txt', 'doc_id': 'cb68a831-6c3a-444c-8a91-b605e738db74'}),\n",
       " Document(page_content='In state after state, new laws have been passed, not only to suppress the vote, but to subvert entire elections. \\n\\nWe cannot let this happen. \\n\\nTonight. I call on the Senate to: Pass the Freedom to Vote Act. Pass the John Lewis Voting Rights Act. And while you’re at it, pass the Disclose Act so Americans can know who is funding our elections. \\n\\nTonight, I’d like to honor someone who has dedicated his life to serve this country: Justice Stephen Breyer—an Army veteran, Constitutional scholar, and retiring Justice of the United States Supreme Court. Justice Breyer, thank you for your service. \\n\\nOne of the most serious constitutional responsibilities a President has is nominating someone to serve on the United States Supreme Court. \\n\\nAnd I did that 4 days ago, when I nominated Circuit Court of Appeals Judge Ketanji Brown Jackson. One of our nation’s top legal minds, who will continue Justice Breyer’s legacy of excellence.', metadata={'source': 'F:\\\\CMC\\\\CMC_Study\\\\Code\\\\data\\\\state_of_the_union.txt', 'doc_id': '36498ad4-b192-487b-9323-3369e9bf4efa'}),\n",
       " Document(page_content='And I did that 4 days ago, when I nominated Circuit Court of Appeals Judge Ketanji Brown Jackson. One of our nation’s top legal minds, who will continue Justice Breyer’s legacy of excellence. \\n\\nA former top litigator in private practice. A former federal public defender. And from a family of public school educators and police officers. A consensus builder. Since she’s been nominated, she’s received a broad range of support—from the Fraternal Order of Police to former judges appointed by Democrats and Republicans. \\n\\nAnd if we are to advance liberty and justice, we need to secure the Border and fix the immigration system. \\n\\nWe can do both. At our border, we’ve installed new technology like cutting-edge scanners to better detect drug smuggling.  \\n\\nWe’ve set up joint patrols with Mexico and Guatemala to catch more human traffickers.  \\n\\nWe’re putting in place dedicated immigration judges so families fleeing persecution and violence can have their cases heard faster.', metadata={'source': 'F:\\\\CMC\\\\CMC_Study\\\\Code\\\\data\\\\state_of_the_union.txt', 'doc_id': '36498ad4-b192-487b-9323-3369e9bf4efa'}),\n",
       " Document(page_content='We’re putting in place dedicated immigration judges so families fleeing persecution and violence can have their cases heard faster. \\n\\nWe’re securing commitments and supporting partners in South and Central America to host more refugees and secure their own borders. \\n\\nWe can do all this while keeping lit the torch of liberty that has led generations of immigrants to this land—my forefathers and so many of yours. \\n\\nProvide a pathway to citizenship for Dreamers, those on temporary status, farm workers, and essential workers. \\n\\nRevise our laws so businesses have the workers they need and families don’t wait decades to reunite. \\n\\nIt’s not only the right thing to do—it’s the economically smart thing to do. \\n\\nThat’s why immigration reform is supported by everyone from labor unions to religious leaders to the U.S. Chamber of Commerce. \\n\\nLet’s get it done once and for all. \\n\\nAdvancing liberty and justice also requires protecting the rights of women.', metadata={'source': 'F:\\\\CMC\\\\CMC_Study\\\\Code\\\\data\\\\state_of_the_union.txt', 'doc_id': '36498ad4-b192-487b-9323-3369e9bf4efa'}),\n",
       " Document(page_content='Let’s get it done once and for all. \\n\\nAdvancing liberty and justice also requires protecting the rights of women. \\n\\nThe constitutional right affirmed in Roe v. Wade—standing precedent for half a century—is under attack as never before. \\n\\nIf we want to go forward—not backward—we must protect access to health care. Preserve a woman’s right to choose. And let’s continue to advance maternal health care in America. \\n\\nAnd for our LGBTQ+ Americans, let’s finally get the bipartisan Equality Act to my desk. The onslaught of state laws targeting transgender Americans and their families is wrong. \\n\\nAs I said last year, especially to our younger transgender Americans, I will always have your back as your President, so you can be yourself and reach your God-given potential.', metadata={'source': 'F:\\\\CMC\\\\CMC_Study\\\\Code\\\\data\\\\state_of_the_union.txt', 'doc_id': '36498ad4-b192-487b-9323-3369e9bf4efa'}),\n",
       " Document(page_content='As I said last year, especially to our younger transgender Americans, I will always have your back as your President, so you can be yourself and reach your God-given potential. \\n\\nWhile it often appears that we never agree, that isn’t true. I signed 80 bipartisan bills into law last year. From preventing government shutdowns to protecting Asian-Americans from still-too-common hate crimes to reforming military justice. \\n\\nAnd soon, we’ll strengthen the Violence Against Women Act that I first wrote three decades ago. It is important for us to show the nation that we can come together and do big things. \\n\\nSo tonight I’m offering a Unity Agenda for the Nation. Four big things we can do together.  \\n\\nFirst, beat the opioid epidemic. \\n\\nThere is so much we can do. Increase funding for prevention, treatment, harm reduction, and recovery.', metadata={'source': 'F:\\\\CMC\\\\CMC_Study\\\\Code\\\\data\\\\state_of_the_union.txt', 'doc_id': '36498ad4-b192-487b-9323-3369e9bf4efa'}),\n",
       " Document(page_content='First, beat the opioid epidemic. \\n\\nThere is so much we can do. Increase funding for prevention, treatment, harm reduction, and recovery.  \\n\\nGet rid of outdated rules that stop doctors from prescribing treatments. And stop the flow of illicit drugs by working with state and local law enforcement to go after traffickers. \\n\\nIf you’re suffering from addiction, know you are not alone. I believe in recovery, and I celebrate the 23 million Americans in recovery. \\n\\nSecond, let’s take on mental health. Especially among our children, whose lives and education have been turned upside down.  \\n\\nThe American Rescue Plan gave schools money to hire teachers and help students make up for lost learning.  \\n\\nI urge every parent to make sure your school does just that. And we can all play a part—sign up to be a tutor or a mentor. \\n\\nChildren were also struggling before the pandemic. Bullying, violence, trauma, and the harms of social media.', metadata={'source': 'F:\\\\CMC\\\\CMC_Study\\\\Code\\\\data\\\\state_of_the_union.txt', 'doc_id': '36498ad4-b192-487b-9323-3369e9bf4efa'}),\n",
       " Document(page_content='Children were also struggling before the pandemic. Bullying, violence, trauma, and the harms of social media. \\n\\nAs Frances Haugen, who is here with us tonight, has shown, we must hold social media platforms accountable for the national experiment they’re conducting on our children for profit. \\n\\nIt’s time to strengthen privacy protections, ban targeted advertising to children, demand tech companies stop collecting personal data on our children.', metadata={'source': 'F:\\\\CMC\\\\CMC_Study\\\\Code\\\\data\\\\state_of_the_union.txt', 'doc_id': '36498ad4-b192-487b-9323-3369e9bf4efa'}),\n",
       " Document(page_content='It’s time to strengthen privacy protections, ban targeted advertising to children, demand tech companies stop collecting personal data on our children. \\n\\nAnd let’s get all Americans the mental health services they need. More people they can turn to for help, and full parity between physical and mental health care. \\n\\nThird, support our veterans. \\n\\nVeterans are the best of us. \\n\\nI’ve always believed that we have a sacred obligation to equip all those we send to war and care for them and their families when they come home. \\n\\nMy administration is providing assistance with job training and housing, and now helping lower-income veterans get VA care debt-free.  \\n\\nOur troops in Iraq and Afghanistan faced many dangers. \\n\\nOne was stationed at bases and breathing in toxic smoke from “burn pits” that incinerated wastes of war—medical and hazard material, jet fuel, and more. \\n\\nWhen they came home, many of the world’s fittest and best trained warriors were never the same.', metadata={'source': 'F:\\\\CMC\\\\CMC_Study\\\\Code\\\\data\\\\state_of_the_union.txt', 'doc_id': '0b4e801c-f2dc-4c4b-97c3-2d0eb1f9a623'}),\n",
       " Document(page_content='When they came home, many of the world’s fittest and best trained warriors were never the same. \\n\\nHeadaches. Numbness. Dizziness. \\n\\nA cancer that would put them in a flag-draped coffin. \\n\\nI know. \\n\\nOne of those soldiers was my son Major Beau Biden. \\n\\nWe don’t know for sure if a burn pit was the cause of his brain cancer, or the diseases of so many of our troops. \\n\\nBut I’m committed to finding out everything we can. \\n\\nCommitted to military families like Danielle Robinson from Ohio. \\n\\nThe widow of Sergeant First Class Heath Robinson.  \\n\\nHe was born a soldier. Army National Guard. Combat medic in Kosovo and Iraq. \\n\\nStationed near Baghdad, just yards from burn pits the size of football fields. \\n\\nHeath’s widow Danielle is here with us tonight. They loved going to Ohio State football games. He loved building Legos with their daughter. \\n\\nBut cancer from prolonged exposure to burn pits ravaged Heath’s lungs and body. \\n\\nDanielle says Heath was a fighter to the very end.', metadata={'source': 'F:\\\\CMC\\\\CMC_Study\\\\Code\\\\data\\\\state_of_the_union.txt', 'doc_id': '0b4e801c-f2dc-4c4b-97c3-2d0eb1f9a623'}),\n",
       " Document(page_content='But cancer from prolonged exposure to burn pits ravaged Heath’s lungs and body. \\n\\nDanielle says Heath was a fighter to the very end. \\n\\nHe didn’t know how to stop fighting, and neither did she. \\n\\nThrough her pain she found purpose to demand we do better. \\n\\nTonight, Danielle—we are. \\n\\nThe VA is pioneering new ways of linking toxic exposures to diseases, already helping more veterans get benefits. \\n\\nAnd tonight, I’m announcing we’re expanding eligibility to veterans suffering from nine respiratory cancers. \\n\\nI’m also calling on Congress: pass a law to make sure veterans devastated by toxic exposures in Iraq and Afghanistan finally get the benefits and comprehensive health care they deserve. \\n\\nAnd fourth, let’s end cancer as we know it. \\n\\nThis is personal to me and Jill, to Kamala, and to so many of you. \\n\\nCancer is the #2 cause of death in America–second only to heart disease.', metadata={'source': 'F:\\\\CMC\\\\CMC_Study\\\\Code\\\\data\\\\state_of_the_union.txt', 'doc_id': '0b4e801c-f2dc-4c4b-97c3-2d0eb1f9a623'}),\n",
       " Document(page_content='And fourth, let’s end cancer as we know it. \\n\\nThis is personal to me and Jill, to Kamala, and to so many of you. \\n\\nCancer is the #2 cause of death in America–second only to heart disease. \\n\\nLast month, I announced our plan to supercharge  \\nthe Cancer Moonshot that President Obama asked me to lead six years ago. \\n\\nOur goal is to cut the cancer death rate by at least 50% over the next 25 years, turn more cancers from death sentences into treatable diseases.  \\n\\nMore support for patients and families. \\n\\nTo get there, I call on Congress to fund ARPA-H, the Advanced Research Projects Agency for Health. \\n\\nIt’s based on DARPA—the Defense Department project that led to the Internet, GPS, and so much more.  \\n\\nARPA-H will have a singular purpose—to drive breakthroughs in cancer, Alzheimer’s, diabetes, and more. \\n\\nA unity agenda for the nation. \\n\\nWe can do this. \\n\\nMy fellow Americans—tonight , we have gathered in a sacred space—the citadel of our democracy.', metadata={'source': 'F:\\\\CMC\\\\CMC_Study\\\\Code\\\\data\\\\state_of_the_union.txt', 'doc_id': '0b4e801c-f2dc-4c4b-97c3-2d0eb1f9a623'}),\n",
       " Document(page_content='A unity agenda for the nation. \\n\\nWe can do this. \\n\\nMy fellow Americans—tonight , we have gathered in a sacred space—the citadel of our democracy. \\n\\nIn this Capitol, generation after generation, Americans have debated great questions amid great strife, and have done great things. \\n\\nWe have fought for freedom, expanded liberty, defeated totalitarianism and terror. \\n\\nAnd built the strongest, freest, and most prosperous nation the world has ever known. \\n\\nNow is the hour. \\n\\nOur moment of responsibility. \\n\\nOur test of resolve and conscience, of history itself. \\n\\nIt is in this moment that our character is formed. Our purpose is found. Our future is forged. \\n\\nWell I know this nation.  \\n\\nWe will meet the test. \\n\\nTo protect freedom and liberty, to expand fairness and opportunity. \\n\\nWe will save democracy. \\n\\nAs hard as these times have been, I am more optimistic about America today than I have been my whole life. \\n\\nBecause I see the future that is within our grasp.', metadata={'source': 'F:\\\\CMC\\\\CMC_Study\\\\Code\\\\data\\\\state_of_the_union.txt', 'doc_id': '0b4e801c-f2dc-4c4b-97c3-2d0eb1f9a623'}),\n",
       " Document(page_content='We will save democracy. \\n\\nAs hard as these times have been, I am more optimistic about America today than I have been my whole life. \\n\\nBecause I see the future that is within our grasp. \\n\\nBecause I know there is simply nothing beyond our capacity. \\n\\nWe are the only nation on Earth that has always turned every crisis we have faced into an opportunity. \\n\\nThe only nation that can be defined by a single word: possibilities. \\n\\nSo on this night, in our 245th year as a nation, I have come to report on the State of the Union. \\n\\nAnd my report is this: the State of the Union is strong—because you, the American people, are strong. \\n\\nWe are stronger today than we were a year ago. \\n\\nAnd we will be stronger a year from now than we are today. \\n\\nNow is our moment to meet and overcome the challenges of our time. \\n\\nAnd we will, as one people. \\n\\nOne America. \\n\\nThe United States of America. \\n\\nMay God bless you all. May God protect our troops.', metadata={'source': 'F:\\\\CMC\\\\CMC_Study\\\\Code\\\\data\\\\state_of_the_union.txt', 'doc_id': '0b4e801c-f2dc-4c4b-97c3-2d0eb1f9a623'})]"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sub_docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "c64b5772-52f1-4416-9d6e-46c8ccc6306f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "49"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(sub_docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "ebc9e801-6f3f-4fe4-aa76-08a7e9c680d8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['bef2795c-5473-4a59-a238-b6508d25ca2f',\n",
       " '9c5797a6-17cb-418b-b8f7-6170e03a277d',\n",
       " '99ddb8aa-f784-4092-b1e0-f3085663dd23',\n",
       " 'e460895f-b9f7-4bf8-87de-1b18e97ea553',\n",
       " '5098e284-9010-4aef-a1b8-1b595bcc2b8b',\n",
       " 'b2aa5978-e358-46df-804c-419b605ca5f3',\n",
       " '43237bae-1fe7-4fa4-8eb9-356efddaae42',\n",
       " 'd5ded0ac-0105-45fb-8214-8d70d945ea90',\n",
       " 'aca906f3-8807-4da6-936d-89f4e5d92397',\n",
       " '915ea080-3f4b-46cc-9463-637c4b25188a',\n",
       " '775aa9f1-4133-46ef-9b50-d8008d671a01',\n",
       " '744d6b62-f61f-4b80-a8e1-e65b5a0c6992',\n",
       " 'b4286eec-c1b1-4ea9-a15a-d0ec781c9cf0',\n",
       " '5cb30169-94ca-45ab-8d6c-c49032936678',\n",
       " 'e2857e3b-0637-4951-8f6d-88771af1afc3',\n",
       " '18f1a5f6-13af-478a-9fde-bdc47cf24edf',\n",
       " 'ee53c432-e59a-4368-b365-7f23c1844a98',\n",
       " 'a3928525-8743-4893-82aa-bb4341b57b61',\n",
       " 'fad1bbcd-859d-4b36-a0e9-76e6cb510383',\n",
       " '2f4b5c5d-0edb-477d-b294-49ae7b8bd65f',\n",
       " '68e5ea4a-6327-40a4-8148-4578ba297f5e',\n",
       " '69c33b74-257c-4571-bedc-afa84f86548c',\n",
       " '5cc05b14-0a73-4f9e-8943-16f895123185',\n",
       " 'a5da98d2-4715-4735-93b9-aa5a1d445e3c',\n",
       " '866b2119-8e86-46cb-8fe4-d89ad23d6f7e',\n",
       " 'd44d6e4b-61aa-4eb6-89b9-42e7bba54b8a',\n",
       " 'b55efb55-e186-4991-a8ed-421e42d862ad',\n",
       " 'bd8353ed-07f7-4278-89a0-f6304061c14f',\n",
       " '7f8e8e03-1bd9-480b-8760-fe91321fc642',\n",
       " 'f58f9b32-7b42-4628-9088-a66988d1988f',\n",
       " '67c44de6-bbc5-4317-9688-1c98bd55e55d',\n",
       " '404cfce9-010f-49bd-b616-a30ca22a626a',\n",
       " 'edf34753-2d62-45a3-ac86-5892350beeee',\n",
       " '7eccc51d-fd96-4254-b952-2e891c598154',\n",
       " '70c2ccbb-9581-4a5c-a939-7b36fda60205',\n",
       " 'f4066ecd-10a5-409f-95ec-f92f5fe5d16f',\n",
       " '4e90fc73-4e81-4fde-bcf9-a215f3affd06',\n",
       " '424c90b7-5f66-40b8-87f6-f19af6059289',\n",
       " 'aa2d053d-651c-4586-b912-096fff07ca78',\n",
       " '60e2d4ed-54b5-4a79-bdfd-80421ae99ec2',\n",
       " '7905fce1-3594-4182-8db9-fc9dc6aa2c82',\n",
       " '6f0518be-2fdf-4d82-b4af-f4df6e1c076b',\n",
       " 'ec0c5edc-38a8-4320-94b8-653842b70ecd',\n",
       " 'dd625f17-fd09-43bb-a371-7484b767d16b',\n",
       " 'c8ae4083-5a0b-4675-ad21-cd7d4533a663',\n",
       " '858cea70-c6d9-4e68-b88b-83bf2b8c97d0',\n",
       " '1e11a8a4-94cb-4dea-8ff1-85aa1bd06454',\n",
       " 'e6af8d58-b618-4413-a0a3-e594797118a2',\n",
       " 'fb9c911c-47ae-47ca-8833-b2b879bbbb69']"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "retriever.vectorstore.add_documents(sub_docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "2495d0b0-ffba-4b4d-b7ac-c9b5d189f9cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "retriever.docstore.mset(list(zip(doc_ids, docs)))  # ánh xạ id - docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "8ac929cc-d71d-4495-ab71-6b93bf16fffe",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Document(page_content='The only president ever to cut the deficit by more than one trillion dollars in a single year. \\n\\nLowering your costs also means demanding more competition. \\n\\nI’m a capitalist, but capitalism without competition isn’t capitalism. \\n\\nIt’s exploitation—and it drives up prices. \\n\\nWhen corporations don’t have to compete, their profits go up, your prices go up, and small businesses and family farmers and ranchers go under. \\n\\nWe see it happening with ocean carriers moving goods in and out of America. \\n\\nDuring the pandemic, these foreign-owned companies raised prices by as much as 1,000% and made record profits. \\n\\nTonight, I’m announcing a crackdown on these companies overcharging American businesses and consumers. \\n\\nAnd as Wall Street firms take over more nursing homes, quality in those homes has gone down and costs have gone up.  \\n\\nThat ends on my watch. \\n\\nMedicare is going to set higher standards for nursing homes and make sure your loved ones get the care they deserve and expect.', metadata={'doc_id': '13d43643-c51d-4164-a723-ffa7d2164be8', 'source': 'F:\\\\CMC\\\\CMC_Study\\\\Code\\\\data\\\\state_of_the_union.txt'})"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "retriever.vectorstore.similarity_search(\"justice breyer\")[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "2f772278-6e41-4179-80af-036a77cefec0",
   "metadata": {},
   "outputs": [],
   "source": [
    "retrieved_docs = retriever.invoke(\"justice breyer\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "f48e308d-7aae-4508-ac66-04d28b480461",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'So what are we waiting for? Let’s get this done. And while you’re at it, confirm my nominees to the Federal Reserve, which plays a critical role in fighting inflation.  \\n\\nMy plan will not only lower costs to give families a fair shot, it will lower the deficit. \\n\\nThe previous Administration not only ballooned the deficit with tax cuts for the very wealthy and corporations, it undermined the watchdogs whose job was to keep pandemic relief funds from being wasted. \\n\\nBut in my administration, the watchdogs have been welcomed back. \\n\\nWe’re going after the criminals who stole billions in relief money meant for small businesses and millions of Americans.  \\n\\nAnd tonight, I’m announcing that the Justice Department will name a chief prosecutor for pandemic fraud. \\n\\nBy the end of this year, the deficit will be down to less than half what it was before I took office.  \\n\\nThe only president ever to cut the deficit by more than one trillion dollars in a single year. \\n\\nLowering your costs also means '"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "retrieved_docs[0].page_content[:1000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "4fd5207c-84fb-48c3-ab87-6289180e0dd6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "..\n"
     ]
    }
   ],
   "source": [
    "print(\"..\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "d0d13478-dbe8-4c4d-8d07-382ff9d4843b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#---------------------------------------------------------------------------------\n",
    "# Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "cfeb24d5-43f1-4ef4-b385-e70d4f86da03",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.document_loaders import WebBaseLoader\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "\n",
    "loader = WebBaseLoader(\"https://lilianweng.github.io/posts/2023-06-23-agent/\")\n",
    "docs = loader.load()\n",
    "\n",
    "loader = WebBaseLoader(\"https://lilianweng.github.io/posts/2024-02-05-human-data-quality/\")\n",
    "docs.extend(loader.load())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "5daa2207-419f-419a-a636-478420e821dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import uuid\n",
    "\n",
    "from langchain_core.documents import Document\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "# from langchain_openai import ChatOpenAI\n",
    "\n",
    "chain = (\n",
    "    {\"doc\": lambda x: x.page_content}\n",
    "    | ChatPromptTemplate.from_template(\"Summarize the following document:\\n\\n{doc}\")\n",
    "    | model # ChatOpenAI(model=\"gpt-3.5-turbo\",max_retries=0)\n",
    "    | StrOutputParser()\n",
    ")\n",
    "\n",
    "summaries = chain.batch(docs, {\"max_concurrency\": 5})  # chạy đồng thời"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "36b84ed6-5884-4e73-bf6a-55ee1a836f6d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['The document discusses the concept of LLM (Large Language Model) powered autonomous agents, specifically highlighting challenges and limitations in building such systems. The author, Lilian Weng, shares her thoughts on the topic.\\n\\n**Key Points:**\\n\\n1. **LLM-powered Autonomous Agents**: The concept is to build agents that use Large Language Models (LLMs) as their core component.\\n2. **Challenges**:\\n\\t* Finite context length limits the inclusion of historical information and detailed instructions.\\n\\t* Long-term planning and task decomposition remain challenging, making LLMs less robust than humans.\\n\\t* Natural language interface reliability is questionable due to formatting errors and occasional rebellious behavior.\\n\\n**References:**\\n\\nThe document provides a list of 21 references, mostly academic papers and blogs, related to the topic of Large Language Models and their applications in building autonomous agents. Some notable references include:\\n\\n1. **Chain of Thought Prompting**: A paper on prompting LLMs for reasoning.\\n2. **Tree of Thoughts**: A method for deliberative problem-solving with LLMs.\\n3. **LLM+P**: A framework for empowering LLMs with optimal planning proficiency.\\n\\n**Prompt Engineering:**\\n\\nThe document also touches on the concept of prompt engineering, which is crucial for effectively using LLMs as agents.\\n\\nOverall, the document provides an overview of the challenges and opportunities in building LLM-powered autonomous agents.',\n",
       " 'The document discusses the importance of high-quality human data in machine learning and AI. The author, Lilian Weng, provides an overview of various methods for identifying mislabeled data and noisy data, including:\\n\\n1. Area Under the Margin (AUM) thresholding: This method uses a threshold value to separate out mislabeled samples based on their AUM values.\\n2. Noisy Cross-Validation (NCV): This method divides the dataset into two halves and identifies clean samples as those with labels that match the predicted label from the other half of the dataset.\\n3. Iterative Noisy Cross-Validation (INCV): This method runs NCV iteratively, adding more clean samples to a trusted candidate set and removing noisy samples.\\n\\nThe author also references various papers and studies on data quality, including:\\n\\n1. \"Vox populi\" by Francis Galton\\n2. \"Everyone wants to do the model work, not the data work\": Data Cascades in High-Stakes AI\" by Sambasivan et al.\\n3. \"Fast, Cheap, and Creative: Evaluating Translation Quality Using Amazon’s Mechanical Turk\" by Chris Callison-Burch\\n\\nThe document concludes that high-quality human data is essential for machine learning and AI models to perform well, and that identifying and removing noisy or mislabeled data is crucial for improving model performance.\\n\\nOverall, the document provides a comprehensive overview of the importance of data quality in machine learning and AI, as well as various methods for identifying and addressing noisy or mislabeled data.']"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "summaries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "2e4128e2-8f36-428a-a491-9876156cafe0",
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "cannot import name 'LangSmithParams' from 'langchain_core.language_models.chat_models' (F:\\CMC\\CMC_Study\\Code\\venv\\lib\\site-packages\\langchain_core\\language_models\\chat_models.py)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[89], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mlangchain\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mstorage\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m InMemoryByteStore\n\u001b[1;32m----> 2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mlangchain_openai\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m OpenAIEmbeddings\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mlangchain_community\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mvectorstores\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Chroma\n\u001b[0;32m      4\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mlangchain\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mretrievers\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmulti_vector\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m MultiVectorRetriever\n",
      "File \u001b[1;32mF:\\CMC\\CMC_Study\\Code\\venv\\lib\\site-packages\\langchain_openai\\__init__.py:1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mlangchain_openai\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mchat_models\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[0;32m      2\u001b[0m     AzureChatOpenAI,\n\u001b[0;32m      3\u001b[0m     ChatOpenAI,\n\u001b[0;32m      4\u001b[0m )\n\u001b[0;32m      5\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mlangchain_openai\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01membeddings\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[0;32m      6\u001b[0m     AzureOpenAIEmbeddings,\n\u001b[0;32m      7\u001b[0m     OpenAIEmbeddings,\n\u001b[0;32m      8\u001b[0m )\n\u001b[0;32m      9\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mlangchain_openai\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mllms\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m AzureOpenAI, OpenAI\n",
      "File \u001b[1;32mF:\\CMC\\CMC_Study\\Code\\venv\\lib\\site-packages\\langchain_openai\\chat_models\\__init__.py:1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mlangchain_openai\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mchat_models\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mazure\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m AzureChatOpenAI\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mlangchain_openai\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mchat_models\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mbase\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m ChatOpenAI\n\u001b[0;32m      4\u001b[0m __all__ \u001b[38;5;241m=\u001b[39m [\n\u001b[0;32m      5\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mChatOpenAI\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m      6\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAzureChatOpenAI\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m      7\u001b[0m ]\n",
      "File \u001b[1;32mF:\\CMC\\CMC_Study\\Code\\venv\\lib\\site-packages\\langchain_openai\\chat_models\\azure.py:9\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtyping\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Any, Callable, Dict, List, Optional, Union\n\u001b[0;32m      8\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mopenai\u001b[39;00m\n\u001b[1;32m----> 9\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mlangchain_core\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mlanguage_models\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mchat_models\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m LangSmithParams\n\u001b[0;32m     10\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mlangchain_core\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01moutputs\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m ChatResult\n\u001b[0;32m     11\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mlangchain_core\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpydantic_v1\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Field, SecretStr, root_validator\n",
      "\u001b[1;31mImportError\u001b[0m: cannot import name 'LangSmithParams' from 'langchain_core.language_models.chat_models' (F:\\CMC\\CMC_Study\\Code\\venv\\lib\\site-packages\\langchain_core\\language_models\\chat_models.py)"
     ]
    }
   ],
   "source": [
    "from langchain.storage import InMemoryByteStore\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "from langchain_community.vectorstores import Chroma\n",
    "from langchain.retrievers.multi_vector import MultiVectorRetriever\n",
    "\n",
    "# The vectorstore to use to index the child chunks\n",
    "vectorstore = Chroma(collection_name=\"summaries\",\n",
    "                     embedding_function=embeddings)\n",
    "\n",
    "# The storage layer for the parent documents\n",
    "store = InMemoryByteStore()\n",
    "id_key = \"doc_id\"\n",
    "\n",
    "# The retriever\n",
    "retriever = MultiVectorRetriever(\n",
    "    vectorstore=vectorstore,\n",
    "    byte_store=store,\n",
    "    id_key=id_key,\n",
    ")\n",
    "doc_ids = [str(uuid.uuid4()) for _ in docs]\n",
    "\n",
    "# Docs linked to summaries\n",
    "summary_docs = [\n",
    "    Document(page_content=s, metadata={id_key: doc_ids[i]})\n",
    "    for i, s in enumerate(summaries)\n",
    "]\n",
    "\n",
    "# Add\n",
    "retriever.vectorstore.add_documents(summary_docs)\n",
    "retriever.docstore.mset(list(zip(doc_ids, docs)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce3244b6-5331-4d92-8a70-b3262a6cb729",
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"Memory in agents\"\n",
    "sub_docs = vectorstore.similarity_search(query,k=1)\n",
    "print(sub_docs[0])\n",
    "print(len(sub_docs[0].page_content))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78142356-6ef1-4088-9b63-df492d63fe3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "retrieved_docs = retriever.get_relevant_documents(query,n_results=1)\n",
    "retrieved_docs[0].page_content[0:500]\n",
    "print(len(retrieved_docs[0].page_content))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3984d71f-af29-45ac-88b4-c16393239a48",
   "metadata": {},
   "outputs": [],
   "source": [
    "#---------------------------------------------------------------------------------\n",
    "# Hypothetical question -- tạo câu hỏi giải định mà tài liệu phù hợp trả lời, nhúng câu hỏi vs tài liệu\n",
    "# todo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c9560e9-1b9d-4502-ab64-82b4612f7380",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "310c41e2-78ef-4004-a2ac-904ae9a5ee00",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# 3. ColBERT**\n",
    "score_docs đc tính dựa trên từng token của câu hỏi vs từng token của document\n",
    "thay vì tính score từ 1 vector duy nhất do embed document liên quan "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d946c631-4b34-4757-b357-d5589144737f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ec08ea1e644841efbf6a2cbcfc3309a2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "artifact.metadata:   0%|          | 0.00/1.63k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "F:\\CMC\\CMC_Study\\Code\\venv\\lib\\site-packages\\huggingface_hub\\file_download.py:157: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\tuong\\.cache\\huggingface\\hub\\models--colbert-ir--colbertv2.0. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to see activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n",
      "F:\\CMC\\CMC_Study\\Code\\venv\\lib\\site-packages\\huggingface_hub\\file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ca312860a86349808d4e5837975f353e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/743 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "db1ccc4306eb491c97e36da8091fa97d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/438M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1c4cd7fd682c4874a6b1e03a838ac98f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/405 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "29bc6f5881fe44f9bef8cad1080d707b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.txt:   0%|          | 0.00/232k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b15055340413406e86cbfc04f5831709",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/466k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "127524f8bc4e4fc6ab12d420a2a94683",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/112 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from ragatouille import RAGPretrainedModel\n",
    "RAG = RAGPretrainedModel.from_pretrained(\"colbert-ir/colbertv2.0\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "bf980d68-a920-4775-8cda-c8515d59cd22",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "\n",
    "def get_wikipedia_page(title: str):\n",
    "    \"\"\"\n",
    "    Retrieve the full text content of a Wikipedia page.\n",
    "\n",
    "    :param title: str - Title of the Wikipedia page.\n",
    "    :return: str - Full text content of the page as raw string.\n",
    "    \"\"\"\n",
    "    # Wikipedia API endpoint\n",
    "    URL = \"https://en.wikipedia.org/w/api.php\"\n",
    "\n",
    "    # Parameters for the API request\n",
    "    params = {\n",
    "        \"action\": \"query\",\n",
    "        \"format\": \"json\",\n",
    "        \"titles\": title,\n",
    "        \"prop\": \"extracts\",\n",
    "        \"explaintext\": True,\n",
    "    }\n",
    "\n",
    "    # Custom User-Agent header to comply with Wikipedia's best practices\n",
    "    headers = {\"User-Agent\": \"RAGatouille_tutorial/0.0.1 (ben@clavie.eu)\"}\n",
    "\n",
    "    response = requests.get(URL, params=params, headers=headers)\n",
    "    data = response.json()\n",
    "\n",
    "    # Extracting page content\n",
    "    page = next(iter(data[\"query\"][\"pages\"].values()))\n",
    "    return page[\"extract\"] if \"extract\" in page else None\n",
    "\n",
    "full_document = get_wikipedia_page(\"Hayao_Miyazaki\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "bca8be13-0985-411c-93b3-61b1cd6a10c3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New index_name received! Updating current index_name (Miyazaki-123) to Miyazaki-123\n",
      "---- WARNING! You are using PLAID with an experimental replacement for FAISS for greater compatibility ----\n",
      "This is a behaviour change from RAGatouille 0.8.0 onwards.\n",
      "This works fine for most users and smallish datasets, but can be considerably slower than FAISS and could cause worse results in some situations.\n",
      "If you're confident with FAISS working on your machine, pass use_faiss=True to revert to the FAISS-using behaviour.\n",
      "--------------------\n",
      "\n",
      "\n",
      "[Jun 11, 17:51:15] #> Note: Output directory .ragatouille/colbert\\indexes/Miyazaki-123 already exists\n",
      "\n",
      "\n",
      "[Jun 11, 17:51:15] #> Will delete 1 files already at .ragatouille/colbert\\indexes/Miyazaki-123 in 20 seconds...\n",
      "[Jun 11, 17:51:38] [0] \t\t #> Encoding 78 passages..\n",
      "[Jun 11, 17:51:39] [0] \t\t avg_doclen_est = 132.84616088867188 \t len(local_sample) = 78\n",
      "[Jun 11, 17:51:39] [0] \t\t Creating 1,024 partitions.\n",
      "[Jun 11, 17:51:39] [0] \t\t *Estimated* 10,362 embeddings.\n",
      "[Jun 11, 17:51:39] [0] \t\t #> Saving the indexing plan to .ragatouille/colbert\\indexes/Miyazaki-123\\plan.json ..\n",
      "Warning: number of training points (9844) is less than the minimum recommended (10240)\n",
      "used 19 iterations (0.1376s) to cluster 9844 items into 1024 clusters\n",
      "[Jun 11, 17:51:39] Loading decompress_residuals_cpp extension (set COLBERT_LOAD_TORCH_EXTENSION_VERBOSE=True for more info)...\n",
      "PyTorch-based indexing did not succeed with error: DLL load failed while importing decompress_residuals_cpp: The specified module could not be found. ! Reverting to using FAISS and attempting again...\n",
      "________________________________________________________________________________\n",
      "WARNING! You have a GPU available, but only `faiss-cpu` is currently installed.\n",
      " This means that indexing will be slow. To make use of your GPU.\n",
      "Please install `faiss-gpu` by running:\n",
      "pip uninstall --y faiss-cpu & pip install faiss-gpu\n",
      " ________________________________________________________________________________\n",
      "Will continue with CPU indexing in 5 seconds...\n",
      "\n",
      "\n",
      "[Jun 11, 17:51:44] #> Note: Output directory .ragatouille/colbert\\indexes/Miyazaki-123 already exists\n",
      "\n",
      "\n",
      "[Jun 11, 17:51:44] #> Will delete 1 files already at .ragatouille/colbert\\indexes/Miyazaki-123 in 20 seconds...\n",
      "[Jun 11, 17:52:07] [0] \t\t #> Encoding 78 passages..\n",
      "[Jun 11, 17:52:08] [0] \t\t avg_doclen_est = 132.84616088867188 \t len(local_sample) = 78\n",
      "[Jun 11, 17:52:08] [0] \t\t Creating 1,024 partitions.\n",
      "[Jun 11, 17:52:08] [0] \t\t *Estimated* 10,362 embeddings.\n",
      "[Jun 11, 17:52:08] [0] \t\t #> Saving the indexing plan to .ragatouille/colbert\\indexes/Miyazaki-123\\plan.json ..\n",
      "[Jun 11, 17:52:08] Loading decompress_residuals_cpp extension (set COLBERT_LOAD_TORCH_EXTENSION_VERBOSE=True for more info)...\n"
     ]
    },
    {
     "ename": "ImportError",
     "evalue": "DLL load failed while importing decompress_residuals_cpp: The specified module could not be found.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[6], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[43mRAG\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mindex\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m      2\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcollection\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\u001b[43mfull_document\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m      3\u001b[0m \u001b[43m    \u001b[49m\u001b[43mindex_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mMiyazaki-123\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m      4\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmax_document_length\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m180\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m      5\u001b[0m \u001b[43m    \u001b[49m\u001b[43msplit_documents\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m      6\u001b[0m \u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mF:\\CMC\\CMC_Study\\Code\\venv\\lib\\site-packages\\ragatouille\\RAGPretrainedModel.py:211\u001b[0m, in \u001b[0;36mRAGPretrainedModel.index\u001b[1;34m(self, collection, document_ids, document_metadatas, index_name, overwrite_index, max_document_length, split_documents, document_splitter_fn, preprocessing_fn, bsize, use_faiss)\u001b[0m\n\u001b[0;32m    202\u001b[0m     document_splitter_fn \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    203\u001b[0m collection, pid_docid_map, docid_metadata_map \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_process_corpus(\n\u001b[0;32m    204\u001b[0m     collection,\n\u001b[0;32m    205\u001b[0m     document_ids,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    209\u001b[0m     max_document_length,\n\u001b[0;32m    210\u001b[0m )\n\u001b[1;32m--> 211\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mindex\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    212\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcollection\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    213\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpid_docid_map\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpid_docid_map\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    214\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdocid_metadata_map\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdocid_metadata_map\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    215\u001b[0m \u001b[43m    \u001b[49m\u001b[43mindex_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mindex_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    216\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmax_document_length\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmax_document_length\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    217\u001b[0m \u001b[43m    \u001b[49m\u001b[43moverwrite\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moverwrite_index\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    218\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbsize\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbsize\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    219\u001b[0m \u001b[43m    \u001b[49m\u001b[43muse_faiss\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_faiss\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    220\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mF:\\CMC\\CMC_Study\\Code\\venv\\lib\\site-packages\\ragatouille\\models\\colbert.py:341\u001b[0m, in \u001b[0;36mColBERT.index\u001b[1;34m(self, collection, pid_docid_map, docid_metadata_map, index_name, max_document_length, overwrite, bsize, use_faiss)\u001b[0m\n\u001b[0;32m    337\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdocid_pid_map[docid]\u001b[38;5;241m.\u001b[39mappend(pid)\n\u001b[0;32m    339\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdocid_metadata_map \u001b[38;5;241m=\u001b[39m docid_metadata_map\n\u001b[1;32m--> 341\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel_index \u001b[38;5;241m=\u001b[39m \u001b[43mModelIndexFactory\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconstruct\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    342\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mPLAID\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m    343\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    344\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcheckpoint\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    345\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcollection\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    346\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mindex_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    347\u001b[0m \u001b[43m    \u001b[49m\u001b[43moverwrite\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    348\u001b[0m \u001b[43m    \u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mverbose\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m!=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m    349\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbsize\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbsize\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    350\u001b[0m \u001b[43m    \u001b[49m\u001b[43muse_faiss\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_faiss\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    351\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    352\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel_index\u001b[38;5;241m.\u001b[39mconfig\n\u001b[0;32m    353\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_save_index_metadata()\n",
      "File \u001b[1;32mF:\\CMC\\CMC_Study\\Code\\venv\\lib\\site-packages\\ragatouille\\models\\index.py:485\u001b[0m, in \u001b[0;36mModelIndexFactory.construct\u001b[1;34m(index_type, config, checkpoint, collection, index_name, overwrite, verbose, **kwargs)\u001b[0m\n\u001b[0;32m    482\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m index_type \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mauto\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m    483\u001b[0m     \u001b[38;5;66;03m# NOTE: For now only PLAID indexes are supported.\u001b[39;00m\n\u001b[0;32m    484\u001b[0m     index_type \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPLAID\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m--> 485\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m ModelIndexFactory\u001b[38;5;241m.\u001b[39m_MODEL_INDEX_BY_NAME[\n\u001b[0;32m    486\u001b[0m     ModelIndexFactory\u001b[38;5;241m.\u001b[39m_raise_if_invalid_index_type(index_type)\n\u001b[0;32m    487\u001b[0m ]\u001b[38;5;241m.\u001b[39mconstruct(\n\u001b[0;32m    488\u001b[0m     config, checkpoint, collection, index_name, overwrite, verbose, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs\n\u001b[0;32m    489\u001b[0m )\n",
      "File \u001b[1;32mF:\\CMC\\CMC_Study\\Code\\venv\\lib\\site-packages\\ragatouille\\models\\index.py:150\u001b[0m, in \u001b[0;36mPLAIDModelIndex.construct\u001b[1;34m(config, checkpoint, collection, index_name, overwrite, verbose, **kwargs)\u001b[0m\n\u001b[0;32m    140\u001b[0m \u001b[38;5;129m@staticmethod\u001b[39m\n\u001b[0;32m    141\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mconstruct\u001b[39m(\n\u001b[0;32m    142\u001b[0m     config: ColBERTConfig,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    148\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[0;32m    149\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPLAIDModelIndex\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m--> 150\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m PLAIDModelIndex(config)\u001b[38;5;241m.\u001b[39mbuild(\n\u001b[0;32m    151\u001b[0m         checkpoint, collection, index_name, overwrite, verbose, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs\n\u001b[0;32m    152\u001b[0m     )\n",
      "File \u001b[1;32mF:\\CMC\\CMC_Study\\Code\\venv\\lib\\site-packages\\ragatouille\\models\\index.py:254\u001b[0m, in \u001b[0;36mPLAIDModelIndex.build\u001b[1;34m(self, checkpoint, collection, index_name, overwrite, verbose, **kwargs)\u001b[0m\n\u001b[0;32m    248\u001b[0m     indexer \u001b[38;5;241m=\u001b[39m Indexer(\n\u001b[0;32m    249\u001b[0m         checkpoint\u001b[38;5;241m=\u001b[39mcheckpoint,\n\u001b[0;32m    250\u001b[0m         config\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig,\n\u001b[0;32m    251\u001b[0m         verbose\u001b[38;5;241m=\u001b[39mverbose,\n\u001b[0;32m    252\u001b[0m     )\n\u001b[0;32m    253\u001b[0m     indexer\u001b[38;5;241m.\u001b[39mconfigure(avoid_fork_if_possible\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m--> 254\u001b[0m     \u001b[43mindexer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mindex\u001b[49m\u001b[43m(\u001b[49m\u001b[43mname\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mindex_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcollection\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcollection\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moverwrite\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moverwrite\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    256\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\n",
      "File \u001b[1;32mF:\\CMC\\CMC_Study\\Code\\venv\\lib\\site-packages\\colbert\\indexer.py:80\u001b[0m, in \u001b[0;36mIndexer.index\u001b[1;34m(self, name, collection, overwrite)\u001b[0m\n\u001b[0;32m     77\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39merase()\n\u001b[0;32m     79\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m index_does_not_exist \u001b[38;5;129;01mor\u001b[39;00m overwrite \u001b[38;5;241m!=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mreuse\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[1;32m---> 80\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m__launch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcollection\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     82\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mindex_path\n",
      "File \u001b[1;32mF:\\CMC\\CMC_Study\\Code\\venv\\lib\\site-packages\\colbert\\indexer.py:89\u001b[0m, in \u001b[0;36mIndexer.__launch\u001b[1;34m(self, collection)\u001b[0m\n\u001b[0;32m     87\u001b[0m     shared_queues \u001b[38;5;241m=\u001b[39m []\n\u001b[0;32m     88\u001b[0m     shared_lists \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m---> 89\u001b[0m     \u001b[43mlauncher\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlaunch_without_fork\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcollection\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mshared_lists\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mshared_queues\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mverbose\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     91\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m\n\u001b[0;32m     93\u001b[0m manager \u001b[38;5;241m=\u001b[39m mp\u001b[38;5;241m.\u001b[39mManager()\n",
      "File \u001b[1;32mF:\\CMC\\CMC_Study\\Code\\venv\\lib\\site-packages\\colbert\\infra\\launcher.py:93\u001b[0m, in \u001b[0;36mLauncher.launch_without_fork\u001b[1;34m(self, custom_config, *args)\u001b[0m\n\u001b[0;32m     90\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m (custom_config\u001b[38;5;241m.\u001b[39mavoid_fork_if_possible \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrun_config\u001b[38;5;241m.\u001b[39mavoid_fork_if_possible)\n\u001b[0;32m     92\u001b[0m new_config \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mtype\u001b[39m(custom_config)\u001b[38;5;241m.\u001b[39mfrom_existing(custom_config, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrun_config, RunConfig(rank\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m))\n\u001b[1;32m---> 93\u001b[0m return_val \u001b[38;5;241m=\u001b[39m \u001b[43mrun_process_without_mp\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcallee\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnew_config\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     95\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m return_val\n",
      "File \u001b[1;32mF:\\CMC\\CMC_Study\\Code\\venv\\lib\\site-packages\\colbert\\infra\\launcher.py:109\u001b[0m, in \u001b[0;36mrun_process_without_mp\u001b[1;34m(callee, config, *args)\u001b[0m\n\u001b[0;32m    106\u001b[0m os\u001b[38;5;241m.\u001b[39menviron[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCUDA_VISIBLE_DEVICES\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m,\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(\u001b[38;5;28mmap\u001b[39m(\u001b[38;5;28mstr\u001b[39m, config\u001b[38;5;241m.\u001b[39mgpus_[:config\u001b[38;5;241m.\u001b[39mnranks]))\n\u001b[0;32m    108\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m Run()\u001b[38;5;241m.\u001b[39mcontext(config, inherit_config\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m):\n\u001b[1;32m--> 109\u001b[0m     return_val \u001b[38;5;241m=\u001b[39m \u001b[43mcallee\u001b[49m\u001b[43m(\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    110\u001b[0m     torch\u001b[38;5;241m.\u001b[39mcuda\u001b[38;5;241m.\u001b[39mempty_cache()\n\u001b[0;32m    111\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m return_val\n",
      "File \u001b[1;32mF:\\CMC\\CMC_Study\\Code\\venv\\lib\\site-packages\\colbert\\indexing\\collection_indexer.py:33\u001b[0m, in \u001b[0;36mencode\u001b[1;34m(config, collection, shared_lists, shared_queues, verbose)\u001b[0m\n\u001b[0;32m     31\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mencode\u001b[39m(config, collection, shared_lists, shared_queues, verbose: \u001b[38;5;28mint\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m3\u001b[39m):\n\u001b[0;32m     32\u001b[0m     encoder \u001b[38;5;241m=\u001b[39m CollectionIndexer(config\u001b[38;5;241m=\u001b[39mconfig, collection\u001b[38;5;241m=\u001b[39mcollection, verbose\u001b[38;5;241m=\u001b[39mverbose)\n\u001b[1;32m---> 33\u001b[0m     \u001b[43mencoder\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[43mshared_lists\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mF:\\CMC\\CMC_Study\\Code\\venv\\lib\\site-packages\\colbert\\indexing\\collection_indexer.py:68\u001b[0m, in \u001b[0;36mCollectionIndexer.run\u001b[1;34m(self, shared_lists)\u001b[0m\n\u001b[0;32m     65\u001b[0m print_memory_stats(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mRANK:\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrank\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m     67\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39mresume \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msaver\u001b[38;5;241m.\u001b[39mtry_load_codec():\n\u001b[1;32m---> 68\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mshared_lists\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;66;03m# Trains centroids from selected passages\u001b[39;00m\n\u001b[0;32m     69\u001b[0m distributed\u001b[38;5;241m.\u001b[39mbarrier(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrank)\n\u001b[0;32m     70\u001b[0m print_memory_stats(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mRANK:\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrank\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m)\n",
      "File \u001b[1;32mF:\\CMC\\CMC_Study\\Code\\venv\\lib\\site-packages\\colbert\\indexing\\collection_indexer.py:237\u001b[0m, in \u001b[0;36mCollectionIndexer.train\u001b[1;34m(self, shared_lists)\u001b[0m\n\u001b[0;32m    234\u001b[0m print_memory_stats(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mRANK:\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrank\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m    235\u001b[0m \u001b[38;5;28;01mdel\u001b[39;00m sample\n\u001b[1;32m--> 237\u001b[0m bucket_cutoffs, bucket_weights, avg_residual \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_compute_avg_residual\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcentroids\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mheldout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    239\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mverbose \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[0;32m    240\u001b[0m     print_message(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mavg_residual = \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mavg_residual\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m)\n",
      "File \u001b[1;32mF:\\CMC\\CMC_Study\\Code\\venv\\lib\\site-packages\\colbert\\indexing\\collection_indexer.py:315\u001b[0m, in \u001b[0;36mCollectionIndexer._compute_avg_residual\u001b[1;34m(self, centroids, heldout)\u001b[0m\n\u001b[0;32m    314\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_compute_avg_residual\u001b[39m(\u001b[38;5;28mself\u001b[39m, centroids, heldout):\n\u001b[1;32m--> 315\u001b[0m     compressor \u001b[38;5;241m=\u001b[39m \u001b[43mResidualCodec\u001b[49m\u001b[43m(\u001b[49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcentroids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcentroids\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mavg_residual\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[0;32m    317\u001b[0m     heldout_reconstruct \u001b[38;5;241m=\u001b[39m compressor\u001b[38;5;241m.\u001b[39mcompress_into_codes(heldout, out_device\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcuda\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39muse_gpu \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcpu\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m    318\u001b[0m     heldout_reconstruct \u001b[38;5;241m=\u001b[39m compressor\u001b[38;5;241m.\u001b[39mlookup_centroids(heldout_reconstruct, out_device\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcuda\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39muse_gpu \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcpu\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "File \u001b[1;32mF:\\CMC\\CMC_Study\\Code\\venv\\lib\\site-packages\\colbert\\indexing\\codecs\\residual.py:24\u001b[0m, in \u001b[0;36mResidualCodec.__init__\u001b[1;34m(self, config, centroids, avg_residual, bucket_cutoffs, bucket_weights)\u001b[0m\n\u001b[0;32m     21\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, config, centroids, avg_residual\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, bucket_cutoffs\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, bucket_weights\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[0;32m     22\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39muse_gpu \u001b[38;5;241m=\u001b[39m config\u001b[38;5;241m.\u001b[39mtotal_visible_gpus \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[1;32m---> 24\u001b[0m     \u001b[43mResidualCodec\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtry_load_torch_extensions\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43muse_gpu\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     26\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39muse_gpu \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m     27\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcentroids \u001b[38;5;241m=\u001b[39m centroids\u001b[38;5;241m.\u001b[39mcuda()\u001b[38;5;241m.\u001b[39mhalf()\n",
      "File \u001b[1;32mF:\\CMC\\CMC_Study\\Code\\venv\\lib\\site-packages\\colbert\\indexing\\codecs\\residual.py:103\u001b[0m, in \u001b[0;36mResidualCodec.try_load_torch_extensions\u001b[1;34m(cls, use_gpu)\u001b[0m\n\u001b[0;32m    100\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m\n\u001b[0;32m    102\u001b[0m print_message(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mLoading decompress_residuals_cpp extension (set COLBERT_LOAD_TORCH_EXTENSION_VERBOSE=True for more info)...\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m--> 103\u001b[0m decompress_residuals_cpp \u001b[38;5;241m=\u001b[39m \u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    104\u001b[0m \u001b[43m    \u001b[49m\u001b[43mname\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mdecompress_residuals_cpp\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m    105\u001b[0m \u001b[43m    \u001b[49m\u001b[43msources\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\n\u001b[0;32m    106\u001b[0m \u001b[43m        \u001b[49m\u001b[43mos\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpath\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mjoin\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    107\u001b[0m \u001b[43m            \u001b[49m\u001b[43mpathlib\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mPath\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;18;43m__file__\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mparent\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mresolve\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mdecompress_residuals.cpp\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\n\u001b[0;32m    108\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    109\u001b[0m \u001b[43m        \u001b[49m\u001b[43mos\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpath\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mjoin\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    110\u001b[0m \u001b[43m            \u001b[49m\u001b[43mpathlib\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mPath\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;18;43m__file__\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mparent\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mresolve\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mdecompress_residuals.cu\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\n\u001b[0;32m    111\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    112\u001b[0m \u001b[43m    \u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    113\u001b[0m \u001b[43m    \u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mos\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgetenv\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mCOLBERT_LOAD_TORCH_EXTENSION_VERBOSE\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mFalse\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m==\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mTrue\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m    114\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    115\u001b[0m \u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39mdecompress_residuals \u001b[38;5;241m=\u001b[39m decompress_residuals_cpp\u001b[38;5;241m.\u001b[39mdecompress_residuals_cpp\n\u001b[0;32m    117\u001b[0m print_message(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mLoading packbits_cpp extension (set COLBERT_LOAD_TORCH_EXTENSION_VERBOSE=True for more info)...\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[1;32mF:\\CMC\\CMC_Study\\Code\\venv\\lib\\site-packages\\torch\\utils\\cpp_extension.py:1309\u001b[0m, in \u001b[0;36mload\u001b[1;34m(name, sources, extra_cflags, extra_cuda_cflags, extra_ldflags, extra_include_paths, build_directory, verbose, with_cuda, is_python_module, is_standalone, keep_intermediates)\u001b[0m\n\u001b[0;32m   1217\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mload\u001b[39m(name,\n\u001b[0;32m   1218\u001b[0m          sources: Union[\u001b[38;5;28mstr\u001b[39m, List[\u001b[38;5;28mstr\u001b[39m]],\n\u001b[0;32m   1219\u001b[0m          extra_cflags\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1227\u001b[0m          is_standalone\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[0;32m   1228\u001b[0m          keep_intermediates\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m):\n\u001b[0;32m   1229\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m   1230\u001b[0m \u001b[38;5;124;03m    Load a PyTorch C++ extension just-in-time (JIT).\u001b[39;00m\n\u001b[0;32m   1231\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1307\u001b[0m \u001b[38;5;124;03m        ...     verbose=True)\u001b[39;00m\n\u001b[0;32m   1308\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m-> 1309\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_jit_compile\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1310\u001b[0m \u001b[43m        \u001b[49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1311\u001b[0m \u001b[43m        \u001b[49m\u001b[43m[\u001b[49m\u001b[43msources\u001b[49m\u001b[43m]\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43misinstance\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43msources\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mstr\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43msources\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1312\u001b[0m \u001b[43m        \u001b[49m\u001b[43mextra_cflags\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1313\u001b[0m \u001b[43m        \u001b[49m\u001b[43mextra_cuda_cflags\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1314\u001b[0m \u001b[43m        \u001b[49m\u001b[43mextra_ldflags\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1315\u001b[0m \u001b[43m        \u001b[49m\u001b[43mextra_include_paths\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1316\u001b[0m \u001b[43m        \u001b[49m\u001b[43mbuild_directory\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43m_get_build_directory\u001b[49m\u001b[43m(\u001b[49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mverbose\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1317\u001b[0m \u001b[43m        \u001b[49m\u001b[43mverbose\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1318\u001b[0m \u001b[43m        \u001b[49m\u001b[43mwith_cuda\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1319\u001b[0m \u001b[43m        \u001b[49m\u001b[43mis_python_module\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1320\u001b[0m \u001b[43m        \u001b[49m\u001b[43mis_standalone\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1321\u001b[0m \u001b[43m        \u001b[49m\u001b[43mkeep_intermediates\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mkeep_intermediates\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mF:\\CMC\\CMC_Study\\Code\\venv\\lib\\site-packages\\torch\\utils\\cpp_extension.py:1745\u001b[0m, in \u001b[0;36m_jit_compile\u001b[1;34m(name, sources, extra_cflags, extra_cuda_cflags, extra_ldflags, extra_include_paths, build_directory, verbose, with_cuda, is_python_module, is_standalone, keep_intermediates)\u001b[0m\n\u001b[0;32m   1742\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m is_standalone:\n\u001b[0;32m   1743\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m _get_exec_path(name, build_directory)\n\u001b[1;32m-> 1745\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_import_module_from_library\u001b[49m\u001b[43m(\u001b[49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbuild_directory\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mis_python_module\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mF:\\CMC\\CMC_Study\\Code\\venv\\lib\\site-packages\\torch\\utils\\cpp_extension.py:2143\u001b[0m, in \u001b[0;36m_import_module_from_library\u001b[1;34m(module_name, path, is_python_module)\u001b[0m\n\u001b[0;32m   2141\u001b[0m spec \u001b[38;5;241m=\u001b[39m importlib\u001b[38;5;241m.\u001b[39mutil\u001b[38;5;241m.\u001b[39mspec_from_file_location(module_name, filepath)\n\u001b[0;32m   2142\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m spec \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m-> 2143\u001b[0m module \u001b[38;5;241m=\u001b[39m \u001b[43mimportlib\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mutil\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodule_from_spec\u001b[49m\u001b[43m(\u001b[49m\u001b[43mspec\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   2144\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(spec\u001b[38;5;241m.\u001b[39mloader, importlib\u001b[38;5;241m.\u001b[39mabc\u001b[38;5;241m.\u001b[39mLoader)\n\u001b[0;32m   2145\u001b[0m spec\u001b[38;5;241m.\u001b[39mloader\u001b[38;5;241m.\u001b[39mexec_module(module)\n",
      "File \u001b[1;32m<frozen importlib._bootstrap>:571\u001b[0m, in \u001b[0;36mmodule_from_spec\u001b[1;34m(spec)\u001b[0m\n",
      "File \u001b[1;32m<frozen importlib._bootstrap_external>:1176\u001b[0m, in \u001b[0;36mcreate_module\u001b[1;34m(self, spec)\u001b[0m\n",
      "File \u001b[1;32m<frozen importlib._bootstrap>:241\u001b[0m, in \u001b[0;36m_call_with_frames_removed\u001b[1;34m(f, *args, **kwds)\u001b[0m\n",
      "\u001b[1;31mImportError\u001b[0m: DLL load failed while importing decompress_residuals_cpp: The specified module could not be found."
     ]
    }
   ],
   "source": [
    "RAG.index(\n",
    "    collection=[full_document],\n",
    "    index_name=\"Miyazaki-123\",\n",
    "    max_document_length=180,\n",
    "    split_documents=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c87e839-2561-4424-a733-a0b51a7f5a42",
   "metadata": {},
   "outputs": [],
   "source": [
    "results = RAG.search(query=\"What animation studio did Miyazaki found?\", k=3)\n",
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d1f5ce4-692b-45b6-b1b7-491c0e2f002a",
   "metadata": {},
   "outputs": [],
   "source": [
    "retriever = RAG.as_langchain_retriever(k=3)\n",
    "retriever.invoke(\"What animation studio did Miyazaki found?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4bf243d0-bf2a-4ca3-93f2-92d752fa11cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"..\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e228c48-8787-43ae-b159-0c07eb33146a",
   "metadata": {},
   "source": [
    "# 4. RAPTOR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5ad20272-866b-467e-a344-d25bd75eaa6d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<function matplotlib.pyplot.show(close=None, block=None)>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1000x600 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import tiktoken\n",
    "from bs4 import BeautifulSoup as Soup\n",
    "from langchain_community.document_loaders.recursive_url_loader import RecursiveUrlLoader\n",
    "\n",
    "\n",
    "def num_tokens_from_string(string: str, encoding_name: str) -> int:\n",
    "    \"\"\"Returns the number of tokens in a text string.\"\"\"\n",
    "    encoding = tiktoken.get_encoding(encoding_name)\n",
    "    num_tokens = len(encoding.encode(string))\n",
    "    return num_tokens\n",
    "\n",
    "\n",
    "# LCEL docs\n",
    "url = \"https://python.langchain.com/docs/expression_language/\"\n",
    "loader = RecursiveUrlLoader(\n",
    "    url=url, max_depth=20, extractor=lambda x: Soup(x, \"html.parser\").text\n",
    ")\n",
    "docs = loader.load()\n",
    "\n",
    "# LCEL w/ PydanticOutputParser (outside the primary LCEL docs)\n",
    "url = \"https://python.langchain.com/docs/modules/model_io/output_parsers/quick_start\"\n",
    "loader = RecursiveUrlLoader(\n",
    "    url=url, max_depth=1, extractor=lambda x: Soup(x, \"html.parser\").text\n",
    ")\n",
    "docs_pydantic = loader.load()\n",
    "\n",
    "# LCEL w/ Self Query (outside the primary LCEL docs)\n",
    "url = \"https://python.langchain.com/docs/modules/data_connection/retrievers/self_query/\"\n",
    "loader = RecursiveUrlLoader(\n",
    "    url=url, max_depth=1, extractor=lambda x: Soup(x, \"html.parser\").text\n",
    ")\n",
    "docs_sq = loader.load()\n",
    "\n",
    "# url = \"https://python.langchain.com/v0.1/docs/modules/data_connection/document_loaders/custom/\"\n",
    "# loader = RecursiveUrlLoader(\n",
    "#     url=url, max_depth=1, extractor=lambda x: Soup(x, \"html.parser\").text\n",
    "# )\n",
    "# docs_dc = loader.load()\n",
    "\n",
    "# Doc texts\n",
    "docs.extend([*docs_pydantic, *docs_sq])#, *docs_dc])\n",
    "docs_texts = [d.page_content for d in docs]\n",
    "\n",
    "# Calculate the number of tokens for each document\n",
    "counts = [num_tokens_from_string(d, \"cl100k_base\") for d in docs_texts]\n",
    "\n",
    "# Plotting the histogram of token counts\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.hist(counts, bins=30, color=\"blue\", edgecolor=\"black\", alpha=0.7)\n",
    "plt.title(\"Histogram of Token Counts\")\n",
    "plt.xlabel(\"Token Count\")\n",
    "plt.ylabel(\"Frequency\")\n",
    "plt.grid(axis=\"y\", alpha=0.75)\n",
    "\n",
    "# Display the histogram\n",
    "plt.show"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "70c579a7-b377-4b50-86a9-73c20ae43f24",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(docs_texts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e0450ac2-fca4-442a-a196-71986c3be788",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['\\n\\n\\n\\n\\nLangChain Expression Language (LCEL) | 🦜️🔗 LangChain\\n\\n\\n\\n\\n\\n\\n\\nSkip to main contentLangChain v0.2 is out! You are currently viewing the old v0.1 docs. View the latest docs here.ComponentsIntegrationsGuidesAPI ReferenceMorePeopleVersioningContributingTemplatesCookbooksTutorialsYouTubev0.1v0.2v0.1🦜️🔗LangSmithLangSmith DocsLangServe GitHubTemplates GitHubTemplates HubLangChain HubJS/TS Docs💬SearchGet startedIntroductionQuickstartInstallationUse casesQ&A with RAGExtracting structured outputChatbotsTool use and agentsQuery analysisQ&A over SQL + CSVMoreExpression LanguageGet startedRunnable interfacePrimitivesAdvantages of LCELStreamingAdd message history (memory)MoreEcosystem🦜🛠️ LangSmith🦜🕸️LangGraph🦜️🏓 LangServeSecurityExpression LanguageLangChain Expression Language (LCEL)LangChain Expression Language, or LCEL, is a declarative way to easily compose chains together.\\nLCEL was designed from day 1 to support putting prototypes in production, with no code changes, from the simplest “prompt + LLM” chain to the most complex chains (we’ve seen folks successfully run LCEL chains with 100s of steps in production). To highlight a few of the reasons you might want to use LCEL:First-class streaming support\\nWhen you build your chains with LCEL you get the best possible time-to-first-token (time elapsed until the first chunk of output comes out). For some chains this means eg. we stream tokens straight from an LLM to a streaming output parser, and you get back parsed, incremental chunks of output at the same rate as the LLM provider outputs the raw tokens.Async support\\nAny chain built with LCEL can be called both with the synchronous API (eg. in your Jupyter notebook while prototyping) as well as with the asynchronous API (eg. in a LangServe server). This enables using the same code for prototypes and in production, with great performance, and the ability to handle many concurrent requests in the same server.Optimized parallel execution\\nWhenever your LCEL chains have steps that can be executed in parallel (eg if you fetch documents from multiple retrievers) we automatically do it, both in the sync and the async interfaces, for the smallest possible latency.Retries and fallbacks\\nConfigure retries and fallbacks for any part of your LCEL chain. This is a great way to make your chains more reliable at scale. We’re currently working on adding streaming support for retries/fallbacks, so you can get the added reliability without any latency cost.Access intermediate results\\nFor more complex chains it’s often very useful to access the results of intermediate steps even before the final output is produced. This can be used to let end-users know something is happening, or even just to debug your chain. You can stream intermediate results, and it’s available on every LangServe server.Input and output schemas\\nInput and output schemas give every LCEL chain Pydantic and JSONSchema schemas inferred from the structure of your chain. This can be used for validation of inputs and outputs, and is an integral part of LangServe.Seamless LangSmith tracing\\nAs your chains get more and more complex, it becomes increasingly important to understand what exactly is happening at every step.\\nWith LCEL, all steps are automatically logged to LangSmith for maximum observability and debuggability.Seamless LangServe deployment\\nAny chain created with LCEL can be easily deployed using LangServe.Help us out by providing feedback on this documentation page:PreviousWeb scrapingNextGet startedCommunityDiscordTwitterGitHubPythonJS/TSMoreHomepageBlogYouTubeCopyright © 2024 LangChain, Inc.\\n\\n\\n\\n',\n",
       " '\\n\\n\\n\\n\\nQuickstart | 🦜️🔗 LangChain\\n\\n\\n\\n\\n\\n\\n\\nSkip to main contentLangChain v0.2 is out! You are currently viewing the old v0.1 docs. View the latest docs here.ComponentsIntegrationsGuidesAPI ReferenceMorePeopleVersioningContributingTemplatesCookbooksTutorialsYouTubev0.1v0.2v0.1🦜️🔗LangSmithLangSmith DocsLangServe GitHubTemplates GitHubTemplates HubLangChain HubJS/TS Docs💬SearchModel I/OPromptsChat modelsLLMsOutput parsersQuickstartOutput ParsersCustom Output ParserstypesRetrievalDocument loadersText splittersEmbedding modelsVector storesRetrieversIndexingCompositionToolsAgentsChainsMoreComponentsModel I/OOutput parsersQuickstartOn this pageQuickstartLanguage models output text. But many times you may want to get more structured information than just text back. This is where output parsers come in.Output parsers are classes that help structure language model responses. There are two main methods an output parser must implement:\"Get format instructions\": A method which returns a string containing instructions for how the output of a language model should be formatted.\"Parse\": A method which takes in a string (assumed to be the response from a language model) and parses it into some structure.And then one optional one:\"Parse with prompt\": A method which takes in a string (assumed to be the response from a language model) and a prompt (assumed to be the prompt that generated such a response) and parses it into some structure. The prompt is largely provided in the event the OutputParser wants to retry or fix the output in some way, and needs information from the prompt to do so.Get started\\u200bBelow we go over the main type of output parser, the PydanticOutputParser.from langchain.output_parsers import PydanticOutputParserfrom langchain_core.prompts import PromptTemplatefrom langchain_core.pydantic_v1 import BaseModel, Field, validatorfrom langchain_openai import OpenAImodel = OpenAI(model_name=\"gpt-3.5-turbo-instruct\", temperature=0.0)# Define your desired data structure.class Joke(BaseModel):    setup: str = Field(description=\"question to set up a joke\")    punchline: str = Field(description=\"answer to resolve the joke\")    # You can add custom validation logic easily with Pydantic.    @validator(\"setup\")    def question_ends_with_question_mark(cls, field):        if field[-1] != \"?\":            raise ValueError(\"Badly formed question!\")        return field# Set up a parser + inject instructions into the prompt template.parser = PydanticOutputParser(pydantic_object=Joke)prompt = PromptTemplate(    template=\"Answer the user query.\\\\n{format_instructions}\\\\n{query}\\\\n\",    input_variables=[\"query\"],    partial_variables={\"format_instructions\": parser.get_format_instructions()},)# And a query intended to prompt a language model to populate the data structure.prompt_and_model = prompt | modeloutput = prompt_and_model.invoke({\"query\": \"Tell me a joke.\"})parser.invoke(output)API Reference:PydanticOutputParserPromptTemplateOpenAIJoke(setup=\\'Why did the chicken cross the road?\\', punchline=\\'To get to the other side!\\')LCEL\\u200bOutput parsers implement the Runnable interface, the basic building block of the LangChain Expression Language (LCEL). This means they support invoke, ainvoke, stream, astream, batch, abatch, astream_log calls.Output parsers accept a string or BaseMessage as input and can return an arbitrary type.parser.invoke(output)Joke(setup=\\'Why did the chicken cross the road?\\', punchline=\\'To get to the other side!\\')Instead of manually invoking the parser, we also could\\'ve just added it to our Runnable sequence:chain = prompt | model | parserchain.invoke({\"query\": \"Tell me a joke.\"})Joke(setup=\\'Why did the chicken cross the road?\\', punchline=\\'To get to the other side!\\')While all parsers support the streaming interface, only certain parsers can stream through partially parsed objects, since this is highly dependent on the output type. Parsers which cannot construct partial objects will simply yield the fully parsed output.The SimpleJsonOutputParser for example can stream through partial outputs:from langchain.output_parsers.json import SimpleJsonOutputParserjson_prompt = PromptTemplate.from_template(    \"Return a JSON object with an `answer` key that answers the following question: {question}\")json_parser = SimpleJsonOutputParser()json_chain = json_prompt | model | json_parserAPI Reference:SimpleJsonOutputParserlist(json_chain.stream({\"question\": \"Who invented the microscope?\"}))[{}, {\\'answer\\': \\'\\'}, {\\'answer\\': \\'Ant\\'}, {\\'answer\\': \\'Anton\\'}, {\\'answer\\': \\'Antonie\\'}, {\\'answer\\': \\'Antonie van\\'}, {\\'answer\\': \\'Antonie van Lee\\'}, {\\'answer\\': \\'Antonie van Leeu\\'}, {\\'answer\\': \\'Antonie van Leeuwen\\'}, {\\'answer\\': \\'Antonie van Leeuwenho\\'}, {\\'answer\\': \\'Antonie van Leeuwenhoek\\'}]While the PydanticOutputParser cannot:list(chain.stream({\"query\": \"Tell me a joke.\"}))[Joke(setup=\\'Why did the chicken cross the road?\\', punchline=\\'To get to the other side!\\')]Help us out by providing feedback on this documentation page:PreviousOutput ParsersNextOutput ParsersGet startedLCELCommunityDiscordTwitterGitHubPythonJS/TSMoreHomepageBlogYouTubeCopyright © 2024 LangChain, Inc.\\n\\n\\n\\n',\n",
       " '\\n\\n\\n\\n\\nSelf-querying | 🦜️🔗 LangChain\\n\\n\\n\\n\\n\\n\\n\\nSkip to main contentLangChain v0.2 is out! You are currently viewing the old v0.1 docs. View the latest docs here.ComponentsIntegrationsGuidesAPI ReferenceMorePeopleVersioningContributingTemplatesCookbooksTutorialsYouTubev0.1v0.2v0.1🦜️🔗LangSmithLangSmith DocsLangServe GitHubTemplates GitHubTemplates HubLangChain HubJS/TS Docs💬SearchModel I/OPromptsChat modelsLLMsOutput parsersRetrievalDocument loadersText splittersEmbedding modelsVector storesRetrieversVector store-backed retrieverRetrieversMultiQueryRetrieverContextual compressionCustom RetrieverEnsemble RetrieverLong-Context ReorderMultiVector RetrieverParent Document RetrieverSelf-queryingTime-weighted vector store retrieverIndexingCompositionToolsAgentsChainsMoreComponentsRetrievalRetrieversSelf-queryingOn this pageSelf-queryinginfoHead to Integrations for documentation on vector stores with built-in support for self-querying.A self-querying retriever is one that, as the name suggests, has the ability to query itself. Specifically, given any natural language query, the retriever uses a query-constructing LLM chain to write a structured query and then applies that structured query to its underlying VectorStore. This allows the retriever to not only use the user-input query for semantic similarity comparison with the contents of stored documents but to also extract filters from the user query on the metadata of stored documents and to execute those filters.Get started\\u200bFor demonstration purposes we\\'ll use a Chroma vector store. We\\'ve created a small demo set of documents that contain summaries of movies.Note: The self-query retriever requires you to have lark package installed.%pip install --upgrade --quiet  lark langchain-chromafrom langchain_chroma import Chromafrom langchain_core.documents import Documentfrom langchain_openai import OpenAIEmbeddingsdocs = [    Document(        page_content=\"A bunch of scientists bring back dinosaurs and mayhem breaks loose\",        metadata={\"year\": 1993, \"rating\": 7.7, \"genre\": \"science fiction\"},    ),    Document(        page_content=\"Leo DiCaprio gets lost in a dream within a dream within a dream within a ...\",        metadata={\"year\": 2010, \"director\": \"Christopher Nolan\", \"rating\": 8.2},    ),    Document(        page_content=\"A psychologist / detective gets lost in a series of dreams within dreams within dreams and Inception reused the idea\",        metadata={\"year\": 2006, \"director\": \"Satoshi Kon\", \"rating\": 8.6},    ),    Document(        page_content=\"A bunch of normal-sized women are supremely wholesome and some men pine after them\",        metadata={\"year\": 2019, \"director\": \"Greta Gerwig\", \"rating\": 8.3},    ),    Document(        page_content=\"Toys come alive and have a blast doing so\",        metadata={\"year\": 1995, \"genre\": \"animated\"},    ),    Document(        page_content=\"Three men walk into the Zone, three men walk out of the Zone\",        metadata={            \"year\": 1979,            \"director\": \"Andrei Tarkovsky\",            \"genre\": \"thriller\",            \"rating\": 9.9,        },    ),]vectorstore = Chroma.from_documents(docs, OpenAIEmbeddings())API Reference:DocumentOpenAIEmbeddingsCreating our self-querying retriever\\u200bNow we can instantiate our retriever. To do this we\\'ll need to provide some information upfront about the metadata fields that our documents support and a short description of the document contents.from langchain.chains.query_constructor.base import AttributeInfofrom langchain.retrievers.self_query.base import SelfQueryRetrieverfrom langchain_openai import ChatOpenAImetadata_field_info = [    AttributeInfo(        name=\"genre\",        description=\"The genre of the movie. One of [\\'science fiction\\', \\'comedy\\', \\'drama\\', \\'thriller\\', \\'romance\\', \\'action\\', \\'animated\\']\",        type=\"string\",    ),    AttributeInfo(        name=\"year\",        description=\"The year the movie was released\",        type=\"integer\",    ),    AttributeInfo(        name=\"director\",        description=\"The name of the movie director\",        type=\"string\",    ),    AttributeInfo(        name=\"rating\", description=\"A 1-10 rating for the movie\", type=\"float\"    ),]document_content_description = \"Brief summary of a movie\"llm = ChatOpenAI(temperature=0)retriever = SelfQueryRetriever.from_llm(    llm,    vectorstore,    document_content_description,    metadata_field_info,)API Reference:AttributeInfoSelfQueryRetrieverChatOpenAITesting it out\\u200bAnd now we can actually try using our retriever!# This example only specifies a filterretriever.invoke(\"I want to watch a movie rated higher than 8.5\")[Document(page_content=\\'Three men walk into the Zone, three men walk out of the Zone\\', metadata={\\'director\\': \\'Andrei Tarkovsky\\', \\'genre\\': \\'thriller\\', \\'rating\\': 9.9, \\'year\\': 1979}), Document(page_content=\\'A psychologist / detective gets lost in a series of dreams within dreams within dreams and Inception reused the idea\\', metadata={\\'director\\': \\'Satoshi Kon\\', \\'rating\\': 8.6, \\'year\\': 2006})]# This example specifies a query and a filterretriever.invoke(\"Has Greta Gerwig directed any movies about women\")[Document(page_content=\\'A bunch of normal-sized women are supremely wholesome and some men pine after them\\', metadata={\\'director\\': \\'Greta Gerwig\\', \\'rating\\': 8.3, \\'year\\': 2019})]# This example specifies a composite filterretriever.invoke(\"What\\'s a highly rated (above 8.5) science fiction film?\")[Document(page_content=\\'A psychologist / detective gets lost in a series of dreams within dreams within dreams and Inception reused the idea\\', metadata={\\'director\\': \\'Satoshi Kon\\', \\'rating\\': 8.6, \\'year\\': 2006}), Document(page_content=\\'Three men walk into the Zone, three men walk out of the Zone\\', metadata={\\'director\\': \\'Andrei Tarkovsky\\', \\'genre\\': \\'thriller\\', \\'rating\\': 9.9, \\'year\\': 1979})]# This example specifies a query and composite filterretriever.invoke(    \"What\\'s a movie after 1990 but before 2005 that\\'s all about toys, and preferably is animated\")[Document(page_content=\\'Toys come alive and have a blast doing so\\', metadata={\\'genre\\': \\'animated\\', \\'year\\': 1995})]Filter k\\u200bWe can also use the self query retriever to specify k: the number of documents to fetch.We can do this by passing enable_limit=True to the constructor.retriever = SelfQueryRetriever.from_llm(    llm,    vectorstore,    document_content_description,    metadata_field_info,    enable_limit=True,)# This example only specifies a relevant queryretriever.invoke(\"What are two movies about dinosaurs\")[Document(page_content=\\'A bunch of scientists bring back dinosaurs and mayhem breaks loose\\', metadata={\\'genre\\': \\'science fiction\\', \\'rating\\': 7.7, \\'year\\': 1993}), Document(page_content=\\'Toys come alive and have a blast doing so\\', metadata={\\'genre\\': \\'animated\\', \\'year\\': 1995})]Constructing from scratch with LCEL\\u200bTo see what\\'s going on under the hood, and to have more custom control, we can reconstruct our retriever from scratch.First, we need to create a query-construction chain. This chain will take a user query and generated a StructuredQuery object which captures the filters specified by the user. We provide some helper functions for creating a prompt and output parser. These have a number of tunable params that we\\'ll ignore here for simplicity.from langchain.chains.query_constructor.base import (    StructuredQueryOutputParser,    get_query_constructor_prompt,)prompt = get_query_constructor_prompt(    document_content_description,    metadata_field_info,)output_parser = StructuredQueryOutputParser.from_components()query_constructor = prompt | llm | output_parserAPI Reference:StructuredQueryOutputParserget_query_constructor_promptLet\\'s look at our prompt:print(prompt.format(query=\"dummy question\"))Your goal is to structure the user\\'s query to match the request schema provided below.<< Structured Request Schema >>When responding use a markdown code snippet with a JSON object formatted in the following schema:```json{    \"query\": string \\\\ text string to compare to document contents    \"filter\": string \\\\ logical condition statement for filtering documents}The query string should contain only text that is expected to match the contents of documents. Any conditions in the filter should not be mentioned in the query as well.A logical condition statement is composed of one or more comparison and logical operation statements.A comparison statement takes the form: comp(attr, val):comp (eq | ne | gt | gte | lt | lte | contain | like | in | nin): comparatorattr (string):  name of attribute to apply the comparison toval (string): is the comparison valueA logical operation statement takes the form op(statement1, statement2, ...):op (and | or | not): logical operatorstatement1, statement2, ... (comparison statements or logical operation statements): one or more statements to apply the operation toMake sure that you only use the comparators and logical operators listed above and no others.\\nMake sure that filters only refer to attributes that exist in the data source.\\nMake sure that filters only use the attributed names with its function names if there are functions applied on them.\\nMake sure that filters only use format YYYY-MM-DD when handling date data typed values.\\nMake sure that filters take into account the descriptions of attributes and only make comparisons that are feasible given the type of data being stored.\\nMake sure that filters are only used as needed. If there are no filters that should be applied return \"NO_FILTER\" for the filter value.<< Example 1. >>\\nData Source:{    \"content\": \"Lyrics of a song\",    \"attributes\": {        \"artist\": {            \"type\": \"string\",            \"description\": \"Name of the song artist\"        },        \"length\": {            \"type\": \"integer\",            \"description\": \"Length of the song in seconds\"        },        \"genre\": {            \"type\": \"string\",            \"description\": \"The song genre, one of \"pop\", \"rock\" or \"rap\"\"        }    }}User Query:\\nWhat are songs by Taylor Swift or Katy Perry about teenage romance under 3 minutes long in the dance pop genreStructured Request:{    \"query\": \"teenager love\",    \"filter\": \"and(or(eq(\\\\\"artist\\\\\", \\\\\"Taylor Swift\\\\\"), eq(\\\\\"artist\\\\\", \\\\\"Katy Perry\\\\\")), lt(\\\\\"length\\\\\", 180), eq(\\\\\"genre\\\\\", \\\\\"pop\\\\\"))\"}<< Example 2. >>\\nData Source:{    \"content\": \"Lyrics of a song\",    \"attributes\": {        \"artist\": {            \"type\": \"string\",            \"description\": \"Name of the song artist\"        },        \"length\": {            \"type\": \"integer\",            \"description\": \"Length of the song in seconds\"        },        \"genre\": {            \"type\": \"string\",            \"description\": \"The song genre, one of \"pop\", \"rock\" or \"rap\"\"        }    }}User Query:\\nWhat are songs that were not published on SpotifyStructured Request:{    \"query\": \"\",    \"filter\": \"NO_FILTER\"}<< Example 3. >>\\nData Source:{    \"content\": \"Brief summary of a movie\",    \"attributes\": {    \"genre\": {        \"description\": \"The genre of the movie. One of [\\'science fiction\\', \\'comedy\\', \\'drama\\', \\'thriller\\', \\'romance\\', \\'action\\', \\'animated\\']\",        \"type\": \"string\"    },    \"year\": {        \"description\": \"The year the movie was released\",        \"type\": \"integer\"    },    \"director\": {        \"description\": \"The name of the movie director\",        \"type\": \"string\"    },    \"rating\": {        \"description\": \"A 1-10 rating for the movie\",        \"type\": \"float\"    }}}User Query:\\ndummy questionStructured Request:And what our full chain produces:```pythonquery_constructor.invoke(    {        \"query\": \"What are some sci-fi movies from the 90\\'s directed by Luc Besson about taxi drivers\"    })StructuredQuery(query=\\'taxi driver\\', filter=Operation(operator=<Operator.AND: \\'and\\'>, arguments=[Comparison(comparator=<Comparator.EQ: \\'eq\\'>, attribute=\\'genre\\', value=\\'science fiction\\'), Operation(operator=<Operator.AND: \\'and\\'>, arguments=[Comparison(comparator=<Comparator.GTE: \\'gte\\'>, attribute=\\'year\\', value=1990), Comparison(comparator=<Comparator.LT: \\'lt\\'>, attribute=\\'year\\', value=2000)]), Comparison(comparator=<Comparator.EQ: \\'eq\\'>, attribute=\\'director\\', value=\\'Luc Besson\\')]), limit=None)The query constructor is the key element of the self-query retriever. To make a great retrieval system you\\'ll need to make sure your query constructor works well. Often this requires adjusting the prompt, the examples in the prompt, the attribute descriptions, etc. For an example that walks through refining a query constructor on some hotel inventory data, check out this cookbook.The next key element is the structured query translator. This is the object responsible for translating the generic StructuredQuery object into a metadata filter in the syntax of the vector store you\\'re using. LangChain comes with a number of built-in translators. To see them all head to the Integrations section.from langchain.retrievers.self_query.chroma import ChromaTranslatorretriever = SelfQueryRetriever(    query_constructor=query_constructor,    vectorstore=vectorstore,    structured_query_translator=ChromaTranslator(),)API Reference:ChromaTranslatorretriever.invoke(    \"What\\'s a movie after 1990 but before 2005 that\\'s all about toys, and preferably is animated\")[Document(page_content=\\'Toys come alive and have a blast doing so\\', metadata={\\'genre\\': \\'animated\\', \\'year\\': 1995})]Help us out by providing feedback on this documentation page:PreviousParent Document RetrieverNextTime-weighted vector store retrieverGet startedCreating our self-querying retrieverTesting it outFilter kConstructing from scratch with LCELCommunityDiscordTwitterGitHubPythonJS/TSMoreHomepageBlogYouTubeCopyright © 2024 LangChain, Inc.\\n\\n\\n\\n']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "docs_texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1e0e5d00-e425-44b5-bfcd-96815730c3e2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num tokens in all context: 5060\n"
     ]
    }
   ],
   "source": [
    "# Doc texts concat\n",
    "d_sorted = sorted(docs, key=lambda x: x.metadata[\"source\"])\n",
    "d_reversed = list(reversed(d_sorted))\n",
    "concatenated_content = \"\\n\\n\\n --- \\n\\n\\n\".join(\n",
    "    [doc.page_content for doc in d_reversed]\n",
    ")\n",
    "print(\n",
    "    \"Num tokens in all context: %s\"\n",
    "    % num_tokens_from_string(concatenated_content, \"cl100k_base\")\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5f4e437b-3c57-4aea-997c-9718b03821c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Doc texts split\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "\n",
    "chunk_size_tok = 2000\n",
    "text_splitter = RecursiveCharacterTextSplitter.from_tiktoken_encoder(\n",
    "    chunk_size=chunk_size_tok, chunk_overlap=0\n",
    ")\n",
    "texts_split = text_splitter.split_text(concatenated_content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0ea5177e-79b7-4a2c-82cf-f9a862728d64",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Quickstart | 🦜️🔗 LangChain\\n\\n\\n\\n\\n\\n\\n\\nSkip to main contentLangChain v0.2 is out! You are currently viewing the old v0.1 docs. View the latest docs here.ComponentsIntegrationsGuidesAPI ReferenceMorePeopleVersioningContributingTemplatesCookbooksTutorialsYouTubev0.1v0.2v0.1🦜️🔗LangSmithLangSmith DocsLangServe GitHubTemplates GitHubTemplates HubLangChain HubJS/TS Docs💬SearchModel I/OPromptsChat modelsLLMsOutput parsersQuickstartOutput ParsersCustom Output ParserstypesRetrievalDocument loadersText splittersEmbedding modelsVector storesRetrieversIndexingCompositionToolsAgentsChainsMoreComponentsModel I/OOutput parsersQuickstartOn this pageQuickstartLanguage models output text. But many times you may want to get more structured information than just text back. This is where output parsers come in.Output parsers are classes that help structure language model responses. There are two main methods an output parser must implement:\"Get format instructions\": A method which returns a string containing instructions for how the output of a language model should be formatted.\"Parse\": A method which takes in a string (assumed to be the response from a language model) and parses it into some structure.And then one optional one:\"Parse with prompt\": A method which takes in a string (assumed to be the response from a language model) and a prompt (assumed to be the prompt that generated such a response) and parses it into some structure. The prompt is largely provided in the event the OutputParser wants to retry or fix the output in some way, and needs information from the prompt to do so.Get started\\u200bBelow we go over the main type of output parser, the PydanticOutputParser.from langchain.output_parsers import PydanticOutputParserfrom langchain_core.prompts import PromptTemplatefrom langchain_core.pydantic_v1 import BaseModel, Field, validatorfrom langchain_openai import OpenAImodel = OpenAI(model_name=\"gpt-3.5-turbo-instruct\", temperature=0.0)# Define your desired data structure.class Joke(BaseModel):    setup: str = Field(description=\"question to set up a joke\")    punchline: str = Field(description=\"answer to resolve the joke\")    # You can add custom validation logic easily with Pydantic.    @validator(\"setup\")    def question_ends_with_question_mark(cls, field):        if field[-1] != \"?\":            raise ValueError(\"Badly formed question!\")        return field# Set up a parser + inject instructions into the prompt template.parser = PydanticOutputParser(pydantic_object=Joke)prompt = PromptTemplate(    template=\"Answer the user query.\\\\n{format_instructions}\\\\n{query}\\\\n\",    input_variables=[\"query\"],    partial_variables={\"format_instructions\": parser.get_format_instructions()},)# And a query intended to prompt a language model to populate the data structure.prompt_and_model = prompt | modeloutput = prompt_and_model.invoke({\"query\": \"Tell me a joke.\"})parser.invoke(output)API Reference:PydanticOutputParserPromptTemplateOpenAIJoke(setup=\\'Why did the chicken cross the road?\\', punchline=\\'To get to the other side!\\')LCEL\\u200bOutput parsers implement the Runnable interface, the basic building block of the LangChain Expression Language (LCEL). This means they support invoke, ainvoke, stream, astream, batch, abatch, astream_log calls.Output parsers accept a string or BaseMessage as input and can return an arbitrary type.parser.invoke(output)Joke(setup=\\'Why did the chicken cross the road?\\', punchline=\\'To get to the other side!\\')Instead of manually invoking the parser, we also could\\'ve just added it to our Runnable sequence:chain = prompt | model | parserchain.invoke({\"query\": \"Tell me a joke.\"})Joke(setup=\\'Why did the chicken cross the road?\\', punchline=\\'To get to the other side!\\')While all parsers support the streaming interface, only certain parsers can stream through partially parsed objects, since this is highly dependent on the output type. Parsers which cannot construct partial objects will simply yield the fully parsed output.The SimpleJsonOutputParser for example can stream through partial outputs:from langchain.output_parsers.json import SimpleJsonOutputParserjson_prompt = PromptTemplate.from_template(    \"Return a JSON object with an `answer` key that answers the following question: {question}\")json_parser = SimpleJsonOutputParser()json_chain = json_prompt | model | json_parserAPI Reference:SimpleJsonOutputParserlist(json_chain.stream({\"question\": \"Who invented the microscope?\"}))[{}, {\\'answer\\': \\'\\'}, {\\'answer\\': \\'Ant\\'}, {\\'answer\\': \\'Anton\\'}, {\\'answer\\': \\'Antonie\\'}, {\\'answer\\': \\'Antonie van\\'}, {\\'answer\\': \\'Antonie van Lee\\'}, {\\'answer\\': \\'Antonie van Leeu\\'}, {\\'answer\\': \\'Antonie van Leeuwen\\'}, {\\'answer\\': \\'Antonie van Leeuwenho\\'}, {\\'answer\\': \\'Antonie van Leeuwenhoek\\'}]While the PydanticOutputParser cannot:list(chain.stream({\"query\": \"Tell me a joke.\"}))[Joke(setup=\\'Why did the chicken cross the road?\\', punchline=\\'To get to the other side!\\')]Help us out by providing feedback on this documentation page:PreviousOutput ParsersNextOutput ParsersGet startedLCELCommunityDiscordTwitterGitHubPythonJS/TSMoreHomepageBlogYouTubeCopyright © 2024 LangChain, Inc.\\n\\n\\n\\n\\n\\n\\n --- \\n\\n\\n\\n\\n\\n\\n\\nSelf-querying | 🦜️🔗 LangChain',\n",
       " 'Skip to main contentLangChain v0.2 is out! You are currently viewing the old v0.1 docs. View the latest docs here.ComponentsIntegrationsGuidesAPI ReferenceMorePeopleVersioningContributingTemplatesCookbooksTutorialsYouTubev0.1v0.2v0.1🦜️🔗LangSmithLangSmith DocsLangServe GitHubTemplates GitHubTemplates HubLangChain HubJS/TS Docs💬SearchModel I/OPromptsChat modelsLLMsOutput parsersRetrievalDocument loadersText splittersEmbedding modelsVector storesRetrieversVector store-backed retrieverRetrieversMultiQueryRetrieverContextual compressionCustom RetrieverEnsemble RetrieverLong-Context ReorderMultiVector RetrieverParent Document RetrieverSelf-queryingTime-weighted vector store retrieverIndexingCompositionToolsAgentsChainsMoreComponentsRetrievalRetrieversSelf-queryingOn this pageSelf-queryinginfoHead to Integrations for documentation on vector stores with built-in support for self-querying.A self-querying retriever is one that, as the name suggests, has the ability to query itself. Specifically, given any natural language query, the retriever uses a query-constructing LLM chain to write a structured query and then applies that structured query to its underlying VectorStore. This allows the retriever to not only use the user-input query for semantic similarity comparison with the contents of stored documents but to also extract filters from the user query on the metadata of stored documents and to execute those filters.Get started\\u200bFor demonstration purposes we\\'ll use a Chroma vector store. We\\'ve created a small demo set of documents that contain summaries of movies.Note: The self-query retriever requires you to have lark package installed.%pip install --upgrade --quiet  lark langchain-chromafrom langchain_chroma import Chromafrom langchain_core.documents import Documentfrom langchain_openai import OpenAIEmbeddingsdocs = [    Document(        page_content=\"A bunch of scientists bring back dinosaurs and mayhem breaks loose\",        metadata={\"year\": 1993, \"rating\": 7.7, \"genre\": \"science fiction\"},    ),    Document(        page_content=\"Leo DiCaprio gets lost in a dream within a dream within a dream within a ...\",        metadata={\"year\": 2010, \"director\": \"Christopher Nolan\", \"rating\": 8.2},    ),    Document(        page_content=\"A psychologist / detective gets lost in a series of dreams within dreams within dreams and Inception reused the idea\",        metadata={\"year\": 2006, \"director\": \"Satoshi Kon\", \"rating\": 8.6},    ),    Document(        page_content=\"A bunch of normal-sized women are supremely wholesome and some men pine after them\",        metadata={\"year\": 2019, \"director\": \"Greta Gerwig\", \"rating\": 8.3},    ),    Document(        page_content=\"Toys come alive and have a blast doing so\",        metadata={\"year\": 1995, \"genre\": \"animated\"},    ),    Document(        page_content=\"Three men walk into the Zone, three men walk out of the Zone\",        metadata={            \"year\": 1979,            \"director\": \"Andrei Tarkovsky\",            \"genre\": \"thriller\",            \"rating\": 9.9,        },    ),]vectorstore = Chroma.from_documents(docs, OpenAIEmbeddings())API Reference:DocumentOpenAIEmbeddingsCreating our self-querying retriever\\u200bNow we can instantiate our retriever. To do this we\\'ll need to provide some information upfront about the metadata fields that our documents support and a short description of the document contents.from langchain.chains.query_constructor.base import AttributeInfofrom langchain.retrievers.self_query.base import SelfQueryRetrieverfrom langchain_openai import ChatOpenAImetadata_field_info = [    AttributeInfo(        name=\"genre\",        description=\"The genre of the movie. One of [\\'science fiction\\', \\'comedy\\', \\'drama\\', \\'thriller\\', \\'romance\\', \\'action\\', \\'animated\\']\",        type=\"string\",    ),    AttributeInfo(        name=\"year\",        description=\"The year the movie was released\",        type=\"integer\",    ),    AttributeInfo(        name=\"director\",        description=\"The name of the movie director\",        type=\"string\",    ),    AttributeInfo(        name=\"rating\", description=\"A 1-10 rating for the movie\", type=\"float\"    ),]document_content_description = \"Brief summary of a movie\"llm = ChatOpenAI(temperature=0)retriever = SelfQueryRetriever.from_llm(    llm,    vectorstore,    document_content_description,    metadata_field_info,)API Reference:AttributeInfoSelfQueryRetrieverChatOpenAITesting it out\\u200bAnd now we can actually try using our retriever!# This example only specifies a filterretriever.invoke(\"I want to watch a movie rated higher than 8.5\")[Document(page_content=\\'Three men walk into the Zone, three men walk out of the Zone\\', metadata={\\'director\\': \\'Andrei Tarkovsky\\', \\'genre\\': \\'thriller\\', \\'rating\\': 9.9, \\'year\\': 1979}), Document(page_content=\\'A psychologist / detective gets lost in a series of dreams within dreams within dreams and Inception reused the idea\\', metadata={\\'director\\': \\'Satoshi Kon\\', \\'rating\\': 8.6, \\'year\\': 2006})]# This example specifies a query and a filterretriever.invoke(\"Has Greta Gerwig directed any movies about women\")[Document(page_content=\\'A bunch of normal-sized women are supremely wholesome and some men pine after them\\', metadata={\\'director\\': \\'Greta Gerwig\\', \\'rating\\': 8.3, \\'year\\': 2019})]# This example specifies a composite filterretriever.invoke(\"What\\'s a highly rated (above 8.5) science fiction film?\")[Document(page_content=\\'A psychologist / detective gets lost in a series of dreams within dreams within dreams and Inception reused the idea\\', metadata={\\'director\\': \\'Satoshi Kon\\', \\'rating\\': 8.6, \\'year\\': 2006}), Document(page_content=\\'Three men walk into the Zone, three men walk out of the Zone\\', metadata={\\'director\\': \\'Andrei Tarkovsky\\', \\'genre\\': \\'thriller\\', \\'rating\\': 9.9, \\'year\\': 1979})]# This example specifies a query and composite filterretriever.invoke(    \"What\\'s a movie after 1990 but before 2005 that\\'s all about toys, and preferably is animated\")[Document(page_content=\\'Toys come alive and have a blast doing so\\', metadata={\\'genre\\': \\'animated\\', \\'year\\': 1995})]Filter k\\u200bWe can also use the self query retriever to specify k: the number of documents to fetch.We can do this by passing enable_limit=True to the constructor.retriever = SelfQueryRetriever.from_llm(    llm,    vectorstore,    document_content_description,    metadata_field_info,    enable_limit=True,)# This example only specifies a relevant queryretriever.invoke(\"What are two movies about dinosaurs\")[Document(page_content=\\'A bunch of scientists bring back dinosaurs and mayhem breaks loose\\', metadata={\\'genre\\': \\'science fiction\\', \\'rating\\': 7.7, \\'year\\': 1993}), Document(page_content=\\'Toys come alive and have a blast doing so\\', metadata={\\'genre\\': \\'animated\\', \\'year\\': 1995})]Constructing from scratch with LCEL\\u200bTo see what\\'s going on under the hood, and to have more custom control, we can reconstruct our retriever from scratch.First, we need to create a query-construction chain. This chain will take a user query and generated a',\n",
       " 'StructuredQuery object which captures the filters specified by the user. We provide some helper functions for creating a prompt and output parser. These have a number of tunable params that we\\'ll ignore here for simplicity.from langchain.chains.query_constructor.base import (    StructuredQueryOutputParser,    get_query_constructor_prompt,)prompt = get_query_constructor_prompt(    document_content_description,    metadata_field_info,)output_parser = StructuredQueryOutputParser.from_components()query_constructor = prompt | llm | output_parserAPI Reference:StructuredQueryOutputParserget_query_constructor_promptLet\\'s look at our prompt:print(prompt.format(query=\"dummy question\"))Your goal is to structure the user\\'s query to match the request schema provided below.<< Structured Request Schema >>When responding use a markdown code snippet with a JSON object formatted in the following schema:```json{    \"query\": string \\\\ text string to compare to document contents    \"filter\": string \\\\ logical condition statement for filtering documents}The query string should contain only text that is expected to match the contents of documents. Any conditions in the filter should not be mentioned in the query as well.A logical condition statement is composed of one or more comparison and logical operation statements.A comparison statement takes the form: comp(attr, val):comp (eq | ne | gt | gte | lt | lte | contain | like | in | nin): comparatorattr (string):  name of attribute to apply the comparison toval (string): is the comparison valueA logical operation statement takes the form op(statement1, statement2, ...):op (and | or | not): logical operatorstatement1, statement2, ... (comparison statements or logical operation statements): one or more statements to apply the operation toMake sure that you only use the comparators and logical operators listed above and no others.',\n",
       " 'Make sure that filters only refer to attributes that exist in the data source.\\nMake sure that filters only use the attributed names with its function names if there are functions applied on them.\\nMake sure that filters only use format YYYY-MM-DD when handling date data typed values.\\nMake sure that filters take into account the descriptions of attributes and only make comparisons that are feasible given the type of data being stored.\\nMake sure that filters are only used as needed. If there are no filters that should be applied return \"NO_FILTER\" for the filter value.<< Example 1. >>\\nData Source:{    \"content\": \"Lyrics of a song\",    \"attributes\": {        \"artist\": {            \"type\": \"string\",            \"description\": \"Name of the song artist\"        },        \"length\": {            \"type\": \"integer\",            \"description\": \"Length of the song in seconds\"        },        \"genre\": {            \"type\": \"string\",            \"description\": \"The song genre, one of \"pop\", \"rock\" or \"rap\"\"        }    }}User Query:\\nWhat are songs by Taylor Swift or Katy Perry about teenage romance under 3 minutes long in the dance pop genreStructured Request:{    \"query\": \"teenager love\",    \"filter\": \"and(or(eq(\\\\\"artist\\\\\", \\\\\"Taylor Swift\\\\\"), eq(\\\\\"artist\\\\\", \\\\\"Katy Perry\\\\\")), lt(\\\\\"length\\\\\", 180), eq(\\\\\"genre\\\\\", \\\\\"pop\\\\\"))\"}<< Example 2. >>\\nData Source:{    \"content\": \"Lyrics of a song\",    \"attributes\": {        \"artist\": {            \"type\": \"string\",            \"description\": \"Name of the song artist\"        },        \"length\": {            \"type\": \"integer\",            \"description\": \"Length of the song in seconds\"        },        \"genre\": {            \"type\": \"string\",            \"description\": \"The song genre, one of \"pop\", \"rock\" or \"rap\"\"        }    }}User Query:\\nWhat are songs that were not published on SpotifyStructured Request:{    \"query\": \"\",    \"filter\": \"NO_FILTER\"}<< Example 3. >>\\nData Source:{    \"content\": \"Brief summary of a movie\",    \"attributes\": {    \"genre\": {        \"description\": \"The genre of the movie. One of [\\'science fiction\\', \\'comedy\\', \\'drama\\', \\'thriller\\', \\'romance\\', \\'action\\', \\'animated\\']\",        \"type\": \"string\"    },    \"year\": {        \"description\": \"The year the movie was released\",        \"type\": \"integer\"    },    \"director\": {        \"description\": \"The name of the movie director\",        \"type\": \"string\"    },    \"rating\": {        \"description\": \"A 1-10 rating for the movie\",        \"type\": \"float\"    }}}User Query:\\ndummy questionStructured Request:And what our full chain produces:```pythonquery_constructor.invoke(    {        \"query\": \"What are some sci-fi movies from the 90\\'s directed by Luc Besson about taxi drivers\"    })StructuredQuery(query=\\'taxi driver\\', filter=Operation(operator=<Operator.AND: \\'and\\'>, arguments=[Comparison(comparator=<Comparator.EQ: \\'eq\\'>, attribute=\\'genre\\', value=\\'science fiction\\'), Operation(operator=<Operator.AND: \\'and\\'>, arguments=[Comparison(comparator=<Comparator.GTE: \\'gte\\'>, attribute=\\'year\\', value=1990), Comparison(comparator=<Comparator.LT: \\'lt\\'>, attribute=\\'year\\', value=2000)]), Comparison(comparator=<Comparator.EQ: \\'eq\\'>, attribute=\\'director\\', value=\\'Luc Besson\\')]), limit=None)The query constructor is the key element of the self-query retriever. To make a great retrieval system you\\'ll need to make sure your query constructor works well. Often this requires adjusting the prompt, the examples in the prompt, the attribute descriptions, etc. For an example that walks through refining a query constructor on some hotel inventory data, check out this cookbook.The next key element is the structured query translator. This is the object responsible for translating the generic StructuredQuery object into a metadata filter in the syntax of the vector store you\\'re using. LangChain comes with a number of built-in translators. To see them all head to the Integrations section.from langchain.retrievers.self_query.chroma import ChromaTranslatorretriever = SelfQueryRetriever(    query_constructor=query_constructor,    vectorstore=vectorstore,    structured_query_translator=ChromaTranslator(),)API Reference:ChromaTranslatorretriever.invoke(    \"What\\'s a movie after 1990 but before 2005 that\\'s all about toys, and preferably is animated\")[Document(page_content=\\'Toys come alive and have a blast doing so\\', metadata={\\'genre\\': \\'animated\\', \\'year\\': 1995})]Help us out by providing feedback on this documentation page:PreviousParent Document RetrieverNextTime-weighted vector store retrieverGet startedCreating our self-querying retrieverTesting it outFilter kConstructing from scratch with LCELCommunityDiscordTwitterGitHubPythonJS/TSMoreHomepageBlogYouTubeCopyright © 2024 LangChain, Inc.',\n",
       " '--- \\n\\n\\n\\n\\n\\n\\n\\nLangChain Expression Language (LCEL) | 🦜️🔗 LangChain\\n\\n\\n\\n\\n\\n\\n\\nSkip to main contentLangChain v0.2 is out! You are currently viewing the old v0.1 docs. View the latest docs here.ComponentsIntegrationsGuidesAPI ReferenceMorePeopleVersioningContributingTemplatesCookbooksTutorialsYouTubev0.1v0.2v0.1🦜️🔗LangSmithLangSmith DocsLangServe GitHubTemplates GitHubTemplates HubLangChain HubJS/TS Docs💬SearchGet startedIntroductionQuickstartInstallationUse casesQ&A with RAGExtracting structured outputChatbotsTool use and agentsQuery analysisQ&A over SQL + CSVMoreExpression LanguageGet startedRunnable interfacePrimitivesAdvantages of LCELStreamingAdd message history (memory)MoreEcosystem🦜🛠️ LangSmith🦜🕸️LangGraph🦜️🏓 LangServeSecurityExpression LanguageLangChain Expression Language (LCEL)LangChain Expression Language, or LCEL, is a declarative way to easily compose chains together.\\nLCEL was designed from day 1 to support putting prototypes in production, with no code changes, from the simplest “prompt + LLM” chain to the most complex chains (we’ve seen folks successfully run LCEL chains with 100s of steps in production). To highlight a few of the reasons you might want to use LCEL:First-class streaming support\\nWhen you build your chains with LCEL you get the best possible time-to-first-token (time elapsed until the first chunk of output comes out). For some chains this means eg. we stream tokens straight from an LLM to a streaming output parser, and you get back parsed, incremental chunks of output at the same rate as the LLM provider outputs the raw tokens.Async support\\nAny chain built with LCEL can be called both with the synchronous API (eg. in your Jupyter notebook while prototyping) as well as with the asynchronous API (eg. in a LangServe server). This enables using the same code for prototypes and in production, with great performance, and the ability to handle many concurrent requests in the same server.Optimized parallel execution\\nWhenever your LCEL chains have steps that can be executed in parallel (eg if you fetch documents from multiple retrievers) we automatically do it, both in the sync and the async interfaces, for the smallest possible latency.Retries and fallbacks\\nConfigure retries and fallbacks for any part of your LCEL chain. This is a great way to make your chains more reliable at scale. We’re currently working on adding streaming support for retries/fallbacks, so you can get the added reliability without any latency cost.Access intermediate results\\nFor more complex chains it’s often very useful to access the results of intermediate steps even before the final output is produced. This can be used to let end-users know something is happening, or even just to debug your chain. You can stream intermediate results, and it’s available on every LangServe server.Input and output schemas\\nInput and output schemas give every LCEL chain Pydantic and JSONSchema schemas inferred from the structure of your chain. This can be used for validation of inputs and outputs, and is an integral part of LangServe.Seamless LangSmith tracing\\nAs your chains get more and more complex, it becomes increasingly important to understand what exactly is happening at every step.\\nWith LCEL, all steps are automatically logged to LangSmith for maximum observability and debuggability.Seamless LangServe deployment\\nAny chain created with LCEL can be easily deployed using LangServe.Help us out by providing feedback on this documentation page:PreviousWeb scrapingNextGet startedCommunityDiscordTwitterGitHubPythonJS/TSMoreHomepageBlogYouTubeCopyright © 2024 LangChain, Inc.']"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "texts_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0835f098-0c5e-4b10-bae9-80a3853b6834",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(texts_split)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "bfabcf86-f6d9-4e21-8961-6d69ee1a1284",
   "metadata": {},
   "outputs": [],
   "source": [
    "# model \n",
    "from langchain_community.embeddings.ollama import OllamaEmbeddings\n",
    "from langchain_community.llms.ollama import Ollama\n",
    "\n",
    "embd  = OllamaEmbeddings(model=\"llama3\")\n",
    "model = Ollama(model=\"llama3\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c72b6243-cdf7-42d2-b803-e3ed5d0b51ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Dict, List, Optional, Tuple\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import umap\n",
    "from langchain.prompts import ChatPromptTemplate\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from sklearn.mixture import GaussianMixture\n",
    "\n",
    "RANDOM_SEED = 224  # Fixed seed for reproducibility\n",
    "\n",
    "### --- Code from citations referenced above (added comments and docstrings) --- ###\n",
    "\n",
    "\n",
    "def global_cluster_embeddings(\n",
    "    embeddings: np.ndarray,\n",
    "    dim: int,\n",
    "    n_neighbors: Optional[int] = None,\n",
    "    metric: str = \"cosine\",\n",
    ") -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Perform global dimensionality reduction on the embeddings using UMAP.\n",
    "\n",
    "    Parameters:\n",
    "    - embeddings: The input embeddings as a numpy array.\n",
    "    - dim: The target dimensionality for the reduced space.\n",
    "    - n_neighbors: Optional; the number of neighbors to consider for each point.\n",
    "                   If not provided, it defaults to the square root of the number of embeddings.\n",
    "    - metric: The distance metric to use for UMAP.\n",
    "\n",
    "    Returns:\n",
    "    - A numpy array of the embeddings reduced to the specified dimensionality.\n",
    "    \"\"\"\n",
    "    if n_neighbors is None:\n",
    "        n_neighbors = int((len(embeddings) - 1) ** 0.5)\n",
    "    return umap.UMAP(\n",
    "        n_neighbors=n_neighbors, n_components=dim, metric=metric\n",
    "    ).fit_transform(embeddings)\n",
    "\n",
    "\n",
    "def local_cluster_embeddings(\n",
    "    embeddings: np.ndarray, dim: int, num_neighbors: int = 10, metric: str = \"cosine\"\n",
    ") -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Perform local dimensionality reduction on the embeddings using UMAP, typically after global clustering.\n",
    "\n",
    "    Parameters:\n",
    "    - embeddings: The input embeddings as a numpy array.\n",
    "    - dim: The target dimensionality for the reduced space.\n",
    "    - num_neighbors: The number of neighbors to consider for each point.\n",
    "    - metric: The distance metric to use for UMAP.\n",
    "\n",
    "    Returns:\n",
    "    - A numpy array of the embeddings reduced to the specified dimensionality.\n",
    "    \"\"\"\n",
    "    return umap.UMAP(\n",
    "        n_neighbors=num_neighbors, n_components=dim, metric=metric\n",
    "    ).fit_transform(embeddings)\n",
    "\n",
    "\n",
    "def get_optimal_clusters(\n",
    "    embeddings: np.ndarray, max_clusters: int = 50, random_state: int = RANDOM_SEED\n",
    ") -> int:\n",
    "    \"\"\"\n",
    "    Xác định số lượng cụm tối ưu bằng cách sử dụng  Bayesian Information Criterion (BIC) với Gaussian Mixture Model.\n",
    "    \n",
    "    Parameters:\n",
    "    - embeddings: The input embeddings as a numpy array.\n",
    "    - max_clusters: The maximum number of clusters to consider.\n",
    "    - random_state: Seed for reproducibility.\n",
    "\n",
    "    Returns:\n",
    "    - An integer representing the optimal number of clusters found.\n",
    "    \"\"\"\n",
    "    max_clusters = min(max_clusters, len(embeddings))\n",
    "    n_clusters = np.arange(1, max_clusters)\n",
    "    bics = []\n",
    "    for n in n_clusters:\n",
    "        gm = GaussianMixture(n_components=n, random_state=random_state)\n",
    "        gm.fit(embeddings)\n",
    "        bics.append(gm.bic(embeddings))\n",
    "    return n_clusters[np.argmin(bics)]\n",
    "\n",
    "\n",
    "def GMM_cluster(embeddings: np.ndarray, threshold: float, random_state: int = 0):\n",
    "    \"\"\"\n",
    "    Cluster embeddings using a Gaussian Mixture Model (GMM) based on a probability threshold.\n",
    "\n",
    "    Parameters:\n",
    "    - embeddings: The input embeddings as a numpy array.\n",
    "    - threshold: The probability threshold for assigning an embedding to a cluster.\n",
    "    - random_state: Seed for reproducibility.\n",
    "\n",
    "    Returns:\n",
    "    - A tuple containing the cluster labels and the number of clusters determined.\n",
    "    \"\"\"\n",
    "    n_clusters = get_optimal_clusters(embeddings)\n",
    "    gm = GaussianMixture(n_components=n_clusters, random_state=random_state)\n",
    "    gm.fit(embeddings)\n",
    "    probs = gm.predict_proba(embeddings)\n",
    "    labels = [np.where(prob > threshold)[0] for prob in probs]\n",
    "    return labels, n_clusters\n",
    "\n",
    "\n",
    "def perform_clustering(\n",
    "    embeddings: np.ndarray,\n",
    "    dim: int,\n",
    "    threshold: float,\n",
    ") -> List[np.ndarray]:\n",
    "    \"\"\"\n",
    "    Thực hiện phân clustering trên các phần embeddings bằng cách trước tiên giảm kích thước của chúng trên toàn cầu, sau đó phân cụm\n",
    "    sử dụng Gaussian Mixture Model và cuối cùng thực hiện phân cụm cục bộ trong mỗi cụm toàn cầu.\n",
    "    \n",
    "    Perform clustering on the embeddings by first reducing their dimensionality globally, then clustering\n",
    "    using a Gaussian Mixture Model, and finally performing local clustering within each global cluster.\n",
    "\n",
    "    Parameters:\n",
    "    - embeddings: The input embeddings as a numpy array.\n",
    "    - dim: The target dimensionality for UMAP reduction.\n",
    "    - threshold: The probability threshold for assigning an embedding to a cluster in GMM.\n",
    "\n",
    "    Returns:\n",
    "    - A list of numpy arrays, where each array contains the cluster IDs for each embedding.\n",
    "    \"\"\"\n",
    "    if len(embeddings) <= dim + 1:\n",
    "        # Avoid clustering when there's insufficient data\n",
    "        return [np.array([0]) for _ in range(len(embeddings))]\n",
    "\n",
    "    # Global dimensionality reduction\n",
    "    reduced_embeddings_global = global_cluster_embeddings(embeddings, dim)\n",
    "    # Global clustering\n",
    "    global_clusters, n_global_clusters = GMM_cluster(\n",
    "        reduced_embeddings_global, threshold\n",
    "    )\n",
    "\n",
    "    all_local_clusters = [np.array([]) for _ in range(len(embeddings))]\n",
    "    total_clusters = 0\n",
    "\n",
    "    # Iterate through each global cluster to perform local clustering\n",
    "    for i in range(n_global_clusters):\n",
    "        # Extract embeddings belonging to the current global cluster\n",
    "        global_cluster_embeddings_ = embeddings[\n",
    "            np.array([i in gc for gc in global_clusters])\n",
    "        ]\n",
    "\n",
    "        if len(global_cluster_embeddings_) == 0:\n",
    "            continue\n",
    "        if len(global_cluster_embeddings_) <= dim + 1:\n",
    "            # Handle small clusters with direct assignment\n",
    "            local_clusters = [np.array([0]) for _ in global_cluster_embeddings_]\n",
    "            n_local_clusters = 1\n",
    "        else:\n",
    "            # Local dimensionality reduction and clustering\n",
    "            reduced_embeddings_local = local_cluster_embeddings(\n",
    "                global_cluster_embeddings_, dim\n",
    "            )\n",
    "            local_clusters, n_local_clusters = GMM_cluster(\n",
    "                reduced_embeddings_local, threshold\n",
    "            )\n",
    "\n",
    "        # Assign local cluster IDs, adjusting for total clusters already processed\n",
    "        for j in range(n_local_clusters):\n",
    "            local_cluster_embeddings_ = global_cluster_embeddings_[\n",
    "                np.array([j in lc for lc in local_clusters])\n",
    "            ]\n",
    "            indices = np.where(\n",
    "                (embeddings == local_cluster_embeddings_[:, None]).all(-1)\n",
    "            )[1]\n",
    "            for idx in indices:\n",
    "                all_local_clusters[idx] = np.append(\n",
    "                    all_local_clusters[idx], j + total_clusters\n",
    "                )\n",
    "\n",
    "        total_clusters += n_local_clusters\n",
    "\n",
    "    return all_local_clusters\n",
    "\n",
    "\n",
    "### --- Our code below --- ###\n",
    "\n",
    "\n",
    "def embed(texts):\n",
    "    \"\"\"\n",
    "    Generate embeddings for a list of text documents.\n",
    "\n",
    "    This function assumes the existence of an `embd` object with a method `embed_documents`\n",
    "    that takes a list of texts and returns their embeddings.\n",
    "\n",
    "    Parameters:\n",
    "    - texts: List[str], a list of text documents to be embedded.\n",
    "\n",
    "    Returns:\n",
    "    - numpy.ndarray: An array of embeddings for the given text documents.\n",
    "    \"\"\"\n",
    "    text_embeddings = embd.embed_documents(texts)\n",
    "    text_embeddings_np = np.array(text_embeddings)\n",
    "    return text_embeddings_np\n",
    "\n",
    "\n",
    "def embed_cluster_texts(texts):\n",
    "    \"\"\"\n",
    "    Embeds a list of texts and clusters them\n",
    "    -> embed\n",
    "    -> perform_clustering()  thực hiện phân cụm\n",
    "    -> returning a DataFrame with texts, their embeddings, and cluster labels.\n",
    "\n",
    "    Parameters:\n",
    "    - texts: List[str], a list of text documents to be processed.\n",
    "\n",
    "    Returns:\n",
    "    - pandas.DataFrame: A DataFrame containing the original texts, their embeddings, and the assigned cluster labels.\n",
    "    \"\"\"\n",
    "    text_embeddings_np = embed(texts)  # Generate embeddings\n",
    "    cluster_labels = perform_clustering(\n",
    "        text_embeddings_np, 10, 0.1\n",
    "    )  # Phân cụm on the embeddings\n",
    "    df = pd.DataFrame()  # Initialize a DataFrame to store the results\n",
    "    df[\"text\"] = texts  # Store original texts\n",
    "    df[\"embd\"] = list(text_embeddings_np)  # Store embeddings as a list in the DataFrame\n",
    "    df[\"cluster\"] = cluster_labels  # Store cluster labels\n",
    "    return df\n",
    "\n",
    "\n",
    "def fmt_txt(df: pd.DataFrame) -> str:\n",
    "    \"\"\"\n",
    "    Định dạng tài liệu văn bản trong DataFrame thành một chuỗi.\n",
    "\n",
    "    Parameters:\n",
    "    - df: DataFrame containing the 'text' column with text documents to format.\n",
    "\n",
    "    Returns:\n",
    "    - A single string where all text documents are joined by a specific delimiter.\n",
    "    \"\"\"\n",
    "    unique_txt = df[\"text\"].tolist()\n",
    "    return \"--- --- \\n --- --- \".join(unique_txt)\n",
    "\n",
    "\n",
    "def embed_cluster_summarize_texts(\n",
    "    texts: List[str], level: int\n",
    ") -> Tuple[pd.DataFrame, pd.DataFrame]:\n",
    "    \"\"\"\n",
    "    Nhúng, nhóm và tóm tắt danh sách văn bản. \n",
    "    -> tạo ra các phần nhúng cho văn bản,\n",
    "    -> phân cụm chúng dựa trên sự giống nhau, mở rộng phép gán cụm để xử lý dễ dàng hơn\n",
    "    -> sau đó tóm tắt nội dung trong mỗi cụm.\n",
    "\n",
    "    Parameters:\n",
    "    - texts: A list of text documents to be processed.\n",
    "    - level: An integer parameter that could xác định độ sâu hoặc chi tiết xử lý.\n",
    "\n",
    "    Returns:\n",
    "    - Tuple containing two DataFrames:\n",
    "      1. DataFrame đầu tiên (`df_clusters`) bao gồm các văn bản gốc, phần nhúng của chúng và các bài tập cụm.\n",
    "      2. DataFrame thứ hai (`df_summary`) chứa các bản tóm tắt cho từng cụm, mức độ chi tiết được chỉ định,\n",
    "         và các định danh cụm.\n",
    "    \"\"\"\n",
    "\n",
    "    # Nhúng và phân cụm văn bản, tạo ra một DataFrame với các cột 'text', 'embd', and 'cluster'\n",
    "    df_clusters = embed_cluster_texts(texts)\n",
    "    print(\"df_clusters: \\n\", df_clusters)\n",
    "    \"\"\"\n",
    "        data = {\n",
    "            \"text\": [\"Document 1\", \"Document 2\", \"Document 3\"],\n",
    "            \"embd\": [[0.1, 0.2], [0.3, 0.4], [0.5, 0.6]],\n",
    "            \"cluster\": [[1, 2], [1], [2, 3]]\n",
    "        }\n",
    "    \"\"\"\n",
    "    # Prepare to expand the DataFrame for thao tác các cụm dễ dàng hơn\n",
    "    expanded_list = []\n",
    "\n",
    "    # Expand DataFrame entries to document-cluster pairings for straightforward processing\n",
    "    for index, row in df_clusters.iterrows():\n",
    "        for cluster in row[\"cluster\"]:\n",
    "            expanded_list.append(\n",
    "                {\"text\": row[\"text\"], \"embd\": row[\"embd\"], \"cluster\": cluster}\n",
    "            )\n",
    "\n",
    "    # Create a new DataFrame from the expanded list\n",
    "    expanded_df = pd.DataFrame(expanded_list)\n",
    "    # Số cụm được nhận dạng\n",
    "    all_clusters = expanded_df[\"cluster\"].unique()\n",
    "    print(f\"--Generated {len(all_clusters)} clusters--\")\n",
    "\n",
    "    # Summarization\n",
    "    template = \"\"\"Here is a sub-set of LangChain Expression Language doc. \n",
    "    \n",
    "    LangChain Expression Language provides a way to compose chain in LangChain.\n",
    "    \n",
    "    Give a detailed summary of the documentation provided.\n",
    "    \n",
    "    Documentation:\n",
    "    {context}\n",
    "    \"\"\"\n",
    "    prompt = ChatPromptTemplate.from_template(template)\n",
    "    chain = prompt | model | StrOutputParser()\n",
    "\n",
    "    # Định dạng văn bản trong mỗi cụm để tóm tắt\n",
    "    summaries = []\n",
    "    for i in all_clusters:\n",
    "        df_cluster = expanded_df[expanded_df[\"cluster\"] == i]\n",
    "        formatted_txt = fmt_txt(df_cluster)                          # định dạng text trong dataframe thành chuỗi str\n",
    "        summaries.append(chain.invoke({\"context\": formatted_txt}))   # tóm tắt\n",
    "\n",
    "    # Tạo DataFrame để lưu trữ các bản tóm tắt với cụm và cấp độ tương ứng của chúng\n",
    "    df_summary = pd.DataFrame(\n",
    "        {\n",
    "            \"summaries\": summaries,\n",
    "            \"level\": [level] * len(summaries),\n",
    "            \"cluster\": list(all_clusters),\n",
    "        }\n",
    "    )\n",
    "\n",
    "    return df_clusters, df_summary\n",
    "\n",
    "\n",
    "def recursive_embed_cluster_summarize(\n",
    "    texts: List[str], level: int = 1, n_levels: int = 3\n",
    ") -> Dict[int, Tuple[pd.DataFrame, pd.DataFrame]]:\n",
    "    \"\"\"\n",
    "    Đệ quy nhúng, phân cụm và tóm tắt văn bản ở mức độ cụ thể hoặc đến khi số cụm = 1\n",
    "    kết quả được lưu trữ ở mỗi cấp độ\n",
    "    \n",
    "    Parameters:\n",
    "    - texts: List[str], texts to be processed.\n",
    "    - level: int, current recursion level (starts at 1).\n",
    "    - n_levels: int, maximum depth of recursion.\n",
    "\n",
    "    Returns:\n",
    "    - Dict[int, Tuple[pd.DataFrame, pd.DataFrame]], a dictionary where keys are the recursion\n",
    "      levels and values are tuples containing the clusters DataFrame and summaries DataFrame at that level.\n",
    "    \"\"\"\n",
    "    results = {}  # Dictionary to store results at each level\n",
    "\n",
    "    # Thực hiện nhúng, phân cụm và tóm tắt cho cấp độ hiện tại\n",
    "    df_clusters, df_summary = embed_cluster_summarize_texts(texts, level)  # số cụm và tóm tắt\n",
    "\n",
    "    # Lưu trữ kết quả của cấp độ hiện tạ\n",
    "    results[level] = (df_clusters, df_summary)\n",
    "\n",
    "    # Determine if further recursion is possible and meaningful\n",
    "    unique_clusters = df_summary[\"cluster\"].nunique()\n",
    "    if level < n_levels and unique_clusters > 1:\n",
    "        # Sử dụng tóm tắt làm văn bản đầu vào cho cấp độ đệ quy tiếp theo\n",
    "        new_texts = df_summary[\"summaries\"].tolist()\n",
    "        next_level_results = recursive_embed_cluster_summarize(\n",
    "            new_texts, level + 1, n_levels\n",
    "        )\n",
    "\n",
    "        # Hợp nhất các kết quả từ cấp độ tiếp theo vào từ điển kết quả hiện tại\n",
    "        results.update(next_level_results)\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "8f3dd7b0-4c6f-4e79-bf75-55761b9034f7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "df_clusters: \n",
      "                                                 text  \\\n",
      "0  \\n\\n\\n\\n\\nLangChain Expression Language (LCEL)...   \n",
      "1  \\n\\n\\n\\n\\nQuickstart | 🦜️🔗 LangChain\\n\\n\\n\\n\\n...   \n",
      "2  \\n\\n\\n\\n\\nSelf-querying | 🦜️🔗 LangChain\\n\\n\\n\\...   \n",
      "\n",
      "                                                embd cluster  \n",
      "0  [-0.182514950633049, -3.8791277408599854, -0.2...     [0]  \n",
      "1  [-1.1084625720977783, -2.5549447536468506, -0....     [0]  \n",
      "2  [-0.4645853042602539, -2.0416245460510254, -0....     [0]  \n",
      "expanded_df: \n",
      "                                                 text  \\\n",
      "0  \\n\\n\\n\\n\\nLangChain Expression Language (LCEL)...   \n",
      "1  \\n\\n\\n\\n\\nQuickstart | 🦜️🔗 LangChain\\n\\n\\n\\n\\n...   \n",
      "2  \\n\\n\\n\\n\\nSelf-querying | 🦜️🔗 LangChain\\n\\n\\n\\...   \n",
      "\n",
      "                                                embd  cluster  \n",
      "0  [-0.182514950633049, -3.8791277408599854, -0.2...        0  \n",
      "1  [-1.1084625720977783, -2.5549447536468506, -0....        0  \n",
      "2  [-0.4645853042602539, -2.0416245460510254, -0....        0  \n",
      "all_clusters: \n",
      " [0]\n",
      "--Generated 1 clusters--\n",
      "---------------------------\n",
      "{1: (                                                text  \\\n",
      "0  \\n\\n\\n\\n\\nLangChain Expression Language (LCEL)...   \n",
      "1  \\n\\n\\n\\n\\nQuickstart | 🦜️🔗 LangChain\\n\\n\\n\\n\\n...   \n",
      "2  \\n\\n\\n\\n\\nSelf-querying | 🦜️🔗 LangChain\\n\\n\\n\\...   \n",
      "\n",
      "                                                embd cluster  \n",
      "0  [-0.182514950633049, -3.8791277408599854, -0.2...     [0]  \n",
      "1  [-1.1084625720977783, -2.5549447536468506, -0....     [0]  \n",
      "2  [-0.4645853042602539, -2.0416245460510254, -0....     [0]  ,                                            summaries  level  cluster\n",
      "0  I've gone through the sub-set of LangChain Exp...      1        0)}\n",
      "---------------------------\n",
      "results: \n",
      " {1: (                                                text  \\\n",
      "0  \\n\\n\\n\\n\\nLangChain Expression Language (LCEL)...   \n",
      "1  \\n\\n\\n\\n\\nQuickstart | 🦜️🔗 LangChain\\n\\n\\n\\n\\n...   \n",
      "2  \\n\\n\\n\\n\\nSelf-querying | 🦜️🔗 LangChain\\n\\n\\n\\...   \n",
      "\n",
      "                                                embd cluster  \n",
      "0  [-0.182514950633049, -3.8791277408599854, -0.2...     [0]  \n",
      "1  [-1.1084625720977783, -2.5549447536468506, -0....     [0]  \n",
      "2  [-0.4645853042602539, -2.0416245460510254, -0....     [0]  ,                                            summaries  level  cluster\n",
      "0  I've gone through the sub-set of LangChain Exp...      1        0)}\n"
     ]
    }
   ],
   "source": [
    "# Build tree\n",
    "leaf_texts = docs_texts\n",
    "results = recursive_embed_cluster_summarize(leaf_texts, level=1, n_levels=3)\n",
    "print(\"---------------------------\")\n",
    "print(\"results: \\n\", results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f4cf410-3b37-4669-8b89-ac956d9fcd6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.vectorstores import Chroma\n",
    "\n",
    "# Initialize all_texts with leaf_texts\n",
    "all_texts = leaf_texts.copy()\n",
    "\n",
    "# Lặp lại các kết quả để trích xuất summaries từ mỗi cấp độ và thêm chúng vào all_texts\n",
    "for level in sorted(results.keys()):\n",
    "    # Extract summaries from the current level's DataFrame\n",
    "    summaries = results[level][1][\"summaries\"].tolist()\n",
    "    # Extend all_texts with the summaries from the current level\n",
    "    all_texts.extend(summaries)\n",
    "\n",
    "# Now, use all_texts to build the vectorstore with Chroma\n",
    "vectorstore = Chroma.from_texts(texts=all_texts, embedding=embd)\n",
    "retriever = vectorstore.as_retriever()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fcb7163-5a28-4e40-9507-8fe680306300",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain import hub\n",
    "from langchain_core.runnables import RunnablePassthrough\n",
    "\n",
    "# Prompt\n",
    "prompt = hub.pull(\"rlm/rag-prompt\")\n",
    "\n",
    "\n",
    "# Post-processing\n",
    "def format_docs(docs):\n",
    "    return \"\\n\\n\".join(doc.page_content for doc in docs)\n",
    "\n",
    "\n",
    "# Chain\n",
    "rag_chain = (\n",
    "    {\"context\": retriever | format_docs, \"question\": RunnablePassthrough()}\n",
    "    | prompt\n",
    "    | model\n",
    "    | StrOutputParser()\n",
    ")\n",
    "\n",
    "# Question\n",
    "rag_chain.invoke(\"How to define a RAG chain? Give me a specific code example.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d1b6ac8-56a6-4ce9-b34e-20dcbbf32322",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71df7cc5-11d6-4f3b-a981-aa922d1f7fc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"..\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "582b49fc-5b7c-43b0-a32e-d7f1f3a63a64",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
