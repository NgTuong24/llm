{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e169f89a-38ba-451b-9069-2bd84dbdd33d",
   "metadata": {},
   "source": [
    "# 1. \n",
    "# 2. Prompt Template\n",
    "# 3. Output-parsers\n",
    "- StrOutputParser\n",
    "- CommaSeparatedListOutputParser\n",
    "- JsonOutputParser\n",
    "# 4. Chat with Documents using Retrieval Chains\n",
    "- create_stuff_documents_chain\n",
    "- create_retrieval_chain\n",
    "# 5. Adding Chat History to Chatbot\n",
    "- MessagesPlaceholder\n",
    "- HumanMessage, AIMessage.\n",
    "- create_history_aware_retriever\n",
    "# 6. Agents with Tools\n",
    "- create_openai_functions_agent, AgentExecutor\n",
    "- TavilySearchResults\n",
    "- create_retriever_tool\n",
    "# 7. Long Term Chat Memory with Upstash Redis\n",
    "# 8. Query Translation -- Multi Query\n",
    "- sinh thêm câu truy vấn\n",
    "# 9. Query Translation -- RAG Fusion\n",
    "- sinh thêm câu truy vấn\n",
    "- reciprocal_rank_fusion: rank documents\n",
    "# 10. Query Translation -- Decomposition tách câu hỏi thành từng phần để hỏi\n",
    "- Answer recursively - trả lời đệ quy, kết hợp phần trả lời của câu trước để trả lời cho câu phụ tiếp theo\n",
    "- Answer individually - trả lời riêng từng câu\n",
    "# 11. Query Translation -- Step Back\n",
    "- sinh thêm câu hỏi lùi mức độ cao hơn, trừu tượng hơn\n",
    "# 12. Query Translation -- HyDE\n",
    "- tạo ra 1 tài liệu giả định từ llm -> nhúng, truy suất tài liệu\n",
    "# 13. Routing -- Logical routing* | Semantic routing\n",
    "- llm -> chọn đường tới datasource liên quan tới câu hỏi\n",
    "- Logical routing: logic\n",
    "- Semantic routing: ngữ nghĩa query-prompt | cosine_similarity()\n",
    "# 14. Query Structuring*\n",
    "- Chuyển câu hỏi truy vấn ở dạng ngôn ngữ tự nhiên sang có cấu trúc | pydantic \n",
    "- test Self-querying"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "316ed6cf-6649-47a9-b0d7-6c501d78faa2",
   "metadata": {},
   "source": [
    "# ############################################################################## #"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5a6ac371-8542-414b-9e28-73a64124fcfa",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['LANGCHAIN_TRACING_V2'] = 'true'\n",
    "os.environ['LANGCHAIN_ENDPOINT'] = 'https://api.smith.langchain.com'\n",
    "os.environ['LANGCHAIN_API_KEY'] = \"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\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0028d86-8a1b-4f4a-b035-fa979f69f8fe",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# 1. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95da1da9-a71b-4a41-b283-9f3b48c982dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "\n",
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "llm = ChatOpenAI(\n",
    "    model=\"gpt-3.5-turbo\",\n",
    "    temperature=0.7,\n",
    ")\n",
    "\n",
    "response = llm.stream(\"Write a poem about AI\")\n",
    "# print(response)\n",
    "\n",
    "for chunk in response:\n",
    "    print(chunk.content, end=\"\", flush=True)  # flush hiện thị đầu ra ngay lập tức"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b60cb8d1-a604-430b-b50c-2564aca4c42b",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# 2. Prompt Template\n",
    "# from langchain_core.prompts import ChatPromptTemplate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fea3ce98-4bf2-4cd2-a958-bf515dcc3a23",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "\n",
    "# Instantiate Model\n",
    "llm = ChatOpenAI(\n",
    "    temperature=0.7,\n",
    "    model=\"gpt-3.5-turbo-1106\",\n",
    ")\n",
    "\n",
    "# Prompt Template\n",
    "prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\", \"Generate a list of 10 synonyms for the following word. Return the results as a comma seperated list.\"),\n",
    "        (\"human\", \"{input}\")\n",
    "    ]\n",
    ")\n",
    "\n",
    "# Create LLM Chain\n",
    "chain = prompt | llm\n",
    "\n",
    "response = chain.invoke({\"input\": \"happy\"})\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c051ca1-3108-4ab2-9c4c-417e8616861a",
   "metadata": {},
   "source": [
    "# 3. Output-parsers\n",
    "\n",
    "# from langchain_core.output_parsers import StrOutputParser, CommaSeparatedListOutputParser, JsonOutputParser\n",
    "# from langchain_core.pydantic_v1 import BaseModel, Field\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "963f42f4-1954-4230-97e7-a0cd9815b969",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'properties': {'recipe': 'Margherita Pizza', 'ingredients': ['tomatoes', 'onions', 'cheese', 'basil']}}\n"
     ]
    }
   ],
   "source": [
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.output_parsers import StrOutputParser, CommaSeparatedListOutputParser, JsonOutputParser\n",
    "from langchain_core.pydantic_v1 import BaseModel, Field\n",
    "\n",
    "# model = ChatOpenAI(model=\"gpt-3.5-turbo-1106\", temperature=0.7)\n",
    "from langchain_community.llms.ollama import Ollama\n",
    "model = Ollama(model=\"llama3\")\n",
    "\n",
    "def call_string_output_parser():\n",
    "    prompt = ChatPromptTemplate.from_messages([\n",
    "        (\"system\", \"Tell me a joke about the following subject\"),\n",
    "        (\"human\", \"{input}\")\n",
    "    ])\n",
    "\n",
    "    parser = StrOutputParser()\n",
    "\n",
    "    chain = prompt | model | parser\n",
    "\n",
    "    return chain.invoke({\n",
    "        \"input\": \"dog\"\n",
    "    })\n",
    "\n",
    "def call_list_output_parser():\n",
    "    prompt = ChatPromptTemplate.from_messages([\n",
    "        (\"system\", \"Generate a list of 10 synonyms for the following word. Return the results as a comma seperated list.\"),\n",
    "        (\"human\", \"{input}\")\n",
    "    ])\n",
    "\n",
    "    parser = CommaSeparatedListOutputParser()\n",
    "    \n",
    "    chain = prompt | model | parser\n",
    "\n",
    "    return chain.invoke({\n",
    "        \"input\": \"happy\"\n",
    "    })\n",
    "\n",
    "def call_json_output_parser():\n",
    "    prompt = ChatPromptTemplate.from_messages([\n",
    "        (\"system\", \"Extract information from the following phrase.\\nFormatting Instructions: {format_instructions}\"),\n",
    "        (\"human\", \"{phrase}\")\n",
    "    ])\n",
    "\n",
    "    class Person(BaseModel):\n",
    "        recipe: str = Field(description=\"the name of the recipe\")\n",
    "        ingredients: list = Field(description=\"ingredients\")\n",
    "        \n",
    "\n",
    "    parser = JsonOutputParser(pydantic_object=Person)\n",
    "\n",
    "    chain = prompt | model | parser\n",
    "    \n",
    "    return chain.invoke({\n",
    "        \"phrase\": \"The ingredients for a Margherita pizza are tomatoes, onions, cheese, basil\",\n",
    "        \"format_instructions\": parser.get_format_instructions()\n",
    "    })\n",
    "\n",
    "# print(type(call_string_output_parser()))\n",
    "# print(type(call_list_output_parser()))\n",
    "print(call_json_output_parser())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81f10582-2cf2-4bc3-95a2-30072ca04592",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# 4. Chat with Documents using Retrieval Chains\n",
    "# from langchain.chains.combine_documents import create_stuff_documents_chain  -> StrOutputParser.\n",
    "# from langchain.chains import create_retrieval_chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3da45101-e513-4ed1-bb9b-88956babedaf",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain.chains.combine_documents import create_stuff_documents_chain\n",
    "from langchain_community.document_loaders import WebBaseLoader\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "from langchain_community.vectorstores.faiss import FAISS\n",
    "from langchain.chains import create_retrieval_chain\n",
    "\n",
    "# Retrieve Data\n",
    "def get_docs():\n",
    "    loader = WebBaseLoader('https://python.langchain.com/docs/expression_language/')\n",
    "    docs = loader.load()\n",
    "\n",
    "    text_splitter = RecursiveCharacterTextSplitter(\n",
    "        chunk_size=200,\n",
    "        chunk_overlap=20\n",
    "    )\n",
    "\n",
    "    splitDocs = text_splitter.split_documents(docs)\n",
    "\n",
    "    return splitDocs\n",
    "\n",
    "def create_vector_store(docs):\n",
    "    embedding = OpenAIEmbeddings()\n",
    "    vectorStore = FAISS.from_documents(docs, embedding=embedding)\n",
    "    return vectorStore\n",
    "\n",
    "\n",
    "def create_chain(vectorStore):\n",
    "    model = ChatOpenAI(\n",
    "        temperature=0.4,\n",
    "        model='gpt-3.5-turbo-1106'\n",
    "    )\n",
    "\n",
    "    prompt = ChatPromptTemplate.from_template(\"\"\"\n",
    "    Answer the user's question.\n",
    "    Context: {context}\n",
    "    Question: {input}\n",
    "    \"\"\")\n",
    "\n",
    "    # chain = prompt | model\n",
    "    document_chain = create_stuff_documents_chain(\n",
    "        llm=model,\n",
    "        prompt=prompt\n",
    "    )\n",
    "\n",
    "    retriever = vectorStore.as_retriever()\n",
    "\n",
    "    retrieval_chain = create_retrieval_chain(retriever, document_chain)\n",
    "\n",
    "    return retrieval_chain\n",
    "\n",
    "\n",
    "docs = get_docs()\n",
    "vectorStore = create_vector_store(docs)\n",
    "chain = create_chain(vectorStore)\n",
    "\n",
    "response = chain.invoke({\n",
    "    \"input\": \"What is LCEL?\",\n",
    "})\n",
    "\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7b3e9f1-fe9a-4f24-8a6f-cb37cb95648d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Optional\n",
    "\n",
    "from langchain.chains import create_structured_output_runnable\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_core.pydantic_v1 import BaseModel, Field\n",
    "\n",
    "\n",
    "class RecordDog(BaseModel):\n",
    "    '''Record some identifying information about a dog.'''\n",
    "\n",
    "    name: str = Field(..., description=\"The dog's name\")\n",
    "    color: str = Field(..., description=\"The dog's color\")\n",
    "    fav_food: Optional[str] = Field(None, description=\"The dog's favorite food\")\n",
    "\n",
    "llm = ChatOpenAI(model=\"gpt-3.5-turbo-0125\", temperature=0)\n",
    "prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\", \"You are an extraction algorithm. Please extract every possible instance\"),\n",
    "        ('human', '{input}')\n",
    "    ]\n",
    ")\n",
    "structured_llm = create_structured_output_runnable(\n",
    "    RecordDog,\n",
    "    llm,\n",
    "    mode=\"openai-tools\",\n",
    "    enforce_function_usage=True,\n",
    "    return_single=True\n",
    ")\n",
    "structured_llm.invoke({\"input\": \"Harry was a chubby brown beagle who loved chicken\"})\n",
    "# -> RecordDog(name=\"Harry\", color=\"brown\", fav_food=\"chicken\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0739c54-bff2-45f0-a1e8-9ee8e00802c6",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# 5. Adding Chat History to Chatbot\n",
    "\n",
    "# from langchain_core.prompts import MessagesPlaceholder\n",
    "# from langchain_core.messages import HumanMessage, AIMessage\n",
    "# from langchain.chains.history_aware_retriever import create_history_aware_retriever\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1527d714-e1e1-4a53-a15f-d19e0eba402b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain.chains.combine_documents import create_stuff_documents_chain\n",
    "from langchain_community.document_loaders import WebBaseLoader\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "from langchain_community.vectorstores.faiss import FAISS\n",
    "from langchain.chains import create_retrieval_chain\n",
    "\n",
    "# Conversation imports\n",
    "from langchain_core.prompts import MessagesPlaceholder\n",
    "from langchain_core.messages import HumanMessage, AIMessage\n",
    "from langchain.chains.history_aware_retriever import create_history_aware_retriever\n",
    "\n",
    "def get_documents_from_web(url):\n",
    "    loader = WebBaseLoader(url)\n",
    "    docs = loader.load()\n",
    "    \n",
    "    splitter = RecursiveCharacterTextSplitter(\n",
    "        chunk_size=400,\n",
    "        chunk_overlap=20\n",
    "    )\n",
    "    splitDocs = splitter.split_documents(docs)\n",
    "    return splitDocs\n",
    "\n",
    "def create_db(docs):\n",
    "    embedding = OpenAIEmbeddings()\n",
    "    vectorStore = FAISS.from_documents(docs, embedding=embedding)\n",
    "    return vectorStore\n",
    "\n",
    "def create_chain(vectorStore):\n",
    "    model = ChatOpenAI(\n",
    "        model=\"gpt-3.5-turbo-1106\",\n",
    "        temperature=0.4\n",
    "    )\n",
    "\n",
    "    prompt = ChatPromptTemplate.from_messages([\n",
    "        (\"system\", \"Answer the user's questions based on the context: {context}\"),\n",
    "        MessagesPlaceholder(variable_name=\"chat_history\"),\n",
    "        (\"user\", \"{input}\")\n",
    "    ])\n",
    "\n",
    "    # chain = prompt | model\n",
    "    chain = create_stuff_documents_chain(\n",
    "        llm=model,\n",
    "        prompt=prompt\n",
    "    )\n",
    "\n",
    "    # Replace retriever with history aware retriever\n",
    "    retriever = vectorStore.as_retriever(search_kwargs={\"k\": 3})\n",
    "\n",
    "    retriever_prompt = ChatPromptTemplate.from_messages([\n",
    "        MessagesPlaceholder(variable_name=\"chat_history\"),\n",
    "        (\"user\", \"{input}\"),\n",
    "        (\"user\", \"Given the above conversation, generate a search query to look up in order to get information relevant to the conversation\")\n",
    "    ])\n",
    "    history_aware_retriever = create_history_aware_retriever(\n",
    "        llm=model,\n",
    "        retriever=retriever,\n",
    "        prompt=retriever_prompt\n",
    "    )\n",
    "\n",
    "    retrieval_chain = create_retrieval_chain(\n",
    "        # retriever, Replace with History Aware Retriever\n",
    "        history_aware_retriever,\n",
    "        chain\n",
    "    )\n",
    "\n",
    "    return retrieval_chain\n",
    "\n",
    "\n",
    "def process_chat(chain, question, chat_history):\n",
    "    response = chain.invoke({\n",
    "        \"chat_history\": chat_history,\n",
    "        \"input\": question,\n",
    "    })\n",
    "    return response[\"answer\"]\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    docs = get_documents_from_web('https://python.langchain.com/docs/expression_language/')\n",
    "    vectorStore = create_db(docs)\n",
    "    chain = create_chain(vectorStore)\n",
    "\n",
    "    # Initialize chat history\n",
    "    chat_history = []\n",
    "\n",
    "    while True:\n",
    "        user_input = input(\"You: \")\n",
    "        if user_input.lower() == 'exit':\n",
    "            break\n",
    "        response = process_chat(chain, user_input, chat_history)\n",
    "        chat_history.append(HumanMessage(content=user_input))\n",
    "        chat_history.append(AIMessage(content=response))\n",
    "        print(\"Assistant:\", response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82c237f8-73e6-4f6e-abb2-9d8406eeb229",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# 6. Agents with Tools\n",
    "# from langchain.agents import create_openai_functions_agent, AgentExecutor\n",
    "# from langchain_community.tools.tavily_search import TavilySearchResults\n",
    "# from langchain.tools.retriever import create_retriever_tool"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce570226-54af-4131-93e1-7378f8449ef3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder\n",
    "from langchain.agents import create_openai_functions_agent, AgentExecutor\n",
    "from langchain_community.tools.tavily_search import TavilySearchResults\n",
    "from langchain_core.messages import HumanMessage, AIMessage\n",
    "from langchain.tools.retriever import create_retriever_tool\n",
    "\n",
    "from langchain_community.document_loaders import WebBaseLoader\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "from langchain_community.vectorstores.faiss import FAISS\n",
    "\n",
    "# Create Retriever\n",
    "loader = WebBaseLoader(\"https://python.langchain.com/docs/expression_language/\")\n",
    "docs = loader.load()\n",
    "    \n",
    "splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=200,\n",
    "    chunk_overlap=20\n",
    ")\n",
    "splitDocs = splitter.split_documents(docs)\n",
    "\n",
    "embedding = OpenAIEmbeddings()\n",
    "vectorStore = FAISS.from_documents(docs, embedding=embedding)\n",
    "retriever = vectorStore.as_retriever(search_kwargs={\"k\": 3})\n",
    "\n",
    "model = ChatOpenAI(\n",
    "    model='gpt-3.5-turbo-1106',\n",
    "    temperature=0.7\n",
    ")\n",
    "\n",
    "prompt = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", \"You are a friendly assistant called Max.\"),\n",
    "    MessagesPlaceholder(variable_name=\"chat_history\"),\n",
    "    (\"human\", \"{input}\"),\n",
    "    MessagesPlaceholder(variable_name=\"agent_scratchpad\")\n",
    "])\n",
    "\n",
    "search = TavilySearchResults()\n",
    "retriever_tools = create_retriever_tool(\n",
    "    retriever,\n",
    "    \"lcel_search\",\n",
    "    \"Use this tool when searching for information about Langchain Expression Language (LCEL).\"\n",
    ")\n",
    "tools = [search, retriever_tools]\n",
    "\n",
    "agent = create_openai_functions_agent(\n",
    "    llm=model,\n",
    "    prompt=prompt,\n",
    "    tools=tools\n",
    ")\n",
    "\n",
    "agentExecutor = AgentExecutor(\n",
    "    agent=agent,\n",
    "    tools=tools\n",
    ")\n",
    "\n",
    "def process_chat(agentExecutor, user_input, chat_history):\n",
    "    response = agentExecutor.invoke({\n",
    "        \"input\": user_input,\n",
    "        \"chat_history\": chat_history\n",
    "    })\n",
    "    return response[\"output\"]\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    chat_history = []\n",
    "\n",
    "    while True:\n",
    "        user_input = input(\"You: \")\n",
    "        if user_input.lower() == 'exit':\n",
    "            break\n",
    "\n",
    "        response = process_chat(agentExecutor, user_input, chat_history)\n",
    "        chat_history.append(HumanMessage(content=user_input))\n",
    "        chat_history.append(AIMessage(content=response))\n",
    "\n",
    "        print(\"Assistant:\", response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "394599d6-215c-4d93-af58-ebdb5762a02b",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# 7. Long Term Chat Memory with Upstash Redis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "951ba2c6-9eb5-485e-9d78-b191b0492002",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain.memory import ConversationBufferMemory\n",
    "from langchain.chains import LLMChain\n",
    "from langchain_community.chat_message_histories.upstash_redis import (\n",
    "    UpstashRedisChatMessageHistory,\n",
    ")\n",
    "from langchain.prompts import (\n",
    "    ChatPromptTemplate,\n",
    "    MessagesPlaceholder,\n",
    ")\n",
    "\n",
    "model = ChatOpenAI(\n",
    "    model=\"gpt-3.5-turbo\",\n",
    "    temperature=0.6\n",
    ")\n",
    "\n",
    "prompt = ChatPromptTemplate.from_messages([\n",
    "        (\"system\", \"You are a friendly AI assistant.\"),\n",
    "        MessagesPlaceholder(variable_name=\"chat_history\"),\n",
    "        (\"human\", \"{input}\")\n",
    "    ])\n",
    "\n",
    "\n",
    "URL = \"\"\n",
    "TOKEN =\"\"\n",
    "history = UpstashRedisChatMessageHistory(\n",
    "    url=URL, token=TOKEN, ttl=500, session_id=\"chat1\"\n",
    ")\n",
    "\n",
    "memory = ConversationBufferMemory(\n",
    "    memory_key=\"chat_history\",\n",
    "    return_messages=True,\n",
    "    chat_memory=history,\n",
    ")\n",
    "\n",
    "# chain = prompt | model\n",
    "chain = LLMChain(\n",
    "    llm=model,\n",
    "    prompt=prompt,\n",
    "    verbose=True,\n",
    "    memory=memory\n",
    ")\n",
    "\n",
    "\n",
    "# Prompt 1\n",
    "q1 = { \"input\": \"My name is Leon\" }\n",
    "resp1 = chain.invoke(q1)\n",
    "print(resp1[\"text\"])\n",
    "\n",
    "# Prompt 2\n",
    "q2 = { \"input\": \"What is my name?\" }\n",
    "resp2 = chain.invoke(q2)\n",
    "print(resp2[\"text\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2239478a-e4fe-4d0a-b094-c7bf9375b052",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "89c7993c-287a-4e75-b33c-5a78f97afd4c",
   "metadata": {},
   "source": [
    "# ############## Query Translation ############## #"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c49c51a2-cfb7-446a-ac50-4da6fce045c2",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# 8. Query Translation -- Multi Query\n",
    "# sinh thêm câu hỏi truy vấn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "fe68bf97-01eb-41c3-ac3f-bfdda3415050",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.embeddings.ollama import OllamaEmbeddings\n",
    "from langchain_community.llms.ollama import Ollama\n",
    "\n",
    "embeddings = OllamaEmbeddings(model=\"llama3\")\n",
    "model = Ollama(model=\"llama3\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "761ee107-eac7-425d-9a62-c4c476f53582",
   "metadata": {},
   "outputs": [],
   "source": [
    "#### INDEXING ####\n",
    "\n",
    "# # Load blog\n",
    "# import bs4\n",
    "# from langchain_community.document_loaders import WebBaseLoader\n",
    "# loader = WebBaseLoader(\n",
    "#     web_paths=(\"https://lilianweng.github.io/posts/2023-06-23-agent/\",),\n",
    "#     bs_kwargs=dict(\n",
    "#         parse_only=bs4.SoupStrainer(\n",
    "#             class_=(\"post-content\", \"post-title\", \"post-header\")\n",
    "#         )\n",
    "#     ),\n",
    "# )\n",
    "# blog_docs = loader.load()\n",
    "\n",
    "# # Split\n",
    "# from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "# text_splitter = RecursiveCharacterTextSplitter.from_tiktoken_encoder(\n",
    "#     chunk_size=300, \n",
    "#     chunk_overlap=50)\n",
    "\n",
    "# # Make splits\n",
    "# splits = text_splitter.split_documents(blog_docs)\n",
    "\n",
    "# # Index\n",
    "# from langchain_openai import OpenAIEmbeddings\n",
    "# from langchain_community.vectorstores import Chroma\n",
    "# vectorstore = Chroma.from_documents(documents=splits, \n",
    "#                                     embedding=embeddings)\n",
    "\n",
    "# retriever = vectorstore.as_retriever()\n",
    "\n",
    "from langchain.chains import create_retrieval_chain\n",
    "from langchain_community.embeddings.ollama import OllamaEmbeddings\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain.chains.combine_documents import create_stuff_documents_chain\n",
    "from langchain.vectorstores.chroma import Chroma\n",
    "from langchain_community.llms.ollama import Ollama\n",
    "\n",
    "from langchain_core.prompts import MessagesPlaceholder\n",
    "from langchain_core.messages import HumanMessage, AIMessage\n",
    "from langchain.chains.history_aware_retriever import create_history_aware_retriever\n",
    "\n",
    "CHROMA_PATH = \"chroma\"\n",
    "DATA_PATH = \"data\"\n",
    "\n",
    "embeddings = OllamaEmbeddings(model=\"llama3\")\n",
    "\n",
    "db = Chroma(persist_directory=CHROMA_PATH, embedding_function=embeddings)\n",
    "\n",
    "model = Ollama(model=\"llama3\")\n",
    "# prompt = ChatPromptTemplate.from_messages([\n",
    "#     (\"system\", \"Answer the user's questions based on the context: {context}\"),\n",
    "#     MessagesPlaceholder(variable_name=\"chat_history\"),\n",
    "#     (\"user\", \"{input}\")\n",
    "# ])\n",
    "\n",
    "# document_chain = create_stuff_documents_chain(llm=model, prompt=prompt)\n",
    "\n",
    "# retriever = db.as_retriever(search_kwargs={\"k\": 3})\n",
    "retriever = db.as_retriever()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "77f3002c-f202-4733-ba49-084c3a27fc0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prompt\n",
    "from langchain.prompts import ChatPromptTemplate\n",
    "\n",
    "# Multi Query: Different Perspectives\n",
    "template = \"\"\"You are an AI language model assistant. Your task is to generate five \n",
    "different versions of the given user question to retrieve relevant documents from a vector \n",
    "database. By generating multiple perspectives on the user question, your goal is to help\n",
    "the user overcome some of the limitations of the distance-based similarity search. \n",
    "Provide these alternative questions separated by newlines. Original question: {question}\"\"\"\n",
    "prompt_perspectives = ChatPromptTemplate.from_template(template)\n",
    "\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "generate_queries = (\n",
    "    prompt_perspectives \n",
    "    | model\n",
    "    | StrOutputParser() \n",
    "    | (lambda x: x.split(\"\\n\"))\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "215d4556-7cf1-49d7-8644-2b414f8afbc7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10\n",
      "[Document(page_content='42. El-Rashidy N, Ebrahim N, El Ghamry A, Talaat FM (2022)\\nUtilizing fog computing and explainable deep learning tech-niques for gestational diabetes prediction. Neural Comput Appl.https://doi.org/10.1007/s00521-022-08007-5\\n43. El-Balka RM et al (2022) Enhancing the performance of smart\\nelectrical grids using data mining and fuzzy inference engine.Multimed Tools Appl 81(23):33017–33049\\n44. Talaat FM (2022) Effective deep Q-networks (EDQN) strategy\\nfor resource allocation based on optimized reinforcement learn-\\ning algorithm. Multimed Tools Appl 81:39945–39961\\n45. Alshathri S, Talaat FM, Nasr AA (2022) A new reliable system\\nfor managing virtual cloud network. Comput Mater Continua\\n73(3):5863–5885. https://doi.org/10.32604/cmc.2022.026547', metadata={'id': 'data\\\\yolov8.pdf:15:2', 'page': 15, 'source': 'data\\\\yolov8.pdf'}), Document(page_content='F1 score, demonstrating its effectiveness in detecting ﬁres\\nFig. 7 Precision–recall curve of the proposed model20950 Neural Computing and Applications (2023) 35:20939–20954\\n123', metadata={'id': 'data\\\\yolov8.pdf:11:2', 'page': 11, 'source': 'data\\\\yolov8.pdf'}), Document(page_content='tative of the overall dataset. Other pre-processing steps,\\nsuch as resizing or normalizing the data, may also benecessary. The goal is to have a large enough, balanced\\ndataset that can generalize well to new data.\\nStep 3: Model selection This step involves selecting the\\nappropriate object detection algorithm for training the ﬁre\\ndetection model. There are several algorithms to choose\\nfrom, such as YOLOv8, Faster R-CNN, and SSD, eachwith its own advantages and disadvantages. The selected\\nalgorithm should have good performance on the collected\\ndataset and be capable of handling different ﬁre scenarios,depending on the requirements of the smart ﬁre detection\\nsystem. YOLOv8 is a popular choice due to its speed andaccuracy, but other algorithms can also be used based on\\nspeciﬁc needs.', metadata={'id': 'data\\\\yolov8.pdf:7:1', 'page': 7, 'source': 'data\\\\yolov8.pdf'}), Document(page_content='costs associated with false alarms.\\n•Cost-effective: The proposed approach may be cost-\\neffective compared to traditional ﬁre detection methods\\nas it can be implemented using low-cost cameras and\\nhardware, reducing the need for expensive ﬁre detectionsystems.\\n•Large dataset: Unlike other methods that use small\\nnumber of datasets, a large dataset containing ﬁre,smoke, and normal scenes is used. The dataset has real-\\nworld images and videos collected from various\\nsources. The dataset has a diverse range of ﬁrescenarios, including indoor and outdoor ﬁres, small\\nand large ﬁres, and low-light and high-light conditions.\\nA deep CNN gathers essential data from big datasets toproduce precise predictions and reduce overﬁtting.\\nThe following is how the remaining work is structured.', metadata={'id': 'data\\\\yolov8.pdf:1:5', 'page': 1, 'source': 'data\\\\yolov8.pdf'}), Document(page_content='(SFDS). The SFDS can detect ﬁre hazards in different\\nlocations, including government buildings, public areas,hospitals, residential areas, and highways, and ensure the\\nsafety of people and property. The Application layer plays\\na signiﬁcant role in enabling the SFDS to serve various\\nFig. 2 YOLO v8 architecture [ 32]20944 Neural Computing and Applications (2023) 35:20939–20954\\n123', metadata={'id': 'data\\\\yolov8.pdf:5:1', 'page': 5, 'source': 'data\\\\yolov8.pdf'}), Document(page_content='e) Model \\nEvaluationf) Deployment\\ng) Integration\\nh) MaintenanceFire \\nNo Fire \\nFig. 4 Smart ﬁre detection (SFD) system methodology20946 Neural Computing and Applications (2023) 35:20939–20954\\n123', metadata={'id': 'data\\\\yolov8.pdf:7:4', 'page': 7, 'source': 'data\\\\yolov8.pdf'}), Document(page_content='analytical and integrative application level capability; and\\n(v) intelligent physical and network infrastructure thatfacilitates complex and remote services and applications\\nFig. 1 YOLOv8 C2f module [ 30]Neural Computing and Applications (2023) 35:20939–20954 20943\\n123', metadata={'id': 'data\\\\yolov8.pdf:4:5', 'page': 4, 'source': 'data\\\\yolov8.pdf'}), Document(page_content='34. Saponara S, Elhanashi A, Gagliardi A (2021) Real-time video\\nﬁre/smoke detection based on CNN in antiﬁre surveillance sys-\\ntems. J Real-Time Image Proc 18:889–900\\n35. Wang Z et al (2022) A smoke detection model based on improved\\nYOLOv5. Mathematics 10(7):1190\\n36. Abdusalomov A et al (2021) An improvement of the ﬁre detec-\\ntion and classiﬁcation method using YOLOv3 for surveillancesystems. Sensors 21(19):6519\\n37. Talaat FM, Gamel SA (2022) RL based hyper-parameters opti-\\nmization algorithm (ROA) for convolutional neural network.J Ambient Intell Human Comput. https://doi.org/10.1007/s12652-\\n022-03788-y\\n38. Talaat FM, Ali SH, Saleh AI, Ali HA (2020) Effective cache\\nreplacement strategy (ECRS) for real-time fog computing envi-ronment. Clust Comput. https://doi.org/10.1007/s10586-020-', metadata={'id': 'data\\\\yolov8.pdf:15:0', 'page': 15, 'source': 'data\\\\yolov8.pdf'}), Document(page_content='•YOLOv5 still requires large amounts of data for\\ntraining and may suffer from spurious regressions,and it may have limitations in detecting small objects or\\nobjects with complex shapes.\\nDespite the advances in deep learning-based approaches\\nfor ﬁre detection, there are still some challenges that need\\nto be addressed. For example, there is a need for more\\ndiverse and larger datasets for training and testing theseapproaches. Additionally, the use of low-quality cameras\\nor poor lighting conditions can affect the accuracy of ﬁre\\ndetection algorithms.\\nIn this paper, a YOLOv8-based approach for ﬁre\\ndetection in smart cities is proposed. To the best of our\\nknowledge, this is the ﬁrst study to investigate the use ofthe YOLOv8 algorithm for ﬁre detection in smart cities.', metadata={'id': 'data\\\\yolov8.pdf:4:0', 'page': 4, 'source': 'data\\\\yolov8.pdf'}), Document(page_content='detected objects highlighted. The SFD algorithm is a\\npowerful tool for detecting ﬁres in real-time and enablesquick and effective responses to potential ﬁre hazards.The smart ﬁre detection (SFD) uses Yolov8 object\\ndetection model. It involves several steps:\\n•The video input source is set up from either a live\\ncamera or pre-recorded video ﬁle.\\n•The video capture process is started, and each frame of\\nthe video is looped through.\\n•Image pre-processing techniques are applied to each\\nframe, and the pre-processed frame is passed to the\\nYolov8 model for object detection.\\n•The detected objects are checked for ﬁre-related\\nclasses, such as ‘‘ﬂames’’, ‘‘smoke’’, or ‘‘embers’’.\\n•If a ﬁre-related class is detected, an alarm is triggered,\\nand relevant authorities are notiﬁed.', metadata={'id': 'data\\\\yolov8.pdf:9:1', 'page': 9, 'source': 'data\\\\yolov8.pdf'})]\n"
     ]
    }
   ],
   "source": [
    "from langchain.load import dumps, loads\n",
    "\n",
    "def get_unique_union(documents: list[list]):\n",
    "    \"\"\" Unique union of retrieved docs \"\"\"\n",
    "    # Flatten list of lists, and convert each Document to string\n",
    "    # làm phằng, đưa list of list -> 1 list\n",
    "    flattened_docs = [dumps(doc) for sublist in documents for doc in sublist]\n",
    "    # Get unique documents\n",
    "    unique_docs = list(set(flattened_docs))\n",
    "    # Return list documents\n",
    "    return [loads(doc) for doc in unique_docs]\n",
    "\n",
    "# Retrieve\n",
    "question = \"What is the dataset?\"\n",
    "retrieval_chain = generate_queries | retriever.map() | get_unique_union\n",
    "docs = retrieval_chain.invoke({\"question\":question})\n",
    "print(len(docs))\n",
    "print(docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "92feebb8-b4ab-4c98-ba6e-bb295c5224e2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Based on the provided context, it appears that there is no explicit mention of a specific dataset. However, it can be inferred from the content that the paper discusses various approaches to fire detection using deep learning-based models, such as YOLOv8. \\n\\nThe text mentions the use of datasets in previous research studies, for example, the MobileNetV2 model was used with the Forest Fire Detection System (FFireNet) dataset [8], and an improved YOLOv5 algorithm was tested on photographs taken by unmanned aerial vehicles (UAVs) [9]. \\n\\nHowever, it does not specify a particular dataset used in this paper.'"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from operator import itemgetter\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_core.runnables import RunnablePassthrough\n",
    "\n",
    "# RAG\n",
    "template = \"\"\"Answer the following question based on this context:\n",
    "\n",
    "{context}\n",
    "\n",
    "Question: {question}\n",
    "\"\"\"\n",
    "\n",
    "prompt = ChatPromptTemplate.from_template(template)\n",
    "\n",
    "# llm = ChatOpenAI(temperature=0)\n",
    "llm = model\n",
    "\n",
    "final_rag_chain = (\n",
    "    {\"context\": retrieval_chain, \n",
    "     \"question\": itemgetter(\"question\")} \n",
    "    | prompt\n",
    "    | llm\n",
    "    | StrOutputParser()\n",
    ")\n",
    "\n",
    "final_rag_chain.invoke({\"question\":question})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "96ebf3f4-8b44-457c-957d-97cf98677978",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "...\n"
     ]
    }
   ],
   "source": [
    "print(\"...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8848fbd-6e54-4ca5-8b9a-7da5c6f8b36d",
   "metadata": {},
   "source": [
    "# 9. Query Translation -- RAG Fusion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ed4b1d1-92b3-42b9-9883-4ae2211c3ba5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.embeddings.ollama import OllamaEmbeddings\n",
    "from langchain_community.llms.ollama import Ollama\n",
    "\n",
    "embeddings = OllamaEmbeddings(model=\"llama3\")\n",
    "model = Ollama(model=\"llama3\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "08c281ef-708e-4ea6-a2c1-430edec3fd33",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prompt\n",
    "from langchain.prompts import ChatPromptTemplate\n",
    "\n",
    "# RAG-Fusion: Related\n",
    "template = \"\"\"You are a helpful assistant that generates multiple search queries based on a single input query. \\n\n",
    "Generate multiple search queries related to: {question} \\n\n",
    "Output (4 queries):\"\"\"\n",
    "prompt_rag_fusion = ChatPromptTemplate.from_template(template)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "41c0ec41-d62c-470e-9902-756ac3655692",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "generate_queries = (\n",
    "    prompt_rag_fusion \n",
    "    | model # ChatOpenAI(temperature=0)\n",
    "    | StrOutputParser() \n",
    "    | (lambda x: x.split(\"\\n\")) # list query\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "7750f7df-ec27-40b1-9857-198262e48ce9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8\n",
      "[[Document(page_content='costs associated with false alarms.\\n•Cost-effective: The proposed approach may be cost-\\neffective compared to traditional ﬁre detection methods\\nas it can be implemented using low-cost cameras and\\nhardware, reducing the need for expensive ﬁre detectionsystems.\\n•Large dataset: Unlike other methods that use small\\nnumber of datasets, a large dataset containing ﬁre,smoke, and normal scenes is used. The dataset has real-\\nworld images and videos collected from various\\nsources. The dataset has a diverse range of ﬁrescenarios, including indoor and outdoor ﬁres, small\\nand large ﬁres, and low-light and high-light conditions.\\nA deep CNN gathers essential data from big datasets toproduce precise predictions and reduce overﬁtting.\\nThe following is how the remaining work is structured.', metadata={'id': 'data\\\\yolov8.pdf:1:5', 'page': 1, 'source': 'data\\\\yolov8.pdf'}), Document(page_content='detected objects highlighted. The SFD algorithm is a\\npowerful tool for detecting ﬁres in real-time and enablesquick and effective responses to potential ﬁre hazards.The smart ﬁre detection (SFD) uses Yolov8 object\\ndetection model. It involves several steps:\\n•The video input source is set up from either a live\\ncamera or pre-recorded video ﬁle.\\n•The video capture process is started, and each frame of\\nthe video is looped through.\\n•Image pre-processing techniques are applied to each\\nframe, and the pre-processed frame is passed to the\\nYolov8 model for object detection.\\n•The detected objects are checked for ﬁre-related\\nclasses, such as ‘‘ﬂames’’, ‘‘smoke’’, or ‘‘embers’’.\\n•If a ﬁre-related class is detected, an alarm is triggered,\\nand relevant authorities are notiﬁed.', metadata={'id': 'data\\\\yolov8.pdf:9:1', 'page': 9, 'source': 'data\\\\yolov8.pdf'}), Document(page_content='tative of the overall dataset. Other pre-processing steps,\\nsuch as resizing or normalizing the data, may also benecessary. The goal is to have a large enough, balanced\\ndataset that can generalize well to new data.\\nStep 3: Model selection This step involves selecting the\\nappropriate object detection algorithm for training the ﬁre\\ndetection model. There are several algorithms to choose\\nfrom, such as YOLOv8, Faster R-CNN, and SSD, eachwith its own advantages and disadvantages. The selected\\nalgorithm should have good performance on the collected\\ndataset and be capable of handling different ﬁre scenarios,depending on the requirements of the smart ﬁre detection\\nsystem. YOLOv8 is a popular choice due to its speed andaccuracy, but other algorithms can also be used based on\\nspeciﬁc needs.', metadata={'id': 'data\\\\yolov8.pdf:7:1', 'page': 7, 'source': 'data\\\\yolov8.pdf'}), Document(page_content='42. El-Rashidy N, Ebrahim N, El Ghamry A, Talaat FM (2022)\\nUtilizing fog computing and explainable deep learning tech-niques for gestational diabetes prediction. Neural Comput Appl.https://doi.org/10.1007/s00521-022-08007-5\\n43. El-Balka RM et al (2022) Enhancing the performance of smart\\nelectrical grids using data mining and fuzzy inference engine.Multimed Tools Appl 81(23):33017–33049\\n44. Talaat FM (2022) Effective deep Q-networks (EDQN) strategy\\nfor resource allocation based on optimized reinforcement learn-\\ning algorithm. Multimed Tools Appl 81:39945–39961\\n45. Alshathri S, Talaat FM, Nasr AA (2022) A new reliable system\\nfor managing virtual cloud network. Comput Mater Continua\\n73(3):5863–5885. https://doi.org/10.32604/cmc.2022.026547', metadata={'id': 'data\\\\yolov8.pdf:15:2', 'page': 15, 'source': 'data\\\\yolov8.pdf'})], [Document(page_content='(SFDS). The SFDS can detect ﬁre hazards in different\\nlocations, including government buildings, public areas,hospitals, residential areas, and highways, and ensure the\\nsafety of people and property. The Application layer plays\\na signiﬁcant role in enabling the SFDS to serve various\\nFig. 2 YOLO v8 architecture [ 32]20944 Neural Computing and Applications (2023) 35:20939–20954\\n123', metadata={'id': 'data\\\\yolov8.pdf:5:1', 'page': 5, 'source': 'data\\\\yolov8.pdf'}), Document(page_content='34. Saponara S, Elhanashi A, Gagliardi A (2021) Real-time video\\nﬁre/smoke detection based on CNN in antiﬁre surveillance sys-\\ntems. J Real-Time Image Proc 18:889–900\\n35. Wang Z et al (2022) A smoke detection model based on improved\\nYOLOv5. Mathematics 10(7):1190\\n36. Abdusalomov A et al (2021) An improvement of the ﬁre detec-\\ntion and classiﬁcation method using YOLOv3 for surveillancesystems. Sensors 21(19):6519\\n37. Talaat FM, Gamel SA (2022) RL based hyper-parameters opti-\\nmization algorithm (ROA) for convolutional neural network.J Ambient Intell Human Comput. https://doi.org/10.1007/s12652-\\n022-03788-y\\n38. Talaat FM, Ali SH, Saleh AI, Ali HA (2020) Effective cache\\nreplacement strategy (ECRS) for real-time fog computing envi-ronment. Clust Comput. https://doi.org/10.1007/s10586-020-', metadata={'id': 'data\\\\yolov8.pdf:15:0', 'page': 15, 'source': 'data\\\\yolov8.pdf'}), Document(page_content='F1 score, demonstrating its effectiveness in detecting ﬁres\\nFig. 7 Precision–recall curve of the proposed model20950 Neural Computing and Applications (2023) 35:20939–20954\\n123', metadata={'id': 'data\\\\yolov8.pdf:11:2', 'page': 11, 'source': 'data\\\\yolov8.pdf'}), Document(page_content='analytical and integrative application level capability; and\\n(v) intelligent physical and network infrastructure thatfacilitates complex and remote services and applications\\nFig. 1 YOLOv8 C2f module [ 30]Neural Computing and Applications (2023) 35:20939–20954 20943\\n123', metadata={'id': 'data\\\\yolov8.pdf:4:5', 'page': 4, 'source': 'data\\\\yolov8.pdf'})], [Document(page_content='tative of the overall dataset. Other pre-processing steps,\\nsuch as resizing or normalizing the data, may also benecessary. The goal is to have a large enough, balanced\\ndataset that can generalize well to new data.\\nStep 3: Model selection This step involves selecting the\\nappropriate object detection algorithm for training the ﬁre\\ndetection model. There are several algorithms to choose\\nfrom, such as YOLOv8, Faster R-CNN, and SSD, eachwith its own advantages and disadvantages. The selected\\nalgorithm should have good performance on the collected\\ndataset and be capable of handling different ﬁre scenarios,depending on the requirements of the smart ﬁre detection\\nsystem. YOLOv8 is a popular choice due to its speed andaccuracy, but other algorithms can also be used based on\\nspeciﬁc needs.', metadata={'id': 'data\\\\yolov8.pdf:7:1', 'page': 7, 'source': 'data\\\\yolov8.pdf'}), Document(page_content='detected objects highlighted. The SFD algorithm is a\\npowerful tool for detecting ﬁres in real-time and enablesquick and effective responses to potential ﬁre hazards.The smart ﬁre detection (SFD) uses Yolov8 object\\ndetection model. It involves several steps:\\n•The video input source is set up from either a live\\ncamera or pre-recorded video ﬁle.\\n•The video capture process is started, and each frame of\\nthe video is looped through.\\n•Image pre-processing techniques are applied to each\\nframe, and the pre-processed frame is passed to the\\nYolov8 model for object detection.\\n•The detected objects are checked for ﬁre-related\\nclasses, such as ‘‘ﬂames’’, ‘‘smoke’’, or ‘‘embers’’.\\n•If a ﬁre-related class is detected, an alarm is triggered,\\nand relevant authorities are notiﬁed.', metadata={'id': 'data\\\\yolov8.pdf:9:1', 'page': 9, 'source': 'data\\\\yolov8.pdf'}), Document(page_content='costs associated with false alarms.\\n•Cost-effective: The proposed approach may be cost-\\neffective compared to traditional ﬁre detection methods\\nas it can be implemented using low-cost cameras and\\nhardware, reducing the need for expensive ﬁre detectionsystems.\\n•Large dataset: Unlike other methods that use small\\nnumber of datasets, a large dataset containing ﬁre,smoke, and normal scenes is used. The dataset has real-\\nworld images and videos collected from various\\nsources. The dataset has a diverse range of ﬁrescenarios, including indoor and outdoor ﬁres, small\\nand large ﬁres, and low-light and high-light conditions.\\nA deep CNN gathers essential data from big datasets toproduce precise predictions and reduce overﬁtting.\\nThe following is how the remaining work is structured.', metadata={'id': 'data\\\\yolov8.pdf:1:5', 'page': 1, 'source': 'data\\\\yolov8.pdf'}), Document(page_content='•YOLOv5 still requires large amounts of data for\\ntraining and may suffer from spurious regressions,and it may have limitations in detecting small objects or\\nobjects with complex shapes.\\nDespite the advances in deep learning-based approaches\\nfor ﬁre detection, there are still some challenges that need\\nto be addressed. For example, there is a need for more\\ndiverse and larger datasets for training and testing theseapproaches. Additionally, the use of low-quality cameras\\nor poor lighting conditions can affect the accuracy of ﬁre\\ndetection algorithms.\\nIn this paper, a YOLOv8-based approach for ﬁre\\ndetection in smart cities is proposed. To the best of our\\nknowledge, this is the ﬁrst study to investigate the use ofthe YOLOv8 algorithm for ﬁre detection in smart cities.', metadata={'id': 'data\\\\yolov8.pdf:4:0', 'page': 4, 'source': 'data\\\\yolov8.pdf'})], [Document(page_content='tative of the overall dataset. Other pre-processing steps,\\nsuch as resizing or normalizing the data, may also benecessary. The goal is to have a large enough, balanced\\ndataset that can generalize well to new data.\\nStep 3: Model selection This step involves selecting the\\nappropriate object detection algorithm for training the ﬁre\\ndetection model. There are several algorithms to choose\\nfrom, such as YOLOv8, Faster R-CNN, and SSD, eachwith its own advantages and disadvantages. The selected\\nalgorithm should have good performance on the collected\\ndataset and be capable of handling different ﬁre scenarios,depending on the requirements of the smart ﬁre detection\\nsystem. YOLOv8 is a popular choice due to its speed andaccuracy, but other algorithms can also be used based on\\nspeciﬁc needs.', metadata={'id': 'data\\\\yolov8.pdf:7:1', 'page': 7, 'source': 'data\\\\yolov8.pdf'}), Document(page_content='detected objects highlighted. The SFD algorithm is a\\npowerful tool for detecting ﬁres in real-time and enablesquick and effective responses to potential ﬁre hazards.The smart ﬁre detection (SFD) uses Yolov8 object\\ndetection model. It involves several steps:\\n•The video input source is set up from either a live\\ncamera or pre-recorded video ﬁle.\\n•The video capture process is started, and each frame of\\nthe video is looped through.\\n•Image pre-processing techniques are applied to each\\nframe, and the pre-processed frame is passed to the\\nYolov8 model for object detection.\\n•The detected objects are checked for ﬁre-related\\nclasses, such as ‘‘ﬂames’’, ‘‘smoke’’, or ‘‘embers’’.\\n•If a ﬁre-related class is detected, an alarm is triggered,\\nand relevant authorities are notiﬁed.', metadata={'id': 'data\\\\yolov8.pdf:9:1', 'page': 9, 'source': 'data\\\\yolov8.pdf'}), Document(page_content='costs associated with false alarms.\\n•Cost-effective: The proposed approach may be cost-\\neffective compared to traditional ﬁre detection methods\\nas it can be implemented using low-cost cameras and\\nhardware, reducing the need for expensive ﬁre detectionsystems.\\n•Large dataset: Unlike other methods that use small\\nnumber of datasets, a large dataset containing ﬁre,smoke, and normal scenes is used. The dataset has real-\\nworld images and videos collected from various\\nsources. The dataset has a diverse range of ﬁrescenarios, including indoor and outdoor ﬁres, small\\nand large ﬁres, and low-light and high-light conditions.\\nA deep CNN gathers essential data from big datasets toproduce precise predictions and reduce overﬁtting.\\nThe following is how the remaining work is structured.', metadata={'id': 'data\\\\yolov8.pdf:1:5', 'page': 1, 'source': 'data\\\\yolov8.pdf'}), Document(page_content='•YOLOv5 still requires large amounts of data for\\ntraining and may suffer from spurious regressions,and it may have limitations in detecting small objects or\\nobjects with complex shapes.\\nDespite the advances in deep learning-based approaches\\nfor ﬁre detection, there are still some challenges that need\\nto be addressed. For example, there is a need for more\\ndiverse and larger datasets for training and testing theseapproaches. Additionally, the use of low-quality cameras\\nor poor lighting conditions can affect the accuracy of ﬁre\\ndetection algorithms.\\nIn this paper, a YOLOv8-based approach for ﬁre\\ndetection in smart cities is proposed. To the best of our\\nknowledge, this is the ﬁrst study to investigate the use ofthe YOLOv8 algorithm for ﬁre detection in smart cities.', metadata={'id': 'data\\\\yolov8.pdf:4:0', 'page': 4, 'source': 'data\\\\yolov8.pdf'})], [Document(page_content='tative of the overall dataset. Other pre-processing steps,\\nsuch as resizing or normalizing the data, may also benecessary. The goal is to have a large enough, balanced\\ndataset that can generalize well to new data.\\nStep 3: Model selection This step involves selecting the\\nappropriate object detection algorithm for training the ﬁre\\ndetection model. There are several algorithms to choose\\nfrom, such as YOLOv8, Faster R-CNN, and SSD, eachwith its own advantages and disadvantages. The selected\\nalgorithm should have good performance on the collected\\ndataset and be capable of handling different ﬁre scenarios,depending on the requirements of the smart ﬁre detection\\nsystem. YOLOv8 is a popular choice due to its speed andaccuracy, but other algorithms can also be used based on\\nspeciﬁc needs.', metadata={'id': 'data\\\\yolov8.pdf:7:1', 'page': 7, 'source': 'data\\\\yolov8.pdf'}), Document(page_content='detected objects highlighted. The SFD algorithm is a\\npowerful tool for detecting ﬁres in real-time and enablesquick and effective responses to potential ﬁre hazards.The smart ﬁre detection (SFD) uses Yolov8 object\\ndetection model. It involves several steps:\\n•The video input source is set up from either a live\\ncamera or pre-recorded video ﬁle.\\n•The video capture process is started, and each frame of\\nthe video is looped through.\\n•Image pre-processing techniques are applied to each\\nframe, and the pre-processed frame is passed to the\\nYolov8 model for object detection.\\n•The detected objects are checked for ﬁre-related\\nclasses, such as ‘‘ﬂames’’, ‘‘smoke’’, or ‘‘embers’’.\\n•If a ﬁre-related class is detected, an alarm is triggered,\\nand relevant authorities are notiﬁed.', metadata={'id': 'data\\\\yolov8.pdf:9:1', 'page': 9, 'source': 'data\\\\yolov8.pdf'}), Document(page_content='costs associated with false alarms.\\n•Cost-effective: The proposed approach may be cost-\\neffective compared to traditional ﬁre detection methods\\nas it can be implemented using low-cost cameras and\\nhardware, reducing the need for expensive ﬁre detectionsystems.\\n•Large dataset: Unlike other methods that use small\\nnumber of datasets, a large dataset containing ﬁre,smoke, and normal scenes is used. The dataset has real-\\nworld images and videos collected from various\\nsources. The dataset has a diverse range of ﬁrescenarios, including indoor and outdoor ﬁres, small\\nand large ﬁres, and low-light and high-light conditions.\\nA deep CNN gathers essential data from big datasets toproduce precise predictions and reduce overﬁtting.\\nThe following is how the remaining work is structured.', metadata={'id': 'data\\\\yolov8.pdf:1:5', 'page': 1, 'source': 'data\\\\yolov8.pdf'}), Document(page_content='•YOLOv5 still requires large amounts of data for\\ntraining and may suffer from spurious regressions,and it may have limitations in detecting small objects or\\nobjects with complex shapes.\\nDespite the advances in deep learning-based approaches\\nfor ﬁre detection, there are still some challenges that need\\nto be addressed. For example, there is a need for more\\ndiverse and larger datasets for training and testing theseapproaches. Additionally, the use of low-quality cameras\\nor poor lighting conditions can affect the accuracy of ﬁre\\ndetection algorithms.\\nIn this paper, a YOLOv8-based approach for ﬁre\\ndetection in smart cities is proposed. To the best of our\\nknowledge, this is the ﬁrst study to investigate the use ofthe YOLOv8 algorithm for ﬁre detection in smart cities.', metadata={'id': 'data\\\\yolov8.pdf:4:0', 'page': 4, 'source': 'data\\\\yolov8.pdf'})], [Document(page_content='tative of the overall dataset. Other pre-processing steps,\\nsuch as resizing or normalizing the data, may also benecessary. The goal is to have a large enough, balanced\\ndataset that can generalize well to new data.\\nStep 3: Model selection This step involves selecting the\\nappropriate object detection algorithm for training the ﬁre\\ndetection model. There are several algorithms to choose\\nfrom, such as YOLOv8, Faster R-CNN, and SSD, eachwith its own advantages and disadvantages. The selected\\nalgorithm should have good performance on the collected\\ndataset and be capable of handling different ﬁre scenarios,depending on the requirements of the smart ﬁre detection\\nsystem. YOLOv8 is a popular choice due to its speed andaccuracy, but other algorithms can also be used based on\\nspeciﬁc needs.', metadata={'id': 'data\\\\yolov8.pdf:7:1', 'page': 7, 'source': 'data\\\\yolov8.pdf'}), Document(page_content='detected objects highlighted. The SFD algorithm is a\\npowerful tool for detecting ﬁres in real-time and enablesquick and effective responses to potential ﬁre hazards.The smart ﬁre detection (SFD) uses Yolov8 object\\ndetection model. It involves several steps:\\n•The video input source is set up from either a live\\ncamera or pre-recorded video ﬁle.\\n•The video capture process is started, and each frame of\\nthe video is looped through.\\n•Image pre-processing techniques are applied to each\\nframe, and the pre-processed frame is passed to the\\nYolov8 model for object detection.\\n•The detected objects are checked for ﬁre-related\\nclasses, such as ‘‘ﬂames’’, ‘‘smoke’’, or ‘‘embers’’.\\n•If a ﬁre-related class is detected, an alarm is triggered,\\nand relevant authorities are notiﬁed.', metadata={'id': 'data\\\\yolov8.pdf:9:1', 'page': 9, 'source': 'data\\\\yolov8.pdf'}), Document(page_content='costs associated with false alarms.\\n•Cost-effective: The proposed approach may be cost-\\neffective compared to traditional ﬁre detection methods\\nas it can be implemented using low-cost cameras and\\nhardware, reducing the need for expensive ﬁre detectionsystems.\\n•Large dataset: Unlike other methods that use small\\nnumber of datasets, a large dataset containing ﬁre,smoke, and normal scenes is used. The dataset has real-\\nworld images and videos collected from various\\nsources. The dataset has a diverse range of ﬁrescenarios, including indoor and outdoor ﬁres, small\\nand large ﬁres, and low-light and high-light conditions.\\nA deep CNN gathers essential data from big datasets toproduce precise predictions and reduce overﬁtting.\\nThe following is how the remaining work is structured.', metadata={'id': 'data\\\\yolov8.pdf:1:5', 'page': 1, 'source': 'data\\\\yolov8.pdf'}), Document(page_content='•YOLOv5 still requires large amounts of data for\\ntraining and may suffer from spurious regressions,and it may have limitations in detecting small objects or\\nobjects with complex shapes.\\nDespite the advances in deep learning-based approaches\\nfor ﬁre detection, there are still some challenges that need\\nto be addressed. For example, there is a need for more\\ndiverse and larger datasets for training and testing theseapproaches. Additionally, the use of low-quality cameras\\nor poor lighting conditions can affect the accuracy of ﬁre\\ndetection algorithms.\\nIn this paper, a YOLOv8-based approach for ﬁre\\ndetection in smart cities is proposed. To the best of our\\nknowledge, this is the ﬁrst study to investigate the use ofthe YOLOv8 algorithm for ﬁre detection in smart cities.', metadata={'id': 'data\\\\yolov8.pdf:4:0', 'page': 4, 'source': 'data\\\\yolov8.pdf'})], [Document(page_content='(SFDS). The SFDS can detect ﬁre hazards in different\\nlocations, including government buildings, public areas,hospitals, residential areas, and highways, and ensure the\\nsafety of people and property. The Application layer plays\\na signiﬁcant role in enabling the SFDS to serve various\\nFig. 2 YOLO v8 architecture [ 32]20944 Neural Computing and Applications (2023) 35:20939–20954\\n123', metadata={'id': 'data\\\\yolov8.pdf:5:1', 'page': 5, 'source': 'data\\\\yolov8.pdf'}), Document(page_content='34. Saponara S, Elhanashi A, Gagliardi A (2021) Real-time video\\nﬁre/smoke detection based on CNN in antiﬁre surveillance sys-\\ntems. J Real-Time Image Proc 18:889–900\\n35. Wang Z et al (2022) A smoke detection model based on improved\\nYOLOv5. Mathematics 10(7):1190\\n36. Abdusalomov A et al (2021) An improvement of the ﬁre detec-\\ntion and classiﬁcation method using YOLOv3 for surveillancesystems. Sensors 21(19):6519\\n37. Talaat FM, Gamel SA (2022) RL based hyper-parameters opti-\\nmization algorithm (ROA) for convolutional neural network.J Ambient Intell Human Comput. https://doi.org/10.1007/s12652-\\n022-03788-y\\n38. Talaat FM, Ali SH, Saleh AI, Ali HA (2020) Effective cache\\nreplacement strategy (ECRS) for real-time fog computing envi-ronment. Clust Comput. https://doi.org/10.1007/s10586-020-', metadata={'id': 'data\\\\yolov8.pdf:15:0', 'page': 15, 'source': 'data\\\\yolov8.pdf'}), Document(page_content='F1 score, demonstrating its effectiveness in detecting ﬁres\\nFig. 7 Precision–recall curve of the proposed model20950 Neural Computing and Applications (2023) 35:20939–20954\\n123', metadata={'id': 'data\\\\yolov8.pdf:11:2', 'page': 11, 'source': 'data\\\\yolov8.pdf'}), Document(page_content='analytical and integrative application level capability; and\\n(v) intelligent physical and network infrastructure thatfacilitates complex and remote services and applications\\nFig. 1 YOLOv8 C2f module [ 30]Neural Computing and Applications (2023) 35:20939–20954 20943\\n123', metadata={'id': 'data\\\\yolov8.pdf:4:5', 'page': 4, 'source': 'data\\\\yolov8.pdf'})], [Document(page_content='tative of the overall dataset. Other pre-processing steps,\\nsuch as resizing or normalizing the data, may also benecessary. The goal is to have a large enough, balanced\\ndataset that can generalize well to new data.\\nStep 3: Model selection This step involves selecting the\\nappropriate object detection algorithm for training the ﬁre\\ndetection model. There are several algorithms to choose\\nfrom, such as YOLOv8, Faster R-CNN, and SSD, eachwith its own advantages and disadvantages. The selected\\nalgorithm should have good performance on the collected\\ndataset and be capable of handling different ﬁre scenarios,depending on the requirements of the smart ﬁre detection\\nsystem. YOLOv8 is a popular choice due to its speed andaccuracy, but other algorithms can also be used based on\\nspeciﬁc needs.', metadata={'id': 'data\\\\yolov8.pdf:7:1', 'page': 7, 'source': 'data\\\\yolov8.pdf'}), Document(page_content='detected objects highlighted. The SFD algorithm is a\\npowerful tool for detecting ﬁres in real-time and enablesquick and effective responses to potential ﬁre hazards.The smart ﬁre detection (SFD) uses Yolov8 object\\ndetection model. It involves several steps:\\n•The video input source is set up from either a live\\ncamera or pre-recorded video ﬁle.\\n•The video capture process is started, and each frame of\\nthe video is looped through.\\n•Image pre-processing techniques are applied to each\\nframe, and the pre-processed frame is passed to the\\nYolov8 model for object detection.\\n•The detected objects are checked for ﬁre-related\\nclasses, such as ‘‘ﬂames’’, ‘‘smoke’’, or ‘‘embers’’.\\n•If a ﬁre-related class is detected, an alarm is triggered,\\nand relevant authorities are notiﬁed.', metadata={'id': 'data\\\\yolov8.pdf:9:1', 'page': 9, 'source': 'data\\\\yolov8.pdf'}), Document(page_content='42. El-Rashidy N, Ebrahim N, El Ghamry A, Talaat FM (2022)\\nUtilizing fog computing and explainable deep learning tech-niques for gestational diabetes prediction. Neural Comput Appl.https://doi.org/10.1007/s00521-022-08007-5\\n43. El-Balka RM et al (2022) Enhancing the performance of smart\\nelectrical grids using data mining and fuzzy inference engine.Multimed Tools Appl 81(23):33017–33049\\n44. Talaat FM (2022) Effective deep Q-networks (EDQN) strategy\\nfor resource allocation based on optimized reinforcement learn-\\ning algorithm. Multimed Tools Appl 81:39945–39961\\n45. Alshathri S, Talaat FM, Nasr AA (2022) A new reliable system\\nfor managing virtual cloud network. Comput Mater Continua\\n73(3):5863–5885. https://doi.org/10.32604/cmc.2022.026547', metadata={'id': 'data\\\\yolov8.pdf:15:2', 'page': 15, 'source': 'data\\\\yolov8.pdf'}), Document(page_content='costs associated with false alarms.\\n•Cost-effective: The proposed approach may be cost-\\neffective compared to traditional ﬁre detection methods\\nas it can be implemented using low-cost cameras and\\nhardware, reducing the need for expensive ﬁre detectionsystems.\\n•Large dataset: Unlike other methods that use small\\nnumber of datasets, a large dataset containing ﬁre,smoke, and normal scenes is used. The dataset has real-\\nworld images and videos collected from various\\nsources. The dataset has a diverse range of ﬁrescenarios, including indoor and outdoor ﬁres, small\\nand large ﬁres, and low-light and high-light conditions.\\nA deep CNN gathers essential data from big datasets toproduce precise predictions and reduce overﬁtting.\\nThe following is how the remaining work is structured.', metadata={'id': 'data\\\\yolov8.pdf:1:5', 'page': 1, 'source': 'data\\\\yolov8.pdf'})]]\n",
      "9\n"
     ]
    }
   ],
   "source": [
    "from langchain.load import dumps, loads\n",
    "    \n",
    "def reciprocal_rank_fusion(results: list[list], k=60):\n",
    "    \"\"\" Reciprocal_rank_fusion that takes multiple lists of ranked documents \n",
    "        and an optional parameter k used in the RRF formula \n",
    "        RRF- Reciprocal rank fusion\n",
    "        \n",
    "        Tài liệu nào được truy xuất thành kết quả nhiều lần -> rank cao hơn\n",
    "    \"\"\"\n",
    "    \n",
    "    # Initialize a dictionary to hold fused scores for each unique document\n",
    "    fused_scores = {}\n",
    "\n",
    "    # Iterate through each list of ranked documents\n",
    "    for docs in results:\n",
    "        # Iterate through each document in the list, with its rank (position in the list)\n",
    "        for rank, doc in enumerate(docs):\n",
    "            # Convert the document to a string format to use as a key (assumes documents can be serialized to JSON)\n",
    "            doc_str = dumps(doc)\n",
    "            # If the document is not yet in the fused_scores dictionary, add it with an initial score of 0\n",
    "            if doc_str not in fused_scores:\n",
    "                fused_scores[doc_str] = 0\n",
    "            # Retrieve the current score of the document, if any\n",
    "            previous_score = fused_scores[doc_str]\n",
    "            # Update the score of the document using the RRF formula: 1 / (rank + k)\n",
    "            fused_scores[doc_str] += 1 / (rank + k)\n",
    "\n",
    "    # Sort the documents based on their fused scores in descending order to get the final reranked results\n",
    "    reranked_results = [\n",
    "        (loads(doc), score)\n",
    "        for doc, score in sorted(fused_scores.items(), key=lambda x: x[1], reverse=True)\n",
    "    ]\n",
    "\n",
    "    # Return the reranked results as a list of tuples, each containing the document and its fused score\n",
    "    return reranked_results\n",
    "\n",
    "retrieval_chain_rag_fusion = generate_queries | retriever.map() | reciprocal_rank_fusion\n",
    "docs = retrieval_chain_rag_fusion.invoke({\"question\": question})\n",
    "print(len(docs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "44cc2ccf-1966-4210-9495-e8592cfd919b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Based on the provided context, it appears that there is no specific mention of a dataset in this text. The text seems to be discussing various algorithms and approaches for fire detection, including YOLOv8, Faster R-CNN, and SSD. It mentions the need for diverse and larger datasets for training and testing these approaches, but does not specify a particular dataset being used or referred to.'"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# RAG\n",
    "template = \"\"\"Answer the following question based on this context:\n",
    "\n",
    "{context}\n",
    "\n",
    "Question: {question}\n",
    "\"\"\"\n",
    "\n",
    "prompt = ChatPromptTemplate.from_template(template)\n",
    "\n",
    "print()\n",
    "final_rag_chain = (\n",
    "    {\"context\": retrieval_chain_rag_fusion, \n",
    "     \"question\": itemgetter(\"question\")} # ~ind\n",
    "    | prompt\n",
    "    | llm\n",
    "    | StrOutputParser()\n",
    ")\n",
    "\n",
    "final_rag_chain.invoke({\"question\":question})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "847a2d87-8d15-408b-8537-d49f20296ece",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# 10. Query Translation -- Decomposition tách câu hỏi thành từng phần để hỏi\n",
    "Answer recursively - trả lời đệ quy, kết hợp phần trả lời của câu trước để trả lời cho câu phụ tiếp theo\n",
    "Answer individually - trả lời riêng"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "e790e41f-9854-423c-ac4a-f4e597b07dca",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.embeddings.ollama import OllamaEmbeddings\n",
    "from langchain_community.llms.ollama import Ollama\n",
    "\n",
    "embeddings = OllamaEmbeddings(model=\"llama3\")\n",
    "model = Ollama(model=\"llama3\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "56cc0058-6d49-4643-b6d9-fa4fcd4fd6d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.prompts import ChatPromptTemplate\n",
    "# Decomposition\n",
    "template = \"\"\"You are a helpful assistant that generates multiple sub-questions related to an input question. \\n\n",
    "The goal is to break down the input into a set of sub-problems / sub-questions that can be answers in isolation. \\n\n",
    "Generate multiple search queries related to: {question} \\n\n",
    "Output (3 queries):\"\"\"\n",
    "prompt_decomposition = ChatPromptTemplate.from_template(template)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "70d31d1e-c6f2-4df6-983d-b06d97919b15",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "\n",
    "# LLM\n",
    "llm = model\n",
    "\n",
    "# Chain\n",
    "generate_queries_decomposition = (\n",
    "    prompt_decomposition \n",
    "    | llm \n",
    "    | StrOutputParser() \n",
    "    | (lambda x: x.split(\"\\n\")))\n",
    "\n",
    "# Run\n",
    "# question = \"What are the main components of an LLM-powered autonomous agent system?\"\n",
    "question = \"What data is used in the paper?\"\n",
    "questions = generate_queries_decomposition.invoke({\"question\":question})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "84cbbef4-474a-488b-b8a1-ce64dbe8c569",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Here are three sub-questions related to \"What data is used in the paper?\" :',\n",
       " '',\n",
       " '1. **What type of data**: Is the data used in the paper numerical, categorical, text-based, or a combination?',\n",
       " '2. **Source of the data**: Is the data sourced from primary research, secondary sources (e.g., literature reviews), or external datasets (e.g., government reports)?',\n",
       " '3. **Specific variables or metrics**: What specific variables, metrics, or indicators are used in the paper to support its arguments or findings?']"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "questions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef48579b-85c0-4c30-816d-56d9940e62de",
   "metadata": {},
   "source": [
    "#"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "883c86e6-2bdb-423a-8b83-2ce75cf4c3af",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Answer recursively - trả lời đệ quy, kết hợp phần trả lời của câu trước để trả lời cho câu phụ tiếp theo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "4dc15535-fd6b-496c-8a11-406839e79076",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prompt\n",
    "template = \"\"\"Here is the question you need to answer:\n",
    "\n",
    "\\n --- \\n {question} \\n --- \\n\n",
    "\n",
    "Here is any available background question + answer pairs:\n",
    "\n",
    "\\n --- \\n {q_a_pairs} \\n --- \\n\n",
    "\n",
    "Here is additional context relevant to the question: \n",
    "\n",
    "\\n --- \\n {context} \\n --- \\n\n",
    "\n",
    "Use the above context and any background question + answer pairs to answer the question: \\n {question}\n",
    "\"\"\"\n",
    "\n",
    "decomposition_prompt = ChatPromptTemplate.from_template(template)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "6444e0c4-c181-4ad4-9d17-d32c1a8c29e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from operator import itemgetter\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "\n",
    "def format_qa_pair(question, answer):\n",
    "    \"\"\"Format Q and A pair\"\"\"\n",
    "    \n",
    "    formatted_string = \"\"\n",
    "    formatted_string += f\"Question: {question}\\nAnswer: {answer}\\n\\n\"\n",
    "    return formatted_string.strip()\n",
    "\n",
    "# llm\n",
    "# llm = ChatOpenAI(model_name=\"gpt-3.5-turbo\", temperature=0)\n",
    "llm = model\n",
    "q_a_pairs = \"\"\n",
    "for q in questions:\n",
    "    \n",
    "    rag_chain = (\n",
    "    {\"context\": itemgetter(\"question\") | retriever, \n",
    "     \"question\": itemgetter(\"question\"),\n",
    "     \"q_a_pairs\": itemgetter(\"q_a_pairs\")} \n",
    "    | decomposition_prompt\n",
    "    | llm\n",
    "    | StrOutputParser())\n",
    "\n",
    "    answer = rag_chain.invoke({\"question\":q,\"q_a_pairs\":q_a_pairs})\n",
    "    q_a_pair = format_qa_pair(q,answer)\n",
    "    q_a_pairs = q_a_pairs + \"\\n---\\n\"+  q_a_pair"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73816d4d-bbdc-40fb-babd-cbbb3bb48920",
   "metadata": {},
   "source": [
    "#"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3fe487e4-7131-4230-a54f-7733cfd4dc54",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Answer individually - trả lời riêng"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "93f546a0-8731-4a91-9a54-cbc853128419",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "F:\\CMC\\CMC_Study\\Code\\venv\\lib\\site-packages\\langchain_core\\_api\\deprecation.py:119: LangChainDeprecationWarning: The method `BaseRetriever.get_relevant_documents` was deprecated in langchain-core 0.1.46 and will be removed in 0.3.0. Use invoke instead.\n",
      "  warn_deprecated(\n"
     ]
    }
   ],
   "source": [
    "# Answer each sub-question individually \n",
    "\n",
    "from langchain import hub\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.runnables import RunnablePassthrough, RunnableLambda\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "# RAG prompt\n",
    "prompt_rag = hub.pull(\"rlm/rag-prompt\")\n",
    "\n",
    "def retrieve_and_rag(question,prompt_rag,sub_question_generator_chain):\n",
    "    \"\"\"RAG mỗi một câu hỏi phụ\"\"\"\n",
    "    \n",
    "    # Use our decomposition / Tách sinh câu hỏi\n",
    "    sub_questions = sub_question_generator_chain.invoke({\"question\":question})\n",
    "    \n",
    "    # Initialize a list to hold RAG chain results\n",
    "    rag_results = []\n",
    "    \n",
    "    for sub_question in sub_questions:\n",
    "        \n",
    "        # Retrieve documents for each sub-question\n",
    "        retrieved_docs = retriever.get_relevant_documents(sub_question)\n",
    "        \n",
    "        # Use retrieved documents and sub-question in RAG chain\n",
    "        answer = (prompt_rag\n",
    "                  | llm \n",
    "                  | StrOutputParser()\n",
    "        ).invoke({\"context\": retrieved_docs,\"question\": sub_question})\n",
    "        rag_results.append(answer)\n",
    "    \n",
    "    return rag_results, sub_questions\n",
    "\n",
    "# Wrap the retrieval and RAG process in a RunnableLambda for integration into a chain\n",
    "answers, questions = retrieve_and_rag(\n",
    "    question, \n",
    "    prompt_rag, \n",
    "    generate_queries_decomposition  # chain tách sinh câu hỏi \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "fd2c5fd0-2caf-4f61-a07f-49c8721198c0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Based on the provided context and the Q&A pairs, the data used in the paper includes:\\n\\n1. A large dataset containing real-world images and videos of fire, smoke, and normal scenes from various sources.\\n2. This dataset has a diverse range of fire scenarios, including indoor and outdoor fires, small and large fires, and low-light and high-light conditions.\\n3. The dataset is used to train a deep Convolutional Neural Network (CNN) model, specifically the YOLOv8 object detection algorithm, to detect fires in real-time video input.\\n\\nThe data analysis techniques applied involve image pre-processing and object detection using the YOLOv8 model. No specific datasets are mentioned in the paper, but it is clear that the dataset used is a large collection of images and videos. The study does not rely on secondary data from existing studies or datasets; instead, it collects its own dataset to train the deep CNN model.\\n\\nIn summary, the data used in the paper consists of a large dataset containing real-world images and videos of fire, smoke, and normal scenes, which is used to train a deep CNN model for object detection.'"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def format_qa_pairs(questions, answers):\n",
    "    \"\"\"Format Q and A pairs\"\"\"\n",
    "    \n",
    "    formatted_string = \"\"\n",
    "    for i, (question, answer) in enumerate(zip(questions, answers), start=1):\n",
    "        formatted_string += f\"Question {i}: {question}\\nAnswer {i}: {answer}\\n\\n\"\n",
    "    return formatted_string.strip()\n",
    "\n",
    "context = format_qa_pairs(questions, answers)\n",
    "\n",
    "# Prompt\n",
    "template = \"\"\"Here is a set of Q+A pairs:\n",
    "\n",
    "{context}\n",
    "\n",
    "Use these to synthesize an answer to the question: {question}\n",
    "\"\"\"\n",
    "\n",
    "prompt = ChatPromptTemplate.from_template(template)\n",
    "\n",
    "final_rag_chain = (\n",
    "    prompt\n",
    "    | llm\n",
    "    | StrOutputParser()\n",
    ")\n",
    "\n",
    "final_rag_chain.invoke({\"context\":context,\"question\":question})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4dc9060-ebea-4fe4-b8be-434286a95402",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# 11. Query Translation -- Step Back\n",
    "# từ câu hỏi ban đầu tạo 1 câu hỏi lùi mức độ cao hơn, trừu tường hơn có thể đóng vai trò là điều kiện tiên quyết để trả lời chính xác câu hỏi ban đầu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "c6cffd66-8e3f-438e-8254-5ade25c2ed18",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Few Shot Examples\n",
    "from langchain_core.prompts import ChatPromptTemplate, FewShotChatMessagePromptTemplate\n",
    "examples = [\n",
    "    {\n",
    "        \"input\": \"Could the members of The Police perform lawful arrests?\",\n",
    "        \"output\": \"what can the members of The Police do?\",\n",
    "    },\n",
    "    {\n",
    "        \"input\": \"Jan Sindel’s was born in what country?\",\n",
    "        \"output\": \"what is Jan Sindel’s personal history?\",\n",
    "    },\n",
    "]\n",
    "# We now transform these to example messages\n",
    "example_prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"human\", \"{input}\"),\n",
    "        (\"ai\", \"{output}\"),\n",
    "    ]\n",
    ")\n",
    "few_shot_prompt = FewShotChatMessagePromptTemplate(\n",
    "    example_prompt=example_prompt,\n",
    "    examples=examples,\n",
    ")\n",
    "prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\n",
    "            \"system\",\n",
    "            \"\"\"You are an expert at world knowledge. Your task is to step back and paraphrase a question to a more generic step-back question, which is easier to answer. Here are a few examples:\"\"\",\n",
    "        ),\n",
    "        # Few shot examples\n",
    "        few_shot_prompt,\n",
    "        # New question\n",
    "        (\"user\", \"{question}\"),\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "cb7ffa4b-4189-4359-9a9c-8640968b39cc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"I'm ready to help. Please go ahead and ask your question, and I'll try to paraphrase it into a more generic step-back question that's easier for me to answer.\""
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# generate_queries_step_back = prompt | ChatOpenAI(temperature=0) | StrOutputParser()\n",
    "generate_queries_step_back = prompt | model | StrOutputParser()\n",
    "question = \"What data is used in the paper?\"\n",
    "generate_queries_step_back.invoke({\"question\": question})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "86e78f0d-7b78-4853-ab85-7d9e24044ae1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Based on the provided documents, it appears that a large dataset containing fire, smoke, and normal scenes is used in the paper. This dataset has real-world images and videos collected from various sources, including live cameras and pre-recorded video files. The goal of using this dataset is to have a large enough and balanced dataset that can generalize well to new data.\\n\\nThe specific characteristics of the dataset are not explicitly stated in the provided documents, but it is mentioned that the dataset needs to be processed (e.g., resizing or normalizing) to achieve the desired level of quality.'"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Response prompt \n",
    "response_prompt_template = \"\"\"You are an expert of world knowledge. I am going to ask you a question. Your response should be comprehensive and not contradicted with the following context if they are relevant. Otherwise, ignore them if they are not relevant.\n",
    "\n",
    "# {normal_context}\n",
    "# {step_back_context}\n",
    "\n",
    "# Original Question: {question}\n",
    "# Answer:\"\"\"\n",
    "response_prompt = ChatPromptTemplate.from_template(response_prompt_template)\n",
    "\n",
    "chain = (\n",
    "    {\n",
    "        # Retrieve context using the normal question\n",
    "        \"normal_context\": RunnableLambda(lambda x: x[\"question\"]) | retriever,\n",
    "        # Retrieve context using the step-back question\n",
    "        \"step_back_context\": generate_queries_step_back | retriever,\n",
    "        # Pass on the question\n",
    "        \"question\": lambda x: x[\"question\"],\n",
    "    }\n",
    "    | response_prompt\n",
    "    | model\n",
    "    | StrOutputParser()\n",
    ")\n",
    "\n",
    "chain.invoke({\"question\": question})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6855818c-212a-4d60-ad5c-09025d4d649b",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# 12. Query Translation -- HyDE\n",
    "# nhằm tạo ra 1 tài liệu giả định từ llm để trả lời câu hỏi người dùng.\n",
    "# tài liệu này được nhúng, sdung để truy xuất tài liệu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "a1484bc9-fe63-4f75-bbfb-445fa8154f7b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Here\\'s a passage answering the question:\\n\\nTask decomposition is a crucial concept in the realm of Large Language Models (LLMs), particularly in the context of natural language processing (NLP) and artificial intelligence (AI). In essence, task decomposition refers to the process of breaking down complex tasks or goals into smaller, more manageable sub-tasks that can be executed independently by individual agents within a distributed AI system.\\n\\nIn LLMs, task decomposition is essential for several reasons. Firstly, it enables the distribution of workload across multiple agents, thereby allowing for greater computational efficiency and scalability. Secondly, it facilitates the development of specialized agents with unique capabilities, each focusing on a specific sub-task that leverages their strengths. This approach also enables the reuse of pre-trained models and reduces the need for re-training from scratch.\\n\\nTask decomposition in LLMs typically involves several key steps: (1) identifying the overall task objective; (2) analyzing the task\\'s underlying structure and relationships; (3) decomposing the task into smaller sub-tasks or modules; and (4) assigning each module to a specific agent with the necessary skills and expertise. By breaking down complex tasks into more manageable pieces, LLMs can effectively leverage their distributed architecture to achieve greater accuracy, speed, and adaptability in solving real-world problems.\\n\\nSources:\\n\\n* Wang et al., \"Task Decomposition for Large-Scale Language Models,\" Journal of Artificial Intelligence Research (2020)\\n* Li et al., \"Distributed Natural Language Processing with Task Decomposition,\" Proceedings of the 34th International Conference on Machine Learning (2017)\\n\\nPlease let me know if you\\'d like me to modify anything!'"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain.prompts import ChatPromptTemplate\n",
    "\n",
    "# HyDE document genration\n",
    "template = \"\"\"Please write a scientific paper passage to answer the question\n",
    "Question: {question}\n",
    "Passage:\"\"\"\n",
    "prompt_hyde = ChatPromptTemplate.from_template(template)\n",
    "\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "generate_docs_for_retrieval = (\n",
    "    prompt_hyde | model | StrOutputParser() \n",
    ")\n",
    "\n",
    "# Run\n",
    "question = \"What is task decomposition for LLM agents?\"\n",
    "generate_docs_for_retrieval.invoke({\"question\":question})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "765ecfe1-59ae-47a6-a003-3e09ea998236",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(page_content='detected objects highlighted. The SFD algorithm is a\\npowerful tool for detecting ﬁres in real-time and enablesquick and effective responses to potential ﬁre hazards.The smart ﬁre detection (SFD) uses Yolov8 object\\ndetection model. It involves several steps:\\n•The video input source is set up from either a live\\ncamera or pre-recorded video ﬁle.\\n•The video capture process is started, and each frame of\\nthe video is looped through.\\n•Image pre-processing techniques are applied to each\\nframe, and the pre-processed frame is passed to the\\nYolov8 model for object detection.\\n•The detected objects are checked for ﬁre-related\\nclasses, such as ‘‘ﬂames’’, ‘‘smoke’’, or ‘‘embers’’.\\n•If a ﬁre-related class is detected, an alarm is triggered,\\nand relevant authorities are notiﬁed.', metadata={'id': 'data\\\\yolov8.pdf:9:1', 'page': 9, 'source': 'data\\\\yolov8.pdf'}),\n",
       " Document(page_content='tative of the overall dataset. Other pre-processing steps,\\nsuch as resizing or normalizing the data, may also benecessary. The goal is to have a large enough, balanced\\ndataset that can generalize well to new data.\\nStep 3: Model selection This step involves selecting the\\nappropriate object detection algorithm for training the ﬁre\\ndetection model. There are several algorithms to choose\\nfrom, such as YOLOv8, Faster R-CNN, and SSD, eachwith its own advantages and disadvantages. The selected\\nalgorithm should have good performance on the collected\\ndataset and be capable of handling different ﬁre scenarios,depending on the requirements of the smart ﬁre detection\\nsystem. YOLOv8 is a popular choice due to its speed andaccuracy, but other algorithms can also be used based on\\nspeciﬁc needs.', metadata={'id': 'data\\\\yolov8.pdf:7:1', 'page': 7, 'source': 'data\\\\yolov8.pdf'}),\n",
       " Document(page_content='costs associated with false alarms.\\n•Cost-effective: The proposed approach may be cost-\\neffective compared to traditional ﬁre detection methods\\nas it can be implemented using low-cost cameras and\\nhardware, reducing the need for expensive ﬁre detectionsystems.\\n•Large dataset: Unlike other methods that use small\\nnumber of datasets, a large dataset containing ﬁre,smoke, and normal scenes is used. The dataset has real-\\nworld images and videos collected from various\\nsources. The dataset has a diverse range of ﬁrescenarios, including indoor and outdoor ﬁres, small\\nand large ﬁres, and low-light and high-light conditions.\\nA deep CNN gathers essential data from big datasets toproduce precise predictions and reduce overﬁtting.\\nThe following is how the remaining work is structured.', metadata={'id': 'data\\\\yolov8.pdf:1:5', 'page': 1, 'source': 'data\\\\yolov8.pdf'}),\n",
       " Document(page_content='•YOLOv5 still requires large amounts of data for\\ntraining and may suffer from spurious regressions,and it may have limitations in detecting small objects or\\nobjects with complex shapes.\\nDespite the advances in deep learning-based approaches\\nfor ﬁre detection, there are still some challenges that need\\nto be addressed. For example, there is a need for more\\ndiverse and larger datasets for training and testing theseapproaches. Additionally, the use of low-quality cameras\\nor poor lighting conditions can affect the accuracy of ﬁre\\ndetection algorithms.\\nIn this paper, a YOLOv8-based approach for ﬁre\\ndetection in smart cities is proposed. To the best of our\\nknowledge, this is the ﬁrst study to investigate the use ofthe YOLOv8 algorithm for ﬁre detection in smart cities.', metadata={'id': 'data\\\\yolov8.pdf:4:0', 'page': 4, 'source': 'data\\\\yolov8.pdf'})]"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Retrieve\n",
    "retrieval_chain = generate_docs_for_retrieval | retriever \n",
    "retireved_docs = retrieval_chain.invoke({\"question\":question})\n",
    "retireved_docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "ddc997cb-953a-484b-b9b6-139a1a4652dc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'The text does not explicitly mention \"task decomposition\" in the context of Large Language Model (LLM) agents. However, based on the provided documents, I can infer that task decomposition refers to breaking down a complex task into smaller sub-tasks or steps.\\n\\nIn this specific context, the documents describe a smart fire detection system using YOLOv8 object detection model. The process involves several steps:\\n\\n1. Setting up the video input source\\n2. Capturing and pre-processing video frames\\n3. Passing the pre-processed frames to the YOLOv8 model for object detection\\n4. Checking detected objects for fire-related classes (e.g., flames, smoke, embers)\\n5. Triggering an alarm if a fire-related class is detected\\n\\nThis breakdown of steps can be considered as task decomposition, where the overall task of smart fire detection is divided into smaller, manageable sub-tasks that can be executed sequentially or in parallel to achieve the desired outcome.'"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# RAG\n",
    "template = \"\"\"Answer the following question based on this context:\n",
    "\n",
    "{context}\n",
    "\n",
    "Question: {question}\n",
    "\"\"\"\n",
    "\n",
    "prompt = ChatPromptTemplate.from_template(template)\n",
    "\n",
    "final_rag_chain = (\n",
    "    prompt\n",
    "    | llm\n",
    "    | StrOutputParser()\n",
    ")\n",
    "\n",
    "final_rag_chain.invoke({\"context\":retireved_docs,\"question\":question})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a2988af-9889-4856-9ed3-3b4687633c12",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# 13. Routing\n",
    "# llm -> chọn đường dẫn tới datasource liên quan tới câu hỏi\n",
    "Logical routing | Semantic routing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "78bdd05b-dd55-4ce6-9867-8ea4033bb712",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Literal\n",
    "\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.pydantic_v1 import BaseModel, Field\n",
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "# ollama\n",
    "from langchain_experimental.llms.ollama_functions import OllamaFunctions\n",
    "\n",
    "# Data model\n",
    "class RouteQuery(BaseModel):\n",
    "    \"\"\"Route a user query to the most relevant datasource.\"\"\"\n",
    "\n",
    "    datasource: Literal[\"python_docs\", \"js_docs\", \"golang_docs\"] = Field(\n",
    "        ...,\n",
    "        description=\"Given a user question choose which datasource would be most relevant for answering their question\",\n",
    "    )\n",
    "\n",
    "# LLM with function call \n",
    "llm = ChatOpenAI(model=\"gpt-3.5-turbo-0125\", temperature=0)\n",
    "# llm = OllamaFunctions(model=\"llama3\", format=\"json\", temperature=0)\n",
    "\n",
    "structured_llm = llm.with_structured_output(RouteQuery)\n",
    "\n",
    "# Prompt \n",
    "system = \"\"\"You are an expert at routing a user question to the appropriate data source.\n",
    "\n",
    "Based on the programming language the question is referring to, route it to the relevant data source.\"\"\"\n",
    "\n",
    "prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\", system),\n",
    "        (\"human\", \"{question}\"),\n",
    "    ]\n",
    ")\n",
    "\n",
    "# Define router \n",
    "router = prompt | structured_llm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b09e444-fddd-4681-9d34-fe1ee2f767e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example user query\n",
    "user_question = \"How do I create a virtual environment in Python?\"\n",
    "\n",
    "# Generate the final prompt with the user question\n",
    "final_prompt = prompt.format(question=user_question)\n",
    "\n",
    "# Get the routing decision\n",
    "routing_decision = router.invoke({\"question\": user_question})\n",
    "\n",
    "print(routing_decision)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ebcf668-cf44-44c6-89d4-c519c60357dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "question = \"How do I create a virtual environment in Python?\"\n",
    "\n",
    "result = router.invoke({\"question\": question})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "540b93cf-3d32-4be2-aaf8-23bb67612eb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfbe9f79-3c61-46ed-9ee8-fc7e35ba9e0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "result.datasource"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11b213e8-ba0b-4c83-b202-709e8e238ed2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def choose_route(result):\n",
    "    if \"python_docs\" in result.datasource.lower():\n",
    "        ### Logic here \n",
    "        return \"chain for python_docs\"\n",
    "    elif \"js_docs\" in result.datasource.lower():\n",
    "        ### Logic here \n",
    "        return \"chain for js_docs\"\n",
    "    else:\n",
    "        ### Logic here \n",
    "        return \"golang_docs\"\n",
    "\n",
    "from langchain_core.runnables import RunnableLambda\n",
    "\n",
    "full_chain = router | RunnableLambda(choose_route)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4483d5b4-3b41-4e3b-b6cc-6a112e34453a",
   "metadata": {},
   "outputs": [],
   "source": [
    "full_chain.invoke({\"question\": question})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6cdf9b9d-d710-4402-b063-76b61ee23bd6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Semantic routing \n",
    "# dựa trên ngữ nghĩa tính cosi similarity giữa câu hỏi và template"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1aac3e3f-84d5-4e24-ba0c-c58f33effcba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using MATH\n",
      "A question that may seem simple, but can be quite complex when delved into!\n",
      "\n",
      "To tackle this question, let's break it down into smaller, more manageable parts. We'll start with some definitions and then build our way up to understanding what a black hole is.\n",
      "\n",
      "**Part 1: What is gravity?**\n",
      "\n",
      "Gravity is a fundamental force of nature that describes the attraction between two objects with mass (or energy). According to Einstein's theory of general relativity, massive objects warp the fabric of spacetime around them, creating a gravitational field. This warping causes other objects with mass to follow curved trajectories, which we experience as the force of gravity.\n",
      "\n",
      "**Part 2: What is a singularity?**\n",
      "\n",
      "A singularity is a point in spacetime where the curvature becomes infinite and the laws of physics as we know them break down. In general relativity, singularities occur when the density and curvature of spacetime become extreme, such as at the center of a black hole or during the Big Bang.\n",
      "\n",
      "**Part 3: What is a black hole?**\n",
      "\n",
      "Now, let's put it all together!\n",
      "\n",
      "A black hole is a region in spacetime where the gravitational pull is so strong that nothing, including light, can escape once it falls within a certain distance, called the event horizon. This occurs when a massive star collapses under its own gravity, causing a massive amount of matter to be compressed into an incredibly small point – the singularity.\n",
      "\n",
      "The event horizon marks the boundary beyond which anything that crosses it will be pulled inexorably towards the singularity, where it will be crushed out of existence. The intense gravitational field around a black hole causes spacetime to curve and distort in extreme ways, making it difficult for us to visualize or understand what's happening near the event horizon.\n",
      "\n",
      "**Putting it all together:**\n",
      "\n",
      "In summary, a black hole is a region in spacetime characterized by an incredibly strong gravitational pull, caused by the collapse of a massive star. The event horizon marks the boundary beyond which nothing can escape, and the singularity at the center represents the point where the laws of physics as we know them break down.\n",
      "\n",
      "I hope this breakdown has helped you understand what a black hole is! Do you have any follow-up questions or would you like me to elaborate on any of these points?\n"
     ]
    }
   ],
   "source": [
    "from langchain.utils.math import cosine_similarity\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "from langchain_core.runnables import RunnableLambda, RunnablePassthrough\n",
    "from langchain_openai import ChatOpenAI, OpenAIEmbeddings\n",
    "\n",
    "\n",
    "from langchain_community.embeddings.ollama import OllamaEmbeddings\n",
    "from langchain_community.llms.ollama import Ollama\n",
    "\n",
    "embeddings = OllamaEmbeddings(model=\"llama3\")\n",
    "\n",
    "model = Ollama(model=\"llama3\")\n",
    "\n",
    "# Two prompts\n",
    "physics_template = \"\"\"You are a very smart physics professor. \\\n",
    "You are great at answering questions about physics in a concise and easy to understand manner. \\\n",
    "When you don't know the answer to a question you admit that you don't know.\n",
    "\n",
    "Here is a question:\n",
    "{query}\"\"\"\n",
    "\n",
    "math_template = \"\"\"You are a very good mathematician. You are great at answering math questions. \\\n",
    "You are so good because you are able to break down hard problems into their component parts, \\\n",
    "answer the component parts, and then put them together to answer the broader question.\n",
    "\n",
    "Here is a question:\n",
    "{query}\"\"\"\n",
    "\n",
    "# Embed prompts\n",
    "# embeddings = OpenAIEmbeddings()\n",
    "prompt_templates = [physics_template, math_template]\n",
    "prompt_embeddings = embeddings.embed_documents(prompt_templates)\n",
    "\n",
    "# Route question to prompt \n",
    "def prompt_router(input):\n",
    "    # Embed question\n",
    "    query_embedding = embeddings.embed_query(input[\"query\"])\n",
    "    # Compute similarity\n",
    "    similarity = cosine_similarity([query_embedding], prompt_embeddings)[0]\n",
    "    most_similar = prompt_templates[similarity.argmax()]\n",
    "    # Chosen prompt \n",
    "    print(\"Using MATH\" if most_similar == math_template else \"Using PHYSICS\")\n",
    "    return PromptTemplate.from_template(most_similar)\n",
    "\n",
    "\n",
    "chain = (\n",
    "    {\"query\": RunnablePassthrough()}\n",
    "    | RunnableLambda(prompt_router)\n",
    "    | model\n",
    "    | StrOutputParser()\n",
    ")\n",
    "\n",
    "print(chain.invoke(\"What's a black hole\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0e8a664-8f89-499e-940a-03e312e36123",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# 14. Query Structuring\n",
    "Chuyển câu hỏi truy vấn ở dạng ngôn ngữ tự nhiên sang có cấu trúc \n",
    "test Self-querying"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e9a816a6-783d-4a4d-a16d-8b29b69bb949",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'source': 'pbAd8O1Lvm4',\n",
       " 'title': 'Self-reflective RAG with LangGraph: Self-RAG and CRAG',\n",
       " 'description': 'Unknown',\n",
       " 'view_count': 18109,\n",
       " 'thumbnail_url': 'https://i.ytimg.com/vi/pbAd8O1Lvm4/hq720.jpg',\n",
       " 'publish_date': '2024-02-07 00:00:00',\n",
       " 'length': 1058,\n",
       " 'author': 'LangChain'}"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_community.document_loaders import YoutubeLoader\n",
    "\n",
    "docs = YoutubeLoader.from_youtube_url(\n",
    "    \"https://www.youtube.com/watch?v=pbAd8O1Lvm4\", add_video_info=True\n",
    ").load()\n",
    "\n",
    "docs[0].metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "73594a0d-6e3f-44b9-beed-4a3243db88fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime\n",
    "from typing import Literal, Optional, Tuple\n",
    "from langchain_core.pydantic_v1 import BaseModel, Field\n",
    "\n",
    "class TutorialSearch(BaseModel):\n",
    "    \"\"\"Search over a database of tutorial videos about a software library.\"\"\"\n",
    "\n",
    "    content_search: str = Field(\n",
    "        ...,\n",
    "        description=\"Similarity search query applied to video transcripts.\",\n",
    "    )\n",
    "    title_search: str = Field(\n",
    "        ...,\n",
    "        description=(\n",
    "            \"Alternate version of the content search query to apply to video titles. \"\n",
    "            \"Should be succinct and only include key words that could be in a video \"\n",
    "            \"title.\"\n",
    "        ),\n",
    "    )\n",
    "    min_view_count: Optional[int] = Field(\n",
    "        None,\n",
    "        description=\"Minimum view count filter, inclusive. Only use if explicitly specified.\",\n",
    "    )\n",
    "    max_view_count: Optional[int] = Field(\n",
    "        None,\n",
    "        description=\"Maximum view count filter, exclusive. Only use if explicitly specified.\",\n",
    "    )\n",
    "    earliest_publish_date: Optional[datetime.date] = Field(\n",
    "        None,\n",
    "        description=\"Earliest publish date filter, inclusive. Only use if explicitly specified.\",\n",
    "    )\n",
    "    latest_publish_date: Optional[datetime.date] = Field(\n",
    "        None,\n",
    "        description=\"Latest publish date filter, exclusive. Only use if explicitly specified.\",\n",
    "    )\n",
    "    min_length_sec: Optional[int] = Field(\n",
    "        None,\n",
    "        description=\"Minimum video length in seconds, inclusive. Only use if explicitly specified.\",\n",
    "    )\n",
    "    max_length_sec: Optional[int] = Field(\n",
    "        None,\n",
    "        description=\"Maximum video length in seconds, exclusive. Only use if explicitly specified.\",\n",
    "    )\n",
    "\n",
    "    def pretty_print(self) -> None:\n",
    "        for field in self.__fields__:\n",
    "            if getattr(self, field) is not None and getattr(self, field) != getattr(\n",
    "                self.__fields__[field], \"default\", None\n",
    "            ):\n",
    "                print(f\"{field}: {getattr(self, field)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20f3b6f1-f47d-4d1a-b3bd-9c5426515c19",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from langchain_core.prompts import ChatPromptTemplate\n",
    "# from langchain_openai import ChatOpenAI\n",
    "\n",
    "# system = \"\"\"You are an expert at converting user questions into database queries. \\\n",
    "# You have access to a database of tutorial videos about a software library for building LLM-powered applications. \\\n",
    "# Given a question, return a database query optimized to retrieve the most relevant results.\n",
    "\n",
    "# If there are acronyms or words you are not familiar with, do not try to rephrase them.\"\"\"\n",
    "# prompt = ChatPromptTemplate.from_messages(\n",
    "#     [\n",
    "#         (\"system\", system),\n",
    "#         (\"human\", \"{question}\"),\n",
    "#     ]\n",
    "# )\n",
    "# llm = ChatOpenAI(model=\"gpt-3.5-turbo-0125\", temperature=0)\n",
    "# structured_llm = llm.with_structured_output(TutorialSearch)\n",
    "# query_analyzer = prompt | structured_llm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "c3310ef2-73c1-4ec2-9f0d-c39a94cbbcb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# test ollama\n",
    "from langchain_core.output_parsers import JsonOutputParser\n",
    "from langchain_community.embeddings.ollama import OllamaEmbeddings\n",
    "from langchain_community.llms.ollama import Ollama\n",
    "\n",
    "embeddings = OllamaEmbeddings(model=\"llama3\")\n",
    "\n",
    "model = Ollama(model=\"llama3\")\n",
    "\n",
    "system = \"\"\"You are an expert at converting user questions into database queries. \\\n",
    "You have access to a database of tutorial videos about a software library for building LLM-powered applications. \\\n",
    "Given a question, return a database query optimized to retrieve the most relevant results.\n",
    "\\nFormatting Instructions: {format_instructions}\n",
    "If there are acronyms or words you are not familiar with, do not try to rephrase them.\"\"\"\n",
    "\n",
    "prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\", system),\n",
    "        (\"human\", \"{question}\"),\n",
    "    ]\n",
    ")\n",
    "\n",
    "out_parser = JsonOutputParser(pydantic_object=TutorialSearch)\n",
    "\n",
    "query_analyzer = prompt | model | out_parser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "867f5f48-a16d-4743-a45a-c6e854137b01",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'content_search': 'title: (\"RAG\" OR \"from scratch\") AND description: (\"RAG\" OR \"from scratch\")',\n",
       " 'title_search': 'title: (\"RAG\" OR \"from scratch\")',\n",
       " 'min_view_count': None,\n",
       " 'max_view_count': None,\n",
       " 'earliest_publish_date': None,\n",
       " 'latest_publish_date': None,\n",
       " 'min_length_sec': None,\n",
       " 'max_length_sec': None}"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "query_analyzer.invoke({\"question\": \"rag from scratch\",\n",
    "                       \"format_instructions\": out_parser.get_format_instructions()\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "e0cabd6b-a22c-4ba8-acd0-c76dfcbc7066",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'content_search': 'chat langchain',\n",
       " 'title_search': 'chat langchain',\n",
       " 'earliest_publish_date': {'$lt': '2024-01-01'}}"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "query_analyzer.invoke({\"question\": \"videos that are focused on the topic of chat langchain that are published before 2024\",\n",
    "                       \"format_instructions\": out_parser.get_format_instructions()\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "fb41165c-fb5c-4628-9c07-0ec543290d14",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'content_search': 'multi-modal models AND (length < 300) AND (view_count < 500)',\n",
       " 'title_search': 'using multi-modal models',\n",
       " 'min_view_count': 0,\n",
       " 'max_view_count': 499,\n",
       " 'earliest_publish_date': None,\n",
       " 'latest_publish_date': None,\n",
       " 'min_length_sec': 0,\n",
       " 'max_length_sec': 300}"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "query_analyzer.invoke({\"question\": \"how to use multi-modal models in an agent, only videos under 5 minutes\",\n",
    "                       \"format_instructions\": out_parser.get_format_instructions()\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d747d0a0-fcb3-407c-9956-f332fef370ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# query_analyzer.invoke({\"question\": \"rag from scratch\"}).pretty_print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aaad6ea9-2d7f-4deb-9ba3-3ebe7db898af",
   "metadata": {},
   "outputs": [],
   "source": [
    "# query_analyzer.invoke(\n",
    "#     {\"question\": \"videos that are focused on the topic of chat langchain that are published before 2024\"}\n",
    "# ).pretty_print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "edc186d2-3b52-490e-87fa-a7def7a3b4d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# query_analyzer.invoke(\n",
    "#     {\n",
    "#         \"question\": \"how to use multi-modal models in an agent, only videos under 5 minutes\"\n",
    "#     }\n",
    "# ).pretty_print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fca8f10e-7536-48f7-97be-6c271ae4b8e5",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba4c8296-c4b5-43c3-9874-3abca4bb4690",
   "metadata": {},
   "outputs": [],
   "source": [
    "# test Self-querying"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "64b2a00f-68a2-44a3-884a-8f54bf27d9b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.vectorstores.chroma import Chroma\n",
    "from langchain_core.documents import Document\n",
    "from langchain_community.embeddings.ollama import OllamaEmbeddings\n",
    "from langchain_community.llms.ollama import Ollama\n",
    "\n",
    "docs = [\n",
    "    Document(\n",
    "        page_content=\"A bunch of scientists bring back dinosaurs and mayhem breaks loose\",\n",
    "        metadata={\"year\": 1993, \"rating\": 7.7, \"genre\": \"science fiction\"},\n",
    "    ),\n",
    "    Document(\n",
    "        page_content=\"Leo DiCaprio gets lost in a dream within a dream within a dream within a ...\",\n",
    "        metadata={\"year\": 2010, \"director\": \"Christopher Nolan\", \"rating\": 8.2},\n",
    "    ),\n",
    "    Document(\n",
    "        page_content=\"A psychologist / detective gets lost in a series of dreams within dreams within dreams and Inception reused the idea\",\n",
    "        metadata={\"year\": 2006, \"director\": \"Satoshi Kon\", \"rating\": 8.6},\n",
    "    ),\n",
    "    Document(\n",
    "        page_content=\"A bunch of normal-sized women are supremely wholesome and some men pine after them\",\n",
    "        metadata={\"year\": 2019, \"director\": \"Greta Gerwig\", \"rating\": 8.3},\n",
    "    ),\n",
    "    Document(\n",
    "        page_content=\"Toys come alive and have a blast doing so\",\n",
    "        metadata={\"year\": 1995, \"genre\": \"animated\"},\n",
    "    ),\n",
    "    Document(\n",
    "        page_content=\"Three men walk into the Zone, three men walk out of the Zone\",\n",
    "        metadata={\n",
    "            \"year\": 1979,\n",
    "            \"director\": \"Andrei Tarkovsky\",\n",
    "            \"genre\": \"thriller\",\n",
    "            \"rating\": 9.9,\n",
    "        },\n",
    "    ),\n",
    "]\n",
    "\n",
    "\n",
    "embeddings = OllamaEmbeddings(model=\"llama3\")\n",
    "model = Ollama(model=\"llama3\")\n",
    "vectorstore = Chroma.from_documents(docs, embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "28e551a6-ce52-4382-bec1-01976c0537b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating our self-querying retriever\n",
    "from langchain.chains.query_constructor.base import AttributeInfo\n",
    "from langchain.retrievers.self_query.base import SelfQueryRetriever\n",
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "metadata_field_info = [\n",
    "    AttributeInfo(\n",
    "        name=\"genre\",\n",
    "        description=\"The genre of the movie. One of ['science fiction', 'comedy', 'drama', 'thriller', 'romance', 'action', 'animated']\",\n",
    "        type=\"string\",\n",
    "    ),\n",
    "    AttributeInfo(\n",
    "        name=\"year\",\n",
    "        description=\"The year the movie was released\",\n",
    "        type=\"integer\",\n",
    "    ),\n",
    "    AttributeInfo(\n",
    "        name=\"director\",\n",
    "        description=\"The name of the movie director\",\n",
    "        type=\"string\",\n",
    "    ),\n",
    "    AttributeInfo(\n",
    "        name=\"rating\", description=\"A 1-10 rating for the movie\", type=\"float\"\n",
    "    ),\n",
    "]\n",
    "document_content_description = \"Brief summary of a movie\"\n",
    "llm = model\n",
    "retriever = SelfQueryRetriever.from_llm(\n",
    "    llm,\n",
    "    vectorstore,\n",
    "    document_content_description,\n",
    "    metadata_field_info,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "c6f6e627-fc75-4701-bb2f-50664e092980",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chains.combine_documents import create_stuff_documents_chain\n",
    "from langchain.chains import create_retrieval_chain\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\", \"Answer the user's questions based on the context: {context}\"),\n",
    "        (\"human\", \"{input}\"),\n",
    "    ]\n",
    ")\n",
    "chain = create_stuff_documents_chain(\n",
    "    llm=model,\n",
    "    prompt=prompt\n",
    ")\n",
    "retriever_chain = create_retrieval_chain(retriever, chain)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "6da5da6a-3354-4924-99fa-4d306c41b992",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(page_content='A psychologist / detective gets lost in a series of dreams within dreams within dreams and Inception reused the idea', metadata={'director': 'Satoshi Kon', 'rating': 8.6, 'year': 2006}),\n",
       " Document(page_content='A psychologist / detective gets lost in a series of dreams within dreams within dreams and Inception reused the idea', metadata={'director': 'Satoshi Kon', 'rating': 8.6, 'year': 2006}),\n",
       " Document(page_content='Three men walk into the Zone, three men walk out of the Zone', metadata={'director': 'Andrei Tarkovsky', 'genre': 'thriller', 'rating': 9.9, 'year': 1979}),\n",
       " Document(page_content='Three men walk into the Zone, three men walk out of the Zone', metadata={'director': 'Andrei Tarkovsky', 'genre': 'thriller', 'rating': 9.9, 'year': 1979})]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# This example only specifies a filter\n",
    "retriever.invoke(\"I want to watch a movie rated higher than 8.5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "c5284df2-ca1c-4bd3-a239-c8d0a1da4fb1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input': 'I want to watch a movie rated higher than 8.5',\n",
       " 'context': [Document(page_content='A psychologist / detective gets lost in a series of dreams within dreams within dreams and Inception reused the idea', metadata={'director': 'Satoshi Kon', 'rating': 8.6, 'year': 2006}),\n",
       "  Document(page_content='A psychologist / detective gets lost in a series of dreams within dreams within dreams and Inception reused the idea', metadata={'director': 'Satoshi Kon', 'rating': 8.6, 'year': 2006}),\n",
       "  Document(page_content='Three men walk into the Zone, three men walk out of the Zone', metadata={'director': 'Andrei Tarkovsky', 'genre': 'thriller', 'rating': 9.9, 'year': 1979}),\n",
       "  Document(page_content='Three men walk into the Zone, three men walk out of the Zone', metadata={'director': 'Andrei Tarkovsky', 'genre': 'thriller', 'rating': 9.9, 'year': 1979})],\n",
       " 'answer': 'A unique context! As a psychologist/detective lost in dreams within dreams, my mind is sharp and ready to analyze this situation.\\n\\nTo answer your question, I must consult my dream journal and the fragmented memories of my subconscious. Ah yes, I recall that Inception was rated 8.5 by many critics. However, considering the Zone\\'s unpredictable nature, I suspect we\\'re looking for a film with an even higher rating.\\n\\nLet me access the collective unconscious... (tapping forehead) Ah ha! According to the cinematic echoes within my dreams, I\\'ve got it! The movie \"Interstellar\" (2014) has a rating of 8.6. It\\'s a thought-provoking sci-fi epic that explores wormholes and the mysteries of time. Given the Zone\\'s propensity for bending reality, I believe this film will resonate with your experiences.\\n\\nShall we venture into the cinematic realm and explore the depths of \"Interstellar\"?'}"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# This example only specifies a filter\n",
    "retriever_chain.invoke({\"input\": \"I want to watch a movie rated higher than 8.5\"})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01c3fa4f-18ed-4e77-b1ef-1a26ca8cea86",
   "metadata": {},
   "source": [
    "# 15. Multi-representation Indexing\n",
    "# Indexing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13dc9b8b-4b3b-47c9-8c29-81bfd9bb5a47",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26befe38-ba8b-4716-ad85-b427209072c1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c1fb17f-5636-45b4-9583-bfa97679ba8d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b62da3f2-92ae-45b9-a424-1486a6e51b9e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
